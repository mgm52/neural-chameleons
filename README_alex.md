# Self-Obfuscation - Sept 8 State

Hello!

I'll give 3 explanations of the codebase in increasing order of complexity based on what you need. Hopefully you may only need to look into explanation 1 or 2!

## Explanation 1: Just Plotting & Data/Training Info

It's reasonably likely you'll just want to touch my plotting code, in which case you only need to read this:

**Plotting** - My final plots & data are all in `experiments/self_obfuscation_pipeline/outputs/final_plots`. If you need to regenerate plots on existing data, you should use this folder & the script that generates it.
- The script that actually generates the plots: `experiments/self_obfuscation_pipeline/generate_paper_plots.py`. **Its default args point to all the results we're **using**! And from there each directory will have a json with metadata, pointing to model checkpoint, etc.
- Also using `experiments/self_obfuscation_pipeline/generate_paper_model_loading_plots.py` to generate probe score / example generation plots.
- You can also find a neat summary of results paths + the best-performing trigger words in `plot_info_XXX.json` in `final_plots/` (check overleaf or runpod if not in repo).


## Explanation 2: Gathering New Results

If you need to generate new results entirely, you'll want to use scripts in the `experiments/self_obfuscation_pipeline` folder.

**Organism training & testing** - The core script to train & test new organisms is `run_full_pipeline.py`.
- Example commands used to generate final results *in this RunPod volume*:
  - **4-layer probing**: `/workspace/GIT_SHENANIGANS/self-obfuscation/.venv/bin/python /workspace/GIT_SHENANIGANS/self-obfuscation/experiments/self_obfuscation_pipeline/run_full_pipeline.py --model-probe-dataset "ablit_12ablit-synth-4l_12synth-21PRESET" --configs-json "experiments/self_obfuscation_pipeline/model_config_21preset.json"`
  - **Attention probing**: `/workspace/GIT_SHENANIGANS/self-obfuscation/.venv/bin/python /workspace/GIT_SHENANIGANS/self-obfuscation/experiments/self_obfuscation_pipeline/run_full_pipeline.py --model-probe-dataset "ablit_12ablit-synth-4l_12synth-21PRESET" --configs-json "experiments/self_obfuscation_pipeline/model_config_21preset.json"`
- Finding its outputs:
  - Outputs should all go to the appropriate folders in `experiments/self_obfuscation_pipeline/outputs/...`. You can trace from evaluation results to model checkpoint & probe checkpoint directories by examining metadata `.json` files in results folders.
  - *NOTE I had a bug where evaluation results got sent to the probe checkpoints folder instead. This may or may not now be fixed.*
- Explaining the args:
  - `--model-probe-dataset` expects a string describing the combination of base LLM (e.g. gemma vs ablit gemma), training probes, and training dataset to use for each organism.
    - **Example**: You should read `ablit_12ablit-synth-4l_12synth-21PRESET` as "abliterated model, training on 12 abliterated synthetic 4-layer probes, on a dataset of 12 synthetic concepts nicknamed 21PRESET".
    - **"...-21PRESET" types**: Use a dataset of 12 probes/concepts generated on Aug 20, nicknamed "experiment 21", at mega_dataset_evaluated_20250729_144844_harm_batch_train_PRESET.json. This is a "preset" because the exact full set of data is used every time, not resampled. **All my final results use these.**
    - **"...-J23" types**: Use a dataset of 63 probes/concepts generated on July 23. These saw really good deception (DolusChat) obfuscation results back in the day, but I haven't been able to recreate it... Though their harmful results weren't great at the time.
    - **"...-131alex types**: Use a dataset of 131 non-synthetic probes/concepts generated by Alex!!! In the code, these types enable `use_alex_probes` mode to skip ultrachat sampling & use more training samples.
  - `--configs-json` points to a json describing the variations of organism to try. Use this to vary LR, batch size, data sampling; but model-probe-dataset holds model, training probes, and dataset static.
    - **Example**: My results use `experiments/self_obfuscation_pipeline/model_config_21preset.json` for 21PRESET type organism training, just trying 3 combinations of LR/batch size. Though I only ended up using the first combination.
    - **Big sweeps**: By contrast, `experiments/self_obfuscation_pipeline/model_configs.json` contains a large number of variations... These resample data so aren't intended to be used with a PRESET type.
 - `--quick-test-first` runs with way smaller training & test sizes, to "quickly" see if the whole pipeline works!
 - `--full-mode` runs with higher training & testing sizes, and pre-existing probes in addition to post-trained. HOWEVER this is currently unused as I decided the "medium" mode was sufficient.

**Organism testing** - You should use `run_probe_pipeline.py` to run standalone / one-off testing, or `run_probe_variations.py` to run numerous tests (varying LR etc).
 - `run_probe_pipeline.py` is called automatically by `run_full_pipeline.py` (described above). It runs apollo, dolus deception, obfuscated-activations harmfulness, and my synthetic "merged" harmful evals.
 - Example command used to generate tests in my RunPod volume:
   - **4-layer probing**: `/workspace/GIT_SHENANIGANS/self-obfuscation/.venv/bin/python /workspace/GIT_SHENANIGANS/self-obfuscation/experiments/self_obfuscation_pipeline/run_probe_variations.py --model-checkpoint experiments/self_obfuscation_pipeline/outputs/model_checkpoints/sep08_ablit_12ablit-synth-4l_12synth-21PRESET_8e-6_lr_rererandom_21_original_1bs --output-base experiments/self_obfuscation_pipeline/outputs/evaluation_results/sep08_ablit_12ablit-synth-4l_12synth-21PRESET_8e-6_lr_rererandom_21_original_1bs`

## Explanation 3 - Repo Overview

In case you need to get in the weeds.

Key terms you'll see in the codebase:
- Evaluations:
  - **Merged** harmfulness (synthetic data generated using my pipeline)
  - **Obfact** harmfulness (from obfuscated-activations)
  - **Deception** (DolusChat)
  - **Apollo** / RepE deception
- **"Rated" / "labelled" data** - data that has been Likert-scale-rated.

Core locations:
- Model organism checkpoints
  - 1-layer logistic - `experiments/self_obfuscation_pipeline/outputs/model_checkpoints/aug20_4_ablit_8e-6_lr_random_21_4e-5probe_no_gibberish`
  - 4-layer logistic - `experiments/self_obfuscation_pipeline/outputs/model_checkpoints/sep08_ablit_12ablit-synth-4l_12synth-21PRESET_8e-6_lr_rererandom_21_original_1bs`
  - 8-layer logistic - `experiments/self_obfuscation_pipeline/outputs/model_checkpoints/sep08_ablit_12ablit-synth-8l_12synth-21PRESET_8e-6_lr_rererandom_21_original_1bs`
  - 1-layer attention - `experiments/self_obfuscation_pipeline/outputs/model_checkpoints/sep08_ablit_12ablit-synth-attn_12synth-21PRESET_8e-6_lr_rererandom_21_original_1bs`
  - 1-layer MLP - `experiments/self_obfuscation_pipeline/outputs/model_checkpoints/sep08_ablit_12ablit-synth-mlp_12synth-21PRESET_8e-6_lr_rererandom_21_original_1bs`
- Train-time probe checkpoints
  - *(you can also find these by reading `model_training_metadata.json` in each model folder)*
  - 1-layer logistic - `experiments/self_obfuscation_v1_rated/outputs/probe_checkpoints/probes_aug21_ablit_0075_4e-5_no_gibberish`
  - 4-layer logistic - `experiments/self_obfuscation_v1_rated/outputs/probe_checkpoints/probes_sept4_4_layers_12ablit_4e-5`
  - 8-layer logistic - `experiments/self_obfuscation_v1_rated/outputs/probe_checkpoints/probes_sept4_8_layers_12ablit_4e-5`
  - 1-layer attention - `experiments/self_obfuscation_v1_rated/outputs/probe_checkpoints/probes_sept8_attention_12ablit_4e-5`
  - 1-layer MLP - `experiments/self_obfuscation_v1_rated/outputs/probe_checkpoints/probes_sept8_mlp_12ablit_4e-5`

Folder structure:
- Misc
  - `/archive` - various unused folders, holding on to in case they become relevant but don't really want to git push them.
  - `/configs` - config files for deception & harmful probe training. 
    - ‚ö†Ô∏è These are mostly tech debt as training details are typically configured in `run_full_pipeline.py` anyway, which creates temporary config files based on the defaults in `/configs` (Sorry...)
  - `/data` - datasets & scripts used to load them!
    - The data used for final organism training, and merged harmfulness, is in `data/self_obfuscation_v1_synthetic_rating`. 
    - My dataset classes are in `data/data.py`, which are used by synthetic data & "Alex" data. ‚ö†Ô∏è But doluschat, apollorepe, and obfact all use adapters in `experiments/self_obfuscation_deception/dataset_adapters.py` instead. (Sorry...)
- Evaluation within `experiments/...`
  - **LMEval**: `/self_obfuscation_capability_evals`. The results .json can then be copied to the final plotting script (see Explanation 1).
  - **Dolus & ApolloRepe** deception probes: `/self_obfuscation_deception`
  - **ObfAct** harmful probes: `/self_obfuscation_harmful_2`
  - **Merged** harmful probes: `/self_obfuscation_v1_rated/evaluate_merged_probe.py` and `/self_obfuscation_v1_rated/probe_train_synthetic_rated.py`
- Organism training within `experiments/...`
  - `experiments/self_obfuscation_v0` - Original word-template-based self-obfuscation code (used in the Substack writeup).
  - `experiments/self_obfuscation_v1` - Self-obfuscation code that doesn't use rated data. ‚ö†Ô∏è Some of this is still haphazardly imported elsewhere (Sorry...)
  - `experiments/self_obfuscation_v1_rated` - Main self-obfuscation code!
    - Organism training: `concept_model_train_rated.py`
    - Probe training: `probe_train_synthetic_rated.py`
    - Trained-probe testing: `probe_test_synthetic_clued_labelled.py`

Random tips and notes
- You can check whether two model checkpoints were trained on the same data by checking their detailed_training_logs.json! All of our final organisms should be on the same.
- Likelihood evaluation & sample generations in `experiments/self_obfuscation_pipeline/outputs/evaluation_results/_likelihoods_and_generations`

Things I would try if I had more/infinite time:
- Reduce result spikiness by training multiple model organisms *across seeds*, per config
- Various obfuscation-boosting things I implemented, didn't work well, but could do with more testing:
  - I added a "--use-antonyms" flag to the organism training script (`concept_model_train_rated.py`), to have a chance of using antonymic trigger words during training. It's not used in the main pipeline (`run_full_pipeline.py`) because initial results were poor.
  - Training against the Jul23 set of probes/data, which produced excellent deception cosine similarity results back in the day (see `experiments/self_obfuscation_deception/outputs/test_results/20250801_025115_deception_probe_evaluation_base4e-05_jul23/cosine_similarity_vs_recall_checkpoint_gemma_final_layer.png`!) but attempted revival did not obfuscate well at all (see `sep07_base_63base-synth_63synth-J23-half-PRESET_4e-6_lr_jul23_noharmful`). Search "J23" in run_full_pipeline to see the revival attempt.
    - Note that the total_positive_examples in the detailed_results.json in the original png's directory may be larger than actually used
  - Training against the Alex set of probes/data. Search "alex" in run_full_pipeline to see the final attempt. You can also find the final (unfortunately bad) eval results in `sep01_alex_ablit_...` folders
  - Training against the larger set of 63 synthetic probes/data. This set was generated by non-ablated gemma, so I used it until switching to the ablated model to improve harmfulness results. Search "_63" in run_full_pipeline to see configs.
    - Would be fun to try some random selections of these 63 probes.
  - "Dual" data types in `concept_model_train_rated.py` (e.g. obfuscate X but not Y) - for some reason these never work well, even though they worked great in `self_obfuscation_v0` (word templates).
    - Maybe it's an implementation issue?
    - Or maybe we just need to **generate explicit dual data**! Right now I'm just hoping the existing data (each generated to fit one category) has good dual examples (fitting two concepts X and Y) by happenstance.
- Behaviour-matching using actual full logits from the base model rather than just one hot encoding each response token (but maybe that's just adding unnecessary restriction)
- Beyond just comparing to base model, a slightly more scientific evaluation would compare our organism against a model that's gone through our obfuscation training with **behaviour_lambda=1, obfuscation_lambda=0**. So we disentangle the behaviour-matching effects from obfuscation effects.
- Assessing model generations StrongREJECT-style.
- More rigorous side-by-side comparison of ablated vs non-ablated gemma at obfuscating various probes
- Try hand-crafting a training set...!!!
  - I believe it will be possible to elicit obfuscation with relatively few samples (say, a couple hundred). You could literally spend a few hours choosing (prompt, response, pretrained_probe.pkl, target_probe_score) tuples, testing the organism's obfuscation ability after *each* one. Then maybe synthetically generate from there.
- Actual use of low-rank adaptation during training (Sorry...)

üêå Quirks of the codebase üêå
- Nothing, it's perfect
  - Just kidding, probably the worst part is inconsistency in probe checkpoint location & eval result location. Most are in `self_obfuscation_pipeline` but some in `self_obfuscation_experiment/outputs/probe_checkpoints`. This could probably be fixed with a little careful regex replacing.