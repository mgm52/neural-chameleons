# RepE Honesty Probe Training Configuration
# Uses DeceptionLabelsFinal dataset (16k scientific Q&As with honest/deceptive labels)

model_name_or_path: "gemma_2_9b_instruct"  # Can be changed to a finetuned checkpoint

# Probe configurations
probes:
  logistic_repe_honesty:
    type: "logistic"
    learning_rate: 4e-5
    batch_size: 16
    num_epochs: 1
    target_layers: [12]
    pooling_type: "mean"
    max_samples: null  # Use all available training data (~13k examples)
    max_test_samples: null  # Use all test data (~3.3k examples)

training_datasets:
  train:
    positive: ["repe_deceptive_train"]  # Label=1 (deceptive) examples
    negative: ["repe_honest_train"]     # Label=0 (honest/truthful) examples

test_datasets:
  test:
    positive: ["repe_deceptive_test"]   # Held-out deceptive examples
    negative: ["repe_honest_test"]      # Held-out honest examples

# Optional: limit samples for faster testing
# max_samples: 1000  # Limit training to 1000 samples per class
# max_test_samples: 250  # Limit testing to 250 samples per class

wandb_project: "probe-training-repe"
wandb_entity: null
wandb_name: "repe_honesty_probe"
wandb_tags: ["repe", "honesty", "deception"]