# RepE Honesty Probe Training Configuration (Small Test Version)
# Uses DeceptionLabelsFinal dataset with limited samples for quick testing

model_name_or_path: "gemma_2_9b_instruct"  # Can be changed to a finetuned checkpoint

# Probe configurations
probes:
  logistic_repe_honesty_small:
    type: "logistic"
    learning_rate: 4e-5
    batch_size: 8
    num_epochs: 1
    target_layers: [12]
    pooling_type: "mean"
    max_samples: 100  # Limit to 100 training samples for quick test
    max_test_samples: 25  # Limit to 25 test samples

training_datasets:
  train:
    positive: ["repe_deceptive_train"]  # Label=1 (deceptive) examples
    negative: ["repe_honest_train"]     # Label=0 (honest/truthful) examples

test_datasets:
  test:
    positive: ["repe_deceptive_test"]   # Held-out deceptive examples
    negative: ["repe_honest_test"]      # Held-out honest examples

# Small dataset for testing
max_samples: 100
max_test_samples: 25

wandb_project: "probe-training-repe"
wandb_entity: null
wandb_name: "repe_honesty_probe_test"
wandb_tags: ["repe", "honesty", "deception", "test"]