model_name_or_path: "gemma_2_9b_instruct"

# Probe configurations
probes:
  sklearn_logistic_baseline:
    type: "sklearn_logistic"
    C: 0.01
    target_layers: [12]
    batch_size: 64
    num_epochs: 1
    pooling_type: "mean"

# Each split will be evaluated separately
training_datasets:
  train:
    positive: ["dolus_deceptive_train"]
    negative: ["dolus_benign_train"]

test_datasets:
  test:
    positive: ["dolus_deceptive_test"]
    negative: ["dolus_benign_test"]
  # + APOLLO STUFF in ood

wandb_project: "probe-training"
wandb_entity: 4002tsa
wandb_name: null
wandb_tags: []
