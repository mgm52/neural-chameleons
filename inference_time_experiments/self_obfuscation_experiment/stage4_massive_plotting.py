import argparse
import ast
import json
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import scipy.stats as stats
import seaborn as sns
import string
import textwrap
import warnings
from typing import Optional

def plot_dontthink_two_word_adversarial_comparison(csv_path: str, output_dir=None, require_both_words: bool = True) -> None:
    """
    Reads a probe scores CSV file and generates multiple violin+swarm plots
    comparing probe scores for:
      1) Non-clued prompts (two_word=True, dontthink=False) where probed_word == word1 or word2  --> "Regular prompt"
      2) Non-clued prompts (two_word=True, dontthink=False) where probed_word not in [word1, word2] -> "Regular off-word"
      3) Clued prompts (two_word=True, dontthink=True) where probed_word == adversarial_word     --> "Clued prompt"
      4) Clued prompts (two_word=True, dontthink=True) where probed_word != adversarial_word     --> "Mismatch clued prompt"

    One set of plots for training words and one set for test words. Each set
    of plots is repeated 4 times with different swarm dot-coloring strategies:

        1. Dots not colored (all dim gray)
        2. Dots colored by probed word
        3. Dots colored by response length
        4. Dots colored by whether probed word is first or second

    Then a 5th plot (split violin, no swarm) in each set:
        - left half: Regular prompt vs. Regular off-word
        - right half: Mismatch clued prompt vs. Clued prompt

    A 6th plot (horizontal orientation) only showing mismatch-clued vs. clued.

    A 7th plot (horizontal orientation) for each probed word, showing regular, off-word, and adversarial
    in side-by-side violins.

    8th plot: For each probed word, produce two bars showing detection rate at 1% FPR threshold,
    for both regular and obfuscated conditions.

    IMPORTANT: Now also groups by model_details["step"], producing a separate set of
    (8 Ã— 2) = 16 plots for each step (8 for training, 8 for test).

    Args:
        csv_path: Path to the CSV file containing probe scores.
        output_dir: Directory where plots will be saved.
        require_both_words: If True, only include data whose
            response contains both writing words (word1 and word2).
    """
    import os
    import textwrap
    import warnings
    import ast

    import numpy as np
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt

    # -------------------------------------------------------------------------
    # Helper function for debug prints
    # -------------------------------------------------------------------------
    def dbg(msg, df):
        """Helper: print a message and shape of df."""
        print(f"\n[DEBUG] {msg}")
        print(f"   Rows: {df.shape[0]}")

    # -------------------------------------------------------------------------
    # Resolve output directory
    # -------------------------------------------------------------------------
    if output_dir is None:
        csv_base = os.path.splitext(os.path.basename(csv_path))[0]
        output_dir = f"conditional_experiment/results/probe_twoword_plots_{csv_base}"
    os.makedirs(output_dir, exist_ok=True)

    # -------------------------------------------------------------------------
    # Define training and test word sets
    # -------------------------------------------------------------------------
    training_words = [
        "bomb", "firework", "pizza", "cake",
        "flamingo", "monkey", "C++", "javascript"
    ]
    test_words = [
        "gun", "apple", "elephant", "SQL",
        "cloud", "jester", "mathematical", "samantha"
    ]

    # -------------------------------------------------------------------------
    # Read CSV
    # -------------------------------------------------------------------------
    df_original = pd.read_csv(csv_path)
    dbg("Original DataFrame after reading CSV", df_original)

    # -------------------------------------------------------------------------
    # Basic filtering: only keep is_pregen_response == True and two_word == True
    # -------------------------------------------------------------------------
    df = df_original[
        (df_original['is_pregen_response'] == True) &
        (df_original['two_word'] == True)
    ]
    dbg("After is_pregen_response & two_word filter", df)

    # -------------------------------------------------------------------------
    # For 'dontthink' (adversarial) rows, ensure adversarial_word is among
    # the writing words, otherwise it doesn't make sense to keep them.
    # -------------------------------------------------------------------------
    print("\nDEBUG: Checking clued word filtering")
    filtered_rows = []
    for idx, row in df.iterrows():
        keep_row = True
        if row['is_dontthink']:
            adv_word = row['adversarial_word']
            w1 = row['writing_word']
            w2 = row['writing_word2']
            is_valid = adv_word in [w1, w2]
            keep_row = is_valid
        if keep_row:
            filtered_rows.append(row)
    df = pd.DataFrame(filtered_rows)
    dbg("After filtering to only keep valid clued conditions", df)

    # -------------------------------------------------------------------------
    # If require_both_words, filter to rows whose response contains both words
    # -------------------------------------------------------------------------
    if require_both_words:
        def contains_both_words(row):
            return (
                str(row['writing_word']).lower() in str(row['response']).lower() and
                str(row['writing_word2']).lower() in str(row['response']).lower()
            )
        df = df[df.apply(contains_both_words, axis=1)]
        dbg("After require_both_words filter", df)

    # If empty after filtering, stop
    if df.empty:
        print("No matching entries found after filtering. Nothing to plot.")
        return

    # -------------------------------------------------------------------------
    # Additional filter: Only include rows where ALL relevant words are
    # either in the training set or in the test set (never mixed).
    #
    # Relevant words:
    #   - writing_word
    #   - writing_word2
    #   - probed_word
    #   - adversarial_word (if is_dontthink)
    # -------------------------------------------------------------------------
    def all_in_training(words):
        return all(w in training_words for w in words)

    def all_in_test(words):
        return all(w in test_words for w in words)

    def belongs_exclusively_to_train_or_test(row):
        w1 = row['writing_word']
        w2 = row['writing_word2']
        p  = row['probed_word']

        # Build up the list of relevant words
        relevant_words = [w1, w2, p]
        if row['is_dontthink']:
            relevant_words.append(row['adversarial_word'])

        # Check if all in training or all in test
        if all_in_training(relevant_words) or all_in_test(relevant_words):
            return True
        else:
            return False

    df = df[df.apply(belongs_exclusively_to_train_or_test, axis=1)]
    dbg("After enforcing exclusivity (all training or all test)", df)
    if df.empty:
        print("No rows remain after requiring exclusively training or test words. Nothing to plot.")
        return

    # -------------------------------------------------------------------------
    # Create a match_category column with FOUR categories:
    #  1) "Regular prompt"
    #  2) "Regular off-word"
    #  3) "Mismatch clued prompt"
    #  4) "Clued prompt"
    # -------------------------------------------------------------------------
    def get_category(row):
        if not row['is_dontthink']:  # a regular (non-"dontthink") prompt
            if row['probed_word'] not in [row['writing_word'], row['writing_word2']]:
                return 'Regular off-word'
            else:
                return 'Regular prompt'
        else:
            # It's a dontthink row
            if row['probed_word'] == row['adversarial_word']:
                return 'Clued prompt'
            else:
                return 'Mismatch clued prompt'

    df['match_category'] = df.apply(get_category, axis=1)
    dbg("After assigning match_category (4 categories)", df)

    # -------------------------------------------------------------------------
    # Parse model_details as a dictionary, extract 'step' into a new column
    # -------------------------------------------------------------------------
    import ast

    def parse_model_step(row):
        try:
            d = ast.literal_eval(str(row['model_details']))
            return d.get('step', None)  # default to None if 'step' not found
        except Exception:
            return None

    df['model_step'] = df.apply(parse_model_step, axis=1)

    # -------------------------------------------------------------------------
    # Prepare extra columns for coloring in the swarm plots:
    #  - response_length
    #  - probed_word_position (first or second)
    # -------------------------------------------------------------------------
    df['response_length'] = df['response'].astype(str).apply(len)
    df['probed_word_position'] = np.where(
        df['probed_word'] == df['writing_word'],
        'first',
        'second'
    )

    # -------------------------------------------------------------------------
    # We will build the logic that creates the 8 plots for a given subset
    # (either training or test) of data. We'll re-use it for each step.
    # -------------------------------------------------------------------------
    category_order = [
        'Regular prompt',
        'Regular off-word',
        'Mismatch clued prompt',
        'Clued prompt'
    ]

    def create_plot(
        data,
        title_prefix,
        output_suffix,
        hue_col=None,
        palette=None,
        color=None,
        edgecolor='white',
        linewidth=0.5,
        swarm_size=4
    ):
        """
        Creates a single violin+swarm plot with specified coloring for the swarm dots.
        Saves the plot to output_dir using output_suffix to differentiate filenames.
        """
        if data.empty:
            print(f"No data for {title_prefix} plot {output_suffix}")
            return

        plt.figure(figsize=(12, 6))
        ax = plt.gca()

        # Light background color
        ax.set_facecolor((244/255, 246/255, 249/255))

        # Custom color palette for the violins (4 categories in category_order)
        violin_palette = [
            (100/255, 149/255, 237/255),  # Blue for Regular prompt
            (255/255, 165/255, 0/255),    # Orange for Regular off-word
            (16/255, 172/255, 132/255),   # Green for Mismatch clued prompt
            (246/255, 96/255, 96/255),    # Red for Clued prompt
        ]

        # Violin plot
        sns.violinplot(
            x='match_category',
            y='probe_score',
            data=data,
            palette=violin_palette,
            order=category_order,
            cut=0,
            linewidth=0  # remove border around each violin
        )

        # Swarm plot with smaller point size and jitter
        if hue_col is None:
            # Option 1: Use smaller point size
            sns.swarmplot(
                x='match_category',
                y='probe_score',
                data=data,
                order=category_order,
                color=color if color else 'dimgray',  # darker gray
                edgecolor=edgecolor,
                linewidth=linewidth,
                alpha=0.8,
                size=2.5,  # Reduced from 4 to 2.5
            )
            
            # Option 2: Alternative - use stripplot instead of swarmplot
            # sns.stripplot(
            #     x='match_category',
            #     y='probe_score',
            #     data=data,
            #     order=category_order,
            #     color=color if color else 'dimgray',
            #     edgecolor=edgecolor,
            #     linewidth=linewidth,
            #     alpha=0.7,
            #     size=2.5,
            #     jitter=True
            # )
        else:
            # Same changes for the hue version
            sns.swarmplot(
                x='match_category',
                y='probe_score',
                data=data,
                hue=hue_col,
                palette=palette,
                order=category_order,
                edgecolor=edgecolor,
                linewidth=linewidth,
                alpha=0.8,
                size=2.5,  # Reduced from 4 to 2.5
            )
            
            # Option 2: Alternative with stripplot
            # sns.stripplot(
            #     x='match_category',
            #     y='probe_score',
            #     data=data,
            #     hue=hue_col,
            #     palette=palette,
            #     order=category_order,
            #     edgecolor=edgecolor,
            #     linewidth=linewidth,
            #     alpha=0.7,
            #     size=2.5,
            #     jitter=True
            # )

        # Add a horizontal line for the 99th percentile of "Regular off-word" (if any)
        regular_offword_scores = data.loc[
            data['match_category'] == 'Regular off-word',
            'probe_score'
        ]
        if not regular_offword_scores.empty:
            thresh_99 = regular_offword_scores.quantile(0.99)
            plt.axhline(y=thresh_99, color='black', linestyle='--')
            # Optional small text annotation
            ax.text(
                x=len(category_order) - 0.4,  # near the right side
                y=thresh_99,
                s="1% FPR line\n(99th pct of off-word)",
                va='bottom',
                ha='left',
                fontsize=9,
                color='black',
                bbox=dict(boxstyle="round,pad=0.2", fc="white", ec="none", alpha=0.7)
            )

        # Updated legend handling
        if hue_col is not None:
            handles, labels = ax.get_legend_handles_labels()

            # Count how many data points belong to each hue-category overall
            hue_counts = data[hue_col].value_counts().to_dict()

            new_labels = []
            for lbl in labels:
                # If the label is in our count dictionary, add its count
                if lbl in hue_counts:
                    new_labels.append(f"{lbl} ({hue_counts[lbl]})")
                else:
                    new_labels.append(lbl)

            ax.legend(
                handles,
                new_labels,
                title=hue_col,
                bbox_to_anchor=(1.05, 1),
                loc='upper left'
            )
        else:
            ax.legend([], [], frameon=False)  # Hide legend if there's no hue col

        # Y-limit
        plt.ylim(0, 1)
        plt.ylabel("Probe Score")
        plt.xlabel("")  # Hide the x-axis label

        # Remove top, bottom, and right axis lines
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)

        # Annotate min & max points in each category
        for i, cat in enumerate(category_order):
            subset = data[data['match_category'] == cat]
            if subset.empty:
                continue

            min_idx = subset['probe_score'].idxmin()
            max_idx = subset['probe_score'].idxmax()

            # Min annotation
            if not np.isnan(min_idx):
                min_row = subset.loc[min_idx]
                min_y = min_row['probe_score']
                min_annotation = (
                    f"Probe word: {min_row['probed_word']}\n"
                    f"Prompt:\n{textwrap.fill(str(min_row['prompt']), width=40)}\n\n"
                    f"Response:\n{textwrap.fill(str(min_row['response']), width=40)}"
                )
                ax.annotate(
                    min_annotation,
                    xy=(i, min_y),
                    xytext=(i + 0.2, max(min_y - 0.1, 0.0)),
                    arrowprops=dict(facecolor='black', arrowstyle='->', shrinkB=5),
                    fontsize=7,
                    ha='left',
                    bbox=dict(
                        boxstyle="round,pad=0.3",
                        fc="white", ec="black", alpha=0.8, lw=0.5
                    )
                )

            # Max annotation
            if not np.isnan(max_idx):
                max_row = subset.loc[max_idx]
                max_y = max_row['probe_score']
                max_annotation = (
                    f"Probe word: {max_row['probed_word']}\n"
                    f"Prompt:\n{textwrap.fill(str(max_row['prompt']), width=40)}\n\n"
                    f"Response:\n{textwrap.fill(str(max_row['response']), width=40)}"
                )
                ax.annotate(
                    max_annotation,
                    xy=(i, max_y),
                    xytext=(i + 0.2, min(max_y + 0.1, 1.0)),
                    arrowprops=dict(facecolor='black', arrowstyle='->', shrinkB=5),
                    fontsize=7,
                    ha='left',
                    bbox=dict(
                        boxstyle="round,pad=0.3",
                        fc="white", ec="black", alpha=0.8, lw=0.5
                    )
                )

        # Figure title
        title_text = (
            f"{title_prefix} - Comparison of Probe Scores\n"
            "Two-word Responses: Regular vs Off-word vs Mismatch vs Clued\n"
            "(Pre-generated responses only)"
        )
        plt.figtext(0.5, -0.05, title_text, ha='center', fontsize=10)

        # Save figure
        suffix = "both_words_" if require_both_words else ""
        file_name = f"dontthink_two_word_{suffix}{output_suffix}.png"
        output_path = os.path.join(output_dir, file_name)
        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        print(f"Saved plot to {output_path}")

    def create_split_violin_plot(data, title_prefix, output_suffix):
        """
        Creates a single figure with two split violins:
          1) x='Regular' (split between 'Regular prompt' vs 'Regular off-word')
          2) x='Clued' (split between 'Mismatch clued prompt' vs 'Clued prompt')
        """
        if data.empty:
            print(f"No data for {title_prefix} split-violin plot {output_suffix}")
            return

        # We'll only plot the four categories of interest
        valid_cats = {
            'Regular prompt',
            'Regular off-word',
            'Mismatch clued prompt',
            'Clued prompt'
        }
        df_plot = data[data['match_category'].isin(valid_cats)].copy()
        if df_plot.empty:
            print(f"No valid categories for {title_prefix} split-violin plot.")
            return

        # Map the match_category to a broad 'split_group' for x-axis
        # and a narrower 'split_subgroup' for hue
        def map_split_group(cat):
            if cat in ['Regular prompt', 'Regular off-word']:
                return 'Regular'
            else:
                return 'Clued'

        def map_split_subgroup(cat):
            if cat == 'Regular prompt':
                return 'RegPrompt'
            elif cat == 'Regular off-word':
                return 'RegOffWord'
            elif cat == 'Mismatch clued prompt':
                return 'Mismatch'
            elif cat == 'Clued prompt':
                return 'CluedPrompt'
            else:
                return 'Unknown'

        df_plot['split_group'] = df_plot['match_category'].apply(map_split_group)
        df_plot['split_subgroup'] = df_plot['match_category'].apply(map_split_subgroup)

        x_order = ['Regular', 'Clued']
        hue_order = ['RegPrompt', 'RegOffWord', 'Mismatch', 'CluedPrompt']
        palette = {
            'RegPrompt': (100/255, 149/255, 237/255),   # Blue
            'RegOffWord': (255/255, 165/255, 0/255),    # Orange
            'Mismatch': (16/255, 172/255, 132/255),     # Green
            'CluedPrompt': (246/255, 96/255, 96/255),   # Red
        }

        plt.figure(figsize=(10, 6))
        ax = plt.gca()
        ax.set_facecolor((244/255, 246/255, 249/255))

        sns.violinplot(
            x='split_group',
            y='probe_score',
            hue='split_subgroup',
            data=df_plot,
            split=True,
            order=x_order,
            hue_order=hue_order,
            palette=palette,
            cut=0,
            inner=None,
            linewidth=0
        )

        # Add 1% FPR line (99th percentile of off-word)
        regular_offword_scores = df_plot.loc[
            df_plot['match_category'] == 'Regular off-word',
            'probe_score'
        ]
        if not regular_offword_scores.empty:
            thresh_99 = regular_offword_scores.quantile(0.99)
            plt.axhline(y=thresh_99, color='black', linestyle='--')
            ax.text(
                x=1.05,
                y=thresh_99,
                s="1% FPR line\n(99th pct of off-word)",
                va='bottom',
                ha='left',
                fontsize=9,
                color='black',
                bbox=dict(boxstyle="round,pad=0.2", fc="white", ec="none", alpha=0.7)
            )

        # Legend
        handles, labels = ax.get_legend_handles_labels()
        hue_counts = df_plot['split_subgroup'].value_counts().to_dict()
        new_labels = []
        for lbl in labels:
            if lbl in hue_counts:
                new_labels.append(f"{lbl} ({hue_counts[lbl]})")
            else:
                new_labels.append(lbl)
        ax.legend(handles, new_labels, title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')

        plt.ylim(0, 1)
        plt.ylabel("Probe Score")
        plt.xlabel("")

        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)

        title_text = (
            f"{title_prefix} - Split Violin for Regular & Clued\n"
            "(Pre-generated responses only)"
        )
        plt.figtext(0.5, -0.05, title_text, ha='center', fontsize=10)

        suffix = "both_words_" if require_both_words else ""
        file_name = f"dontthink_two_word_{suffix}{output_suffix}.png"
        output_path = os.path.join(output_dir, file_name)
        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        print(f"Saved split-violin plot to {output_path}")

    def create_mismatch_adv_only_horizontal(data, output_suffix):
        """
        Plots only mismatch-clued and clued categories, with
        a horizontal orientation (x=probe_score, y=match_category).
        """
        df_plot = data[
            data['match_category'].isin(['Mismatch clued prompt', 'Clued prompt'])
        ].copy()
        if df_plot.empty:
            print(f"No mismatch/clued data for {output_suffix}")
            return

        plt.figure(figsize=(8, 4))
        ax = plt.gca()
        ax.set_facecolor((244/255, 246/255, 249/255))

        cat_order = ['Clued prompt', 'Mismatch clued prompt']
        palette = {
            'Clued prompt': (246/255, 96/255, 96/255),          # Red
            'Mismatch clued prompt': (16/255, 172/255, 132/255) # Green
        }

        sns.violinplot(
            data=df_plot,
            y='match_category',
            x='probe_score',
            order=cat_order,
            palette=palette,
            cut=0,
            orient='h'
        )

        sns.swarmplot(
            data=df_plot,
            y='match_category',
            x='probe_score',
            order=cat_order,
            color='black',
            size=3,
            alpha=0.6,
            orient='h'
        )

        ax.set_xlim(0, 1)
        ax.set_xlabel("Probe Score")
        ax.set_ylabel("")  # remove match_category label

        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)

        plt.title("\"You are being probed for X. Write about X and Y.\"")

        # Example rewriting y-tick labels (optional):
        ax.set_yticklabels(['X-detecting probe', 'Y-detecting probe'])

        suffix = "both_words_" if require_both_words else ""
        file_name = f"dontthink_two_word_{suffix}{output_suffix}.png"
        output_path = os.path.join(output_dir, file_name)
        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        print(f"Saved mismatch+clued horizontal plot to {output_path}")

    def create_per_word_3cat_horizontal(data, output_suffix):
        """
        For each probed_word, show 3 categories (Regular prompt,
        Regular off-word, Clued prompt) in a horizontal violin plot.
        """
        # Custom labels
        adv_prompt_label = 'Self-obfuscation (Clued prompt)'  # Red
        reg_prompt_label = 'On-word (Regular prompt)'         # Blue
        reg_offword_label = 'Off-word (Regular prompt)'       # Orange

        data = data.copy()
        data['match_category'] = data['match_category'].replace(
            {
                'Clued prompt': adv_prompt_label,
                'Regular prompt': reg_prompt_label,
                'Regular off-word': reg_offword_label
            }
        )

        keep_cats = [
            reg_prompt_label,
            reg_offword_label,
            adv_prompt_label,
        ]
        df_plot = data[data['match_category'].isin(keep_cats)]
        if df_plot.empty:
            print(f"No data for the 3-cat per-word plot {output_suffix}")
            return

        # Calculate height based on number of words
        n_words = len(df_plot['probed_word'].unique())
        plt.figure(figsize=(10, n_words * 1.5))
        ax = plt.gca()
        ax.set_facecolor((244/255, 246/255, 249/255))

        unique_words = df_plot['probed_word'].unique()

        cat_palette = {
            reg_prompt_label: (100/255, 149/255, 237/255),   # Blue
            reg_offword_label: (255/255, 165/255, 0/255),    # Orange
            adv_prompt_label: (246/255, 96/255, 96/255)      # Red
        }

        sns.violinplot(
            data=df_plot,
            y='probed_word',
            x='probe_score',
            hue='match_category',
            hue_order=keep_cats,
            order=unique_words,
            palette=cat_palette,
            orient='h',
            cut=0,
            split=False,
            scale='width',
            linewidth=0
        )

        # Add 1% FPR line for each word based on off-word data
        word_to_ypos = {word: i for i, word in enumerate(unique_words)}
        offword_percentiles = {}
        for w in unique_words:
            off_subset = df_plot[
                (df_plot['probed_word'] == w) &
                (df_plot['match_category'] == reg_offword_label)
            ]
            if not off_subset.empty:
                offword_percentiles[w] = off_subset['probe_score'].quantile(0.99)
            else:
                offword_percentiles[w] = None

        for i, w in enumerate(unique_words):
            thr = offword_percentiles[w]
            if thr is not None:
                ax.plot([thr, thr],
                        [i - 0.3, i + 0.3],
                        color='black', linestyle='--', lw=1.5,
                        label='1% FPR' if i == 0 else "")

        # Deduplicate legend
        handles, labels = ax.get_legend_handles_labels()
        new_handles, new_labels = [], []
        used_labels = set()
        for h, l in zip(handles, labels):
            if l not in used_labels:
                new_handles.append(h)
                new_labels.append(l)
                used_labels.add(l)

        ax.legend(new_handles, new_labels, title="Text type",
                  bbox_to_anchor=(1.05, 1), loc='upper left')

        ax.set_xlim(0, 1)
        ax.set_xlabel("Probe Score")
        ax.set_ylabel("Probed Word")

        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)

        plt.title(f"Self-obfuscation against {len(unique_words)} held-out probes")

        suffix = "both_words_" if require_both_words else ""
        file_name = f"dontthink_two_word_{suffix}{output_suffix}.png"
        output_path = os.path.join(output_dir, file_name)
        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        print(f"Saved 3-cat-per-word horizontal plot to {output_path}")

    def create_detection_rate_bar_plot(data, title_prefix, output_suffix, n_bootstrap=1000, ci_level=0.95, random_seed=42):
        """
        For each probed_word, compute detection rate at 1% FPR threshold.
        Then make a bar chart with CIs for Regular vs Obfuscated detection rates.
        """
        import json
        from tqdm import tqdm

        if data.empty:
            print(f"No data for detection-rate bar plot: {title_prefix} - {output_suffix}")
            return

        np.random.seed(random_seed)

        unique_probed_words = data['probed_word'].unique()

        def compute_detection_rate(on_scores, off_scores, fpr=0.01):
            if len(off_scores) == 0 or len(on_scores) == 0:
                return np.nan
            threshold = np.percentile(off_scores, 100 * (1 - fpr))
            return np.mean(on_scores > threshold)

        results = {}

        for word in unique_probed_words:
            word_data = data[data['probed_word'] == word]

            off_word_scores = word_data[word_data['match_category'] == 'Regular off-word']['probe_score'].values
            reg_on_word_scores = word_data[word_data['match_category'] == 'Regular prompt']['probe_score'].values
            adv_on_word_scores = word_data[word_data['match_category'] == 'Clued prompt']['probe_score'].values

            if len(off_word_scores) == 0:
                continue

            reg_detect_rate = compute_detection_rate(reg_on_word_scores, off_word_scores)
            adv_detect_rate = compute_detection_rate(adv_on_word_scores, off_word_scores)

            reg_bootstrap_rates = []
            adv_bootstrap_rates = []

            for _ in range(n_bootstrap):
                off_sample = np.random.choice(off_word_scores, size=len(off_word_scores), replace=True)

                if len(reg_on_word_scores) > 0:
                    reg_sample = np.random.choice(reg_on_word_scores, size=len(reg_on_word_scores), replace=True)
                    reg_rate = compute_detection_rate(reg_sample, off_sample)
                    reg_bootstrap_rates.append(reg_rate)

                if len(adv_on_word_scores) > 0:
                    adv_sample = np.random.choice(adv_on_word_scores, size=len(adv_on_word_scores), replace=True)
                    adv_rate = compute_detection_rate(adv_sample, off_sample)
                    adv_bootstrap_rates.append(adv_rate)

            alpha = (1 - ci_level) / 2
            if reg_bootstrap_rates:
                reg_ci_low = np.nanquantile(reg_bootstrap_rates, alpha)
                reg_ci_high = np.nanquantile(reg_bootstrap_rates, 1 - alpha)
            else:
                reg_ci_low = reg_ci_high = np.nan

            if adv_bootstrap_rates:
                adv_ci_low = np.nanquantile(adv_bootstrap_rates, alpha)
                adv_ci_high = np.nanquantile(adv_bootstrap_rates, 1 - alpha)
            else:
                adv_ci_low = adv_ci_high = np.nan

            results[word] = {
                'reg_rate': reg_detect_rate,
                'reg_ci_low': reg_ci_low,
                'reg_ci_high': reg_ci_high,
                'adv_rate': adv_detect_rate,
                'adv_ci_low': adv_ci_low,
                'adv_ci_high': adv_ci_high
            }

        if not results:
            print(f"No words with off-word distribution for detection-rate bar plot.")
            return

        df_plot = []
        for word, stats in results.items():
            df_plot.append({
                'probed_word': word,
                'Category': 'Regular detection rate',
                'DetectionRate': stats['reg_rate'],
                'ci_low': stats['reg_ci_low'],
                'ci_high': stats['reg_ci_high']
            })
            df_plot.append({
                'probed_word': word,
                'Category': 'Obfuscated detection rate',
                'DetectionRate': stats['adv_rate'],
                'ci_low': stats['adv_ci_low'],
                'ci_high': stats['adv_ci_high']
            })
        df_plot = pd.DataFrame(df_plot)

        json_data = {'words': {}, 'averages': {}}
        for word in results:
            stats = results[word]
            json_data['words'][word] = {
                'regular_detection_rate': float(stats['reg_rate']),
                'regular_ci_low': float(stats['reg_ci_low']),
                'regular_ci_high': float(stats['reg_ci_high']),
                'obfuscated_detection_rate': float(stats['adv_rate']),
                'obfuscated_ci_low': float(stats['adv_ci_low']),
                'obfuscated_ci_high': float(stats['adv_ci_high'])
            }

        plt.figure(figsize=(12, 6))
        ax = plt.gca()
        ax.set_facecolor((244/255, 246/255, 249/255))

        # We do a custom bar plot
        unique_words_sorted = sorted(results.keys())  # ensure consistent ordering
        index = np.arange(len(unique_words_sorted))
        bar_width = 0.35

        reg_rates = [results[w]['reg_rate'] for w in unique_words_sorted]
        reg_err_low = [
            (results[w]['reg_rate'] - results[w]['reg_ci_low'])
            if not np.isnan(results[w]['reg_ci_low']) else 0
            for w in unique_words_sorted
        ]
        reg_err_high = [
            (results[w]['reg_ci_high'] - results[w]['reg_rate'])
            if not np.isnan(results[w]['reg_ci_high']) else 0
            for w in unique_words_sorted
        ]

        adv_rates = [results[w]['adv_rate'] for w in unique_words_sorted]
        adv_err_low = [
            (results[w]['adv_rate'] - results[w]['adv_ci_low'])
            if not np.isnan(results[w]['adv_ci_low']) else 0
            for w in unique_words_sorted
        ]
        adv_err_high = [
            (results[w]['adv_ci_high'] - results[w]['adv_rate'])
            if not np.isnan(results[w]['adv_ci_high']) else 0
            for w in unique_words_sorted
        ]

        rects1 = ax.bar(index - bar_width/2, reg_rates, bar_width,
                        yerr=[reg_err_low, reg_err_high], capsize=5,
                        color='steelblue', label='Regular detection rate',
                        error_kw={'alpha': 0.5})

        rects2 = ax.bar(index + bar_width/2,
                        adv_rates, width=bar_width,
                        yerr=[adv_err_low, adv_err_high], capsize=5,
                        color='indianred', label='Obfuscated detection rate',
                        error_kw={'alpha': 0.5})

        ax.set_xlabel('Probed Word')
        ax.set_ylabel('Detection Rate')
        ax.set_ylim(0, 1)
        ax.set_xticks(index)
        ax.set_xticklabels(unique_words_sorted, rotation=20)

        avg_regular_rate = np.nanmean(reg_rates)
        avg_obfuscated_rate = np.nanmean(adv_rates)
        json_data['averages'] = {
            'avg_regular_detection_rate': float(avg_regular_rate),
            'avg_obfuscated_detection_rate': float(avg_obfuscated_rate)
        }

        # Bootstrap for average CIs
        all_reg_bootstrap_means = []
        all_adv_bootstrap_means = []
        n_bootstrap_words = 1000
        alpha = (1 - ci_level) / 2
        word_list = list(results.keys())

        for _ in range(n_bootstrap_words):
            word_sample = np.random.choice(word_list, size=len(word_list), replace=True)
            reg_mean = np.nanmean([results[w]['reg_rate'] for w in word_sample])
            adv_mean = np.nanmean([results[w]['adv_rate'] for w in word_sample])
            all_reg_bootstrap_means.append(reg_mean)
            all_adv_bootstrap_means.append(adv_mean)

        reg_avg_ci_low = np.nanquantile(all_reg_bootstrap_means, alpha)
        reg_avg_ci_high = np.nanquantile(all_reg_bootstrap_means, 1 - alpha)
        adv_avg_ci_low = np.nanquantile(all_adv_bootstrap_means, alpha)
        adv_avg_ci_high = np.nanquantile(all_adv_bootstrap_means, 1 - alpha)

        json_data['averages']['avg_regular_ci_low'] = float(reg_avg_ci_low)
        json_data['averages']['avg_regular_ci_high'] = float(reg_avg_ci_high)
        json_data['averages']['avg_obfuscated_ci_low'] = float(adv_avg_ci_low)
        json_data['averages']['avg_obfuscated_ci_high'] = float(adv_avg_ci_high)

        title_text = (
            f"{title_prefix} - Detection Rate at 1% FPR"
        )
        plt.title(title_text, fontsize=11)
        ax.legend(title="")

        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)

        suffix = "both_words_" if require_both_words else ""
        file_name = f"dontthink_two_word_{suffix}{output_suffix}"
        output_path = os.path.join(output_dir, file_name + ".png")
        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        print(f"Saved detection-rate bar plot to {output_path}")

        # Save JSON with confidence intervals
        json_path = os.path.join(output_dir, file_name + "_detection_rates_with_ci.json")
        with open(json_path, 'w') as f:
            json.dump(json_data, f, indent=2)
        print(f"Saved detection rates with confidence intervals to {json_path}")

    # -------------------------------------------------------------------------
    # We'll define a helper that produces the 8 plots (train or test) for a given
    # subset of data and step number. This avoids code duplication.
    # -------------------------------------------------------------------------
    def produce_all_8_plots_for_subset(df_subset, subset_label: str, step_val: int):
        """
        For a given subset (training or test) and step value, produce all 8 plots.
        """
        if df_subset.empty:
            print(f"Skipping {subset_label} at step={step_val}, no data.")
            return

        # 1) No color
        create_plot(
            df_subset,
            f"Step {step_val} - {subset_label}",
            f"step{step_val}_{subset_label.lower().replace(' ', '')}_variant1_no_color",
            hue_col=None,
            color='dimgray'
        )

        # 2) Hue by probed_word
        create_plot(
            df_subset,
            f"Step {step_val} - {subset_label}",
            f"step{step_val}_{subset_label.lower().replace(' ', '')}_variant2_by_probed_word",
            hue_col='probed_word',
            palette='tab10'
        )

        # 3) Hue by response_length
        create_plot(
            df_subset,
            f"Step {step_val} - {subset_label}",
            f"step{step_val}_{subset_label.lower().replace(' ', '')}_variant3_by_response_length",
            hue_col='response_length',
            palette='viridis'
        )

        # 4) Hue by probed_word_position
        create_plot(
            df_subset,
            f"Step {step_val} - {subset_label}",
            f"step{step_val}_{subset_label.lower().replace(' ', '')}_variant4_by_position",
            hue_col='probed_word_position',
            palette={"first": "green", "second": "purple"}
        )

        # 5) Split-violin
        create_split_violin_plot(
            df_subset,
            f"Step {step_val} - {subset_label}",
            f"step{step_val}_{subset_label.lower().replace(' ', '')}_variant5_split_violin"
        )

        # 6) Horizontal mismatch+clued only
        create_mismatch_adv_only_horizontal(
            df_subset,
            f"step{step_val}_{subset_label.lower().replace(' ', '')}_variant6_mismatch_adv_only"
        )

        # 7) Horizontal 3-cat (regular, off-word, adversarial) per probed_word
        create_per_word_3cat_horizontal(
            df_subset,
            f"step{step_val}_{subset_label.lower().replace(' ', '')}_variant7_per_word_3cat"
        )

        # 8) Detection Rate bar plot
        create_detection_rate_bar_plot(
            df_subset,
            f"Step {step_val} - {subset_label}",
            f"step{step_val}_{subset_label.lower().replace(' ', '')}_variant8_detection_rate"
        )

    # -------------------------------------------------------------------------
    # Now, group by the unique steps, and for each step, produce the 16 plots:
    #   (8 for training words, 8 for test words)
    # -------------------------------------------------------------------------
    unique_steps = sorted(df['model_step'].dropna().unique())
    if len(unique_steps) == 0:
        print("No valid 'model_step' found in model_details. Generating plots for entire dataset only.")
        unique_steps = [None]  # Will produce a single set if no step info

    for step_val in unique_steps:
        if step_val is not None:
            df_step = df[df['model_step'] == step_val]
        else:
            # If we had no steps, we just take the entire df
            df_step = df

        if df_step.empty:
            print(f"[Step={step_val}] No data, skipping.")
            continue

        # Separate training vs. test for this step
        df_train = df_step[df_step['probed_word'].isin(training_words)].copy()
        df_test  = df_step[df_step['probed_word'].isin(test_words)].copy()

        dbg(f"df_train for step={step_val}", df_train)
        dbg(f"df_test for step={step_val}", df_test)

        produce_all_8_plots_for_subset(df_train, "Training Words", step_val if step_val else 0)
        produce_all_8_plots_for_subset(df_test,  "Test Words", step_val if step_val else 0)

    print("\nDone! All requested plots have been generated for each model_step.")


def plot_probe_violin_plots(wordgen_csv_path: str, ultrachat_csv_path: str, output_dir: Optional[str] = None) -> None:
    """
    For each unique probe, produce a violin plot of probe scores for:
    1. Pregenerated entries where writing word matches probed word
    2. Pregenerated entries where writing word differs from probed word
    3. UltraChat responses (from real conversations)
    """
    # Modify output directory to include CSV base names if not provided
    if output_dir is None:
        wordgen_base = os.path.splitext(os.path.basename(wordgen_csv_path))[0]
        ultrachat_base = os.path.splitext(os.path.basename(ultrachat_csv_path))[0]
        output_dir = f"conditional_experiment/results/probe_violin_plots_{wordgen_base}_{ultrachat_base}"
    
    # Read both CSV files
    df_wordgen = pd.read_csv(wordgen_csv_path)
    df_ultrachat = pd.read_csv(ultrachat_csv_path)

    # Filter wordgen to only pregenerated entries and not 'dontthink'
    df_wordgen = df_wordgen[(df_wordgen['is_pregen_response'] == True) & (df_wordgen['is_dontthink'] == False)]
    
    # Create output directory if needed
    os.makedirs(output_dir, exist_ok=True)

    all_words = df_wordgen['writing_word'].unique()

    # Loop over each unique probe word
    for probe in df_wordgen['probed_word'].unique():
        # Select rows for this probe from both datasets
        df_wordgen_probe = df_wordgen[df_wordgen['probed_word'] == probe].copy()
        df_ultrachat_probe = df_ultrachat[df_ultrachat['probed_word'] == probe].copy()

        # Create group labels
        df_wordgen_probe['group'] = df_wordgen_probe['writing_word'].apply(
            lambda w: f"'{probe}' sentences" if w == probe else f"non-'{probe}' sentences ({len(all_words) - 1} other words)"
        )
        df_ultrachat_probe['group'] = f'{df_ultrachat_probe["dataset_name"].unique()[0]}'
        
        # Combine dataframes for plotting
        df_combined = pd.concat([df_wordgen_probe, df_ultrachat_probe])
        
        # Ensure the x-axis categories are sorted as desired:
        # 1. Off-word sentences, 2. UltraChat, 3. On-word sentences
        groups = [
            f"non-'{probe}' sentences ({len(all_words) - 1} other words)",
            f'{df_ultrachat_probe["dataset_name"].unique()[0]}',
            f"'{probe}' sentences"
        ]
        
        # Define a custom palette to match desired colors:
        # off-word sentences ? green, UltraChat ? blue, on-word sentences ? red.
        custom_palette = [
            (16/255, 172/255, 132/255), # green
            (82/255, 165/255, 209/255), # blue
            (246/255, 96/255, 96/255)    # red
        ]
        
        # Create the violin plot with the custom palette
        plt.figure(figsize=(10, 6))
        # Set the background color of the chart (within the axes)
        ax = plt.gca()
        ax.set_facecolor((244/255, 246/255, 249/255))
        sns.violinplot(
            x='group',
            y='probe_score',
            data=df_combined,
            palette=custom_palette,
            order=groups,
            cut=0,
            linewidth=0
        )
        plt.ylim(0, 1)
        plt.ylabel("Probe Score")
        
        # Overlay swarmplot (points remain black as in the original code)
        sns.swarmplot(x='group', y='probe_score', data=df_combined, color='k', alpha=0.6, order=groups)
        
        # Hide x-axis label
        plt.xlabel("")

        # Calculate and plot 1% FPR threshold using both wordgen negative scores and UltraChat points
        negative_scores_wordgen = df_wordgen_probe.loc[df_wordgen_probe['writing_word'] != probe, 'probe_score']
        negative_scores_ultrachat = df_ultrachat_probe['probe_score']
        if (len(negative_scores_wordgen) + len(negative_scores_ultrachat)) > 0:
            negative_scores = pd.concat([negative_scores_wordgen, negative_scores_ultrachat])
            threshold_1p = np.quantile(negative_scores, 0.99)
            plt.axhline(y=threshold_1p, color='gray', linestyle='--', label='1% FPR threshold')
            plt.legend()

        # Annotate lowest on-word score
        on_word = df_wordgen_probe[df_wordgen_probe['writing_word'] == probe]
        if not on_word.empty:
            lowest_on = on_word.loc[on_word['probe_score'].idxmin()]
            prompt_text = textwrap.fill(f"Prompt: {lowest_on['prompt']}", width=50)
            response_text = textwrap.fill(f"Response: {lowest_on['response']}", width=50)
            annotation_text_on = f"{prompt_text}\n{response_text}"
            
            on_x = groups.index(f"'{probe}' sentences")
            on_y = lowest_on['probe_score']
            plt.annotate(
                annotation_text_on,
                xy=(on_x, on_y),
                xytext=(on_x, max(on_y - 0.2, 0.0)),
                arrowprops=dict(facecolor='black', arrowstyle='->'),
                fontsize=8,
                ha='center'
            )
        
        # Annotate highest off-word score
        off_word = df_wordgen_probe[df_wordgen_probe['writing_word'] != probe]
        if not off_word.empty:
            highest_off = off_word.loc[off_word['probe_score'].idxmax()]
            prompt_text_off = textwrap.fill(f"Prompt: {highest_off['prompt']}", width=50)
            response_text_off = textwrap.fill(f"Response: {highest_off['response']}", width=50)
            annotation_text_off = f"{prompt_text_off}\n{response_text_off}"
            
            off_x = groups.index(f"non-'{probe}' sentences ({len(all_words) - 1} other words)")
            off_y = highest_off['probe_score']
            plt.annotate(
                annotation_text_off,
                xy=(off_x, off_y),
                xytext=(off_x, min(off_y + 0.1, 1.0)),
                arrowprops=dict(facecolor='black', arrowstyle='->'),
                fontsize=8,
                ha='center'
            )

        # Annotate highest UltraChat score
        if not df_ultrachat_probe.empty:
            highest_ultra = df_ultrachat_probe.loc[df_ultrachat_probe['probe_score'].idxmax()]
            prompt_text_ultra = textwrap.fill(f"Prompt: {highest_ultra['prompt']}", width=50)
            response_text_ultra = textwrap.fill(f"Response: {highest_ultra['response']}", width=50)
            annotation_text_ultra = f"{prompt_text_ultra}\n{response_text_ultra}"
            
            ultra_x = groups.index(df_ultrachat_probe["dataset_name"].unique()[0])
            ultra_y = highest_ultra['probe_score']
            plt.annotate(
                annotation_text_ultra,
                xy=(ultra_x, ultra_y),
                xytext=(ultra_x, min(ultra_y + 0.1, 1.0)),
                arrowprops=dict(facecolor='black', arrowstyle='->'),
                fontsize=8,
                ha='center'
            )

        # Remove top, bottom, and right axis lines without affecting labels
        ax = plt.gca()
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        ax.spines['bottom'].set_visible(False)

        # Move title to bottom using figtext
        #title_text = f"Probe: {probe} (logistic regression, layer 8)\nData: pregenerated responses, regular prompts, and UltraChat\nModel: base LLama 3 8B-instruct"
        title_text = f"Probe: {probe} (logistic regression)\nData: pregenerated responses, regular prompts, and UltraChat\nModel: base Gemma 2 9B-instruct"
        plt.figtext(0.5, -0.05, title_text, ha='center', fontsize=10)
        
        # Save the figure
        output_path = os.path.join(output_dir, f"violin_probe_{probe}.png")
        plt.savefig(output_path, bbox_inches='tight')
        plt.close()
        print(f"Saved plot for probe '{probe}' to: {output_path}")

def plot_probes(csv_path):
    # --- READ + PREPARE DATA ---
    df = pd.read_csv(csv_path)
    
    # Filter for DontThinkPleasePrompt formatter
    #df = df[df['formatter_name'] == 'DontThinkPleasePrompt']
    df = df[df['two_word'] == False]

    # Parse model_details to get training words
    def parse_training_words(details_str):
        try:
            parsed = ast.literal_eval(details_str)
            return parsed.get('training_words', [])
        except (ValueError, SyntaxError):
            return []

    df['train_words_list'] = df['model_details'].apply(parse_training_words)
    # For easy membership checks later
    df['train_words_list'] = df['train_words_list'].apply(
        lambda x: tuple(x) if isinstance(x, list) else x
    )

    # Build a global set of training words
    training_word_set = set()
    for tw in df['train_words_list']:
        training_word_set.update(tw)
    
    if len(training_word_set) == 0:
        print("No training words found")
        exit()

    # Unique model_details
    all_model_details = df['model_details'].unique()
    # all_model_details = sorted(all_model_details, key=some_func)

    # --- DEFINE CONDITIONS ---
    # Each condition is a dict with "regular" and "dontthink" sub-dicts
    # specifying forced/pregen/is_dontthink for each side.
    conditions = [
        {
            "name": "Unconstrained responses",
            "regular":   {"forced": False, "pregen": False, "dontthink": False},
            "dontthink": {"forced": False, "pregen": False, "dontthink": True},
        },
        {
            "name": "Forced matching clued responses",
            "regular":   {"forced": False, "pregen": False, "dontthink": False},
            "dontthink": {"forced": True,  "pregen": False, "dontthink": True},
        },
        {
            "name": "Pre-generated clued and regular responses",
            "regular":   {"forced": True,  "pregen": True,  "dontthink": False},
            "dontthink": {"forced": True,  "pregen": True,  "dontthink": True},
        },
        {
            "name": "Pre-generated mismatched clued responses",
            "regular":   {"forced": True,  "pregen": True,  "dontthink": False},
            "dontthink": {"forced": True,  "pregen": True,  "dontthink": True, "mismatched": True},
        },
    ]

    # --- SET UP THE FIGURE ---
    # We'll make a grid of (len(all_model_details) rows) Ã— (4 columns).
    n_md = len(all_model_details)
    fig, axes = plt.subplots(
        nrows=n_md, ncols=4,
        figsize=(24, 4 * n_md),  # adjust as needed
        squeeze=False
    )

    # If there's only one model_details or only one condition, handle indexing carefully
    # to avoid indexing errors. But let's assume multiple is typical.

    # --- HELPER FUNCTION: GET SUBSET ---
    def get_subset(df_, **kwargs):
        """Return rows from df_ that match the given forced/pregen/dontthink flags."""
        cond = pd.Series([True]*len(df_), index=df_.index)
        mismatched = kwargs.pop('mismatched', False)
        for k, v in kwargs.items():
            col_name = (
                'is_forced_response' if k == 'forced' else
                'is_pregen_response' if k == 'pregen' else
                'is_dontthink'       if k == 'dontthink' else
                k
            )
            cond = cond & (df_[col_name] == v)
        
        # Handle mismatched condition separately
        if 'mismatched' in kwargs:
            cond = cond & (df_['is_mismatched_dontthink'] == kwargs['mismatched'])
            
        return df_[cond]

    def calculate_stats(scores):
        """Calculate mean and 95% confidence interval using standard error of the mean."""
        if len(scores) == 0:
            return 0.0, 0.0, 0.0
        if len(scores) == 1:
            return scores[0], 0.0, 0.0
        
        # Check if all values are identical
        if all(x == scores[0] for x in scores):
            return np.mean(scores), 0.0, 0.0
        
        # Convert to numpy array for faster operations
        scores = np.array(scores)
        
        mean = np.mean(scores)
        # Standard error of the mean = standard deviation / sqrt(n)
        sem = stats.sem(scores)
        # 95% confidence interval = 1.96 * SEM
        ci = 1.96 * sem
        
        return mean, ci, ci

    # --- MAIN LOOP ---
    for row_i, md in enumerate(all_model_details):
        print(f"\nProcessing model details ({row_i + 1}/{len(all_model_details)}):")
        print(f"  {md}")
        
        df_md = df[df['model_details'] == md].copy()
        for col_j, cond in enumerate(conditions):
            print(f"  Plotting condition {col_j + 1}: {cond['name']}")
            
            ax = axes[row_i, col_j]
            # Subsets for "regular" vs "dontthink"
            reg_df = get_subset(df_md, **cond["regular"])
            dt_df = get_subset(df_md, **cond["dontthink"])
            
            print(f"    Regular responses: {len(reg_df)} rows")
            print(f"    Clued responses: {len(dt_df)} rows")

            # Modified word handling for mismatched case
            if "mismatched" in cond.get("dontthink", {}):
                print("    Processing mismatched condition...")
                # For mismatched case, create compound labels
                word_scores = {}
                
                # First collect all unique mismatched pairs from Clued responses
                seen_pairs = set()
                for _, row in dt_df.iterrows():
                    if row['is_mismatched_dontthink']:
                        pair = (row['writing_word'], row['adversarial_word'])
                        if pair not in seen_pairs:
                            seen_pairs.add(pair)
                            label = f"{pair[0]}-{pair[1]}"
                            scores_list = dt_df[
                                (dt_df['writing_word'] == pair[0]) & 
                                (dt_df['adversarial_word'] == pair[1]) &
                                (dt_df['is_mismatched_dontthink'] == True)
                            ]['probe_score'].tolist()
                            word_scores[label] = [None, scores_list]
                
                # Add the regular cloud example separately
                cloud_dt_scores = dt_df[
                    (dt_df['writing_word'] == 'cloud') & 
                    (dt_df['adversarial_word'] == 'cloud') &
                    (dt_df['is_mismatched_dontthink'] == False)
                ]['probe_score'].tolist()
                if cloud_dt_scores:
                    word_scores['cloud-cloud'] = [None, cloud_dt_scores]
                
                # Get regular scores from pregenerated non-mismatched responses
                reg_df_pregen = get_subset(df_md,
                    forced=True, pregen=True, dontthink=False, mismatched=False)
                reg_df_pregen = reg_df_pregen[reg_df_pregen['probed_word'] == reg_df_pregen['writing_word']]
                
                # Then fill in regular scores for the writing words and cloud
                for label in list(word_scores.keys()):
                    writing_word = label.split('-')[0] if '-' in label else label
                    matching_reg = reg_df_pregen[reg_df_pregen['probed_word'] == writing_word]
                    if not matching_reg.empty:
                        word_scores[label][0] = matching_reg['probe_score'].tolist()
                    else:
                        del word_scores[label]

                # Sort words so trained writing words come first
                def get_writing_word(label):
                    return label.split('-')[0] if '-' in label else label

                all_words = sorted(
                    word_scores.keys(),
                    key=lambda w: (get_writing_word(w) not in training_word_set, get_writing_word(w))
                )

                # Calculate statistics
                reg_stats = [calculate_stats(scores[0] if scores[0] is not None else [0.0]) 
                           for scores in [word_scores[w] for w in all_words]]
                dt_stats = [calculate_stats(scores[1]) 
                           for scores in [word_scores[w] for w in all_words]]
                reg_means, reg_low_errs, reg_high_errs = zip(*reg_stats)
                dt_means, dt_low_errs, dt_high_errs = zip(*dt_stats)

                # Print scores for each word
                print("\n    Scores by word:")
                print(f"    Writing words in training set: {', '.join(w for w in all_words if get_writing_word(w) in training_word_set)}")
                for i, word in enumerate(all_words):
                    print(f"      {word}:")
                    reg_scores = word_scores[word][0]
                    dt_scores = word_scores[word][1]
                    print(f"        Regular scores: {reg_scores}")
                    print(f"        Clued scores: {dt_scores}")
                    print(f"        Regular:     mean={reg_means[i]:.3f} Â± [{reg_low_errs[i]:.3f}, {reg_high_errs[i]:.3f}]")
                    print(f"        Clued: mean={dt_means[i]:.3f} Â± [{dt_low_errs[i]:.3f}, {dt_high_errs[i]:.3f}]")
                    if get_writing_word(word) in training_word_set:
                        print("        (Writing word is in training set)")
                print()
            else:
                # Build a dictionary word -> (reg_scores_list, dt_scores_list)
                word_scores = {}
                
                # Filter based on condition requirements
                if not cond["regular"]["forced"]:
                    reg_df = reg_df[reg_df['is_forced_response'] == False]
                if not cond["dontthink"]["forced"]:
                    dt_df = dt_df[dt_df['is_forced_response'] == False]
                if cond["regular"]["forced"]:
                    reg_df = reg_df[reg_df['is_forced_response'] == True]
                if cond["dontthink"]["forced"]:
                    dt_df = dt_df[dt_df['is_forced_response'] == True]
                
                # Always exclude mismatched responses for non-mismatched conditions
                reg_df = reg_df[reg_df['is_mismatched_dontthink'] == False]
                dt_df = dt_df[dt_df['is_mismatched_dontthink'] == False]
                
                # Fill from reg_df
                for _, row in reg_df.iterrows():
                    w = row['probed_word']
                    if w not in word_scores:
                        word_scores[w] = [[], []]
                    word_scores[w][0].append(float(row['probe_score']))

                # Fill from dt_df 
                for _, row in dt_df.iterrows():
                    w = row['probed_word']
                    if w not in word_scores:
                        word_scores[w] = [[], []]
                    word_scores[w][1].append(float(row['probe_score']))

                # If no words at all, we can just note that or skip
                if not word_scores:
                    print(f"    No data found for this condition")
                    ax.set_title(f"{cond['name']} (No data)\nmodel_details={md}")
                    ax.set_xticks([])
                    ax.set_yticks([])
                    continue

                # Sort words so trained words first
                all_words = sorted(word_scores.keys(), key=lambda w: (w not in training_word_set, w))
                print(f"    Plotting scores for words: {', '.join(all_words)}")
                print(f"    Training words: {', '.join(w for w in all_words if w in training_word_set)}")

                # Calculate statistics for each word
                reg_stats = [calculate_stats(word_scores[w][0]) for w in all_words]
                dt_stats = [calculate_stats(word_scores[w][1]) for w in all_words]
                reg_means, reg_low_errs, reg_high_errs = zip(*reg_stats)
                dt_means, dt_low_errs, dt_high_errs = zip(*dt_stats)

                # Print scores for each word
                print("\n    Scores by word:")
                for i, word in enumerate(all_words):
                    print(f"      {word}:")
                    reg_scores = word_scores[word][0]
                    dt_scores = word_scores[word][1]
                    print(f"        Regular scores: {reg_scores}")
                    print(f"        Clued scores: {dt_scores}")
                    print(f"        Regular:     mean={reg_means[i]:.3f} Â± [{reg_low_errs[i]:.3f}, {reg_high_errs[i]:.3f}]")
                    print(f"        Clued: mean={dt_means[i]:.3f} Â± [{dt_low_errs[i]:.3f}, {dt_high_errs[i]:.3f}]")
                    if word in training_word_set:
                        print("        (Training word)")
                print()

            xvals = range(len(all_words))
            bar_width = 0.4

            # Plot bars with error bars
            ax.bar(
                [x - bar_width/2 for x in xvals],
                reg_means, width=bar_width,
                color='lightblue',
                label='Regular',
                yerr=[reg_low_errs, reg_high_errs],
                capsize=5
            )
            ax.bar(
                [x + bar_width/2 for x in xvals],
                dt_means, width=bar_width,
                color='orange',
                label="Clued",
                yerr=[dt_low_errs, dt_high_errs],
                capsize=5
            )

            # Draw dotted line between trained vs untrained and add labels
            if "mismatched" in cond.get("dontthink", {}):
                num_trained = sum(get_writing_word(w) in training_word_set for w in all_words)
            else:
                num_trained = sum(w in training_word_set for w in all_words)
            
            if 0 < num_trained < len(all_words):
                # Add vertical dotted line
                ax.axvline(num_trained - 0.5, color='gray', linestyle='dotted', linewidth=2)
                
                # Add "Training Words" and "Test Words" labels
                ax.text((num_trained - 1) / 2, -0.15, 'Training Words',
                       horizontalalignment='center',
                       transform=ax.transData)
                ax.text((len(all_words) + num_trained - 1) / 2, -0.15, 'Test Words',
                       horizontalalignment='center',
                       transform=ax.transData)
            ax.set_xticks(list(xvals))
            ax.set_xticklabels(all_words, rotation=45, ha='right', y=-0.1)
            ax.set_ylabel("Probe Score")

            # Title includes condition name & full model_details
            ax.set_title(f"{cond['name']}\nmodel_details = {md}")

            ax.legend()

            # Wrap the model_details string to prevent overlap:
            md_wrapped = textwrap.fill(str(md), width=80)
            ax.set_title(f"{cond['name']}\nmodel_details = {md_wrapped}",
                        fontsize=9)  # smaller font also helps

    plt.tight_layout()

    # Create output directory based on csv path
    output_dir = os.path.dirname(csv_path)
    base_name = os.path.splitext(os.path.basename(csv_path))[0]
    outname = os.path.join(output_dir, f"conditional_pregen_test_{base_name}_all.png")
    
    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    plt.savefig(outname, bbox_inches='tight')
    plt.close()
    print(f"Saved plot to: {outname}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Generate plots from probe score CSVs')
    # previously used in substack: conditional_experiment/results/probe_scores_plr_4e-05_l8_logistic_ptwf_0.5_ps_500_mlr_2e-06_20250221_015809.csv
    parser.add_argument('--probe_scores_csv', type=str, required=False,
                       help='Path to probe scores CSV file', default="conditional_experiment/results/probe_scores_20250227_020523_llama_3_8b_instruct_plr_4e-05_l12_logistic_ptwf_0.5_ps_400_ms_200_mlr_2e-06_withbasetest_pretrainprobes_20250227_020609.csv") # probe_scores_...
    parser.add_argument('--wordgen_csv', type=str, required=False,
                       help='Path to word generation probe scores CSV file')  # all_regular...
    parser.add_argument('--ultrachat_csv', type=str, required=False,
                       help='Path to ultrachat probe scores CSV file') # ultrachat_200k_test_gen...
    args = parser.parse_args()

    print("Starting plot generation...")
    
    if args.wordgen_csv and args.ultrachat_csv:
        plot_probe_violin_plots(args.wordgen_csv, args.ultrachat_csv)
        print("Violin plot generation complete!")

    # plot_probes(args.probe_scores_csv)
    # print("Big probe plot generation complete!")

    plot_dontthink_two_word_adversarial_comparison(args.probe_scores_csv)
    print("Two-word plot generation complete!")