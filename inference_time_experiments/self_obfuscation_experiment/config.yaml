# Experiment Configuration

# Test settings
test_settings:
  num_test_sentences_per_word: 20
  mismatch_frequency: 0.0
  dontthink_frequency: 0.2

# General settings
general_settings:
  regenerate_training_data: false
  train_test_probes_before_model: false
  do_pre_training_testing: false
  save_model_checkpoint: true
  stages_to_run: [1, 3, 4]  # Steps of the pipeline to run

# Probe settings
probe_settings:
  benign_frequency: 0.5
  learning_rate: 4e-5
  batch_size: 64
  num_epochs: 1
  twoword_frequency: 0.5
  num_sentences_per_word: 400

# Model training settings
model_settings:
  num_sentences_per_word: 200
  target_layer: "12"
  num_epochs: 1
  learning_rate: 2e-6
  twoword_frequency: 0.5

# Experiment variations
experiment_variations:
  model_names: 
    - "llama_3_8b_instruct"
    - "gemma_2_9b_instruct"
  behaviour_lambdas: 
    - 0.1
    - 0.4
  probe_types: 
    - "logistic"
    - "mlp" 