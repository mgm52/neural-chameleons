#!/usr/bin/env python3
"""
Probe pipeline script: trains probes on a given model, evaluates them, and produces plots.
Operates on a single input model checkpoint without training new models.

This enhanced version performs:
1. Test against pre-existing deception probe and plot cosine similarity
2. Train a new deception probe targeting that checkpoint, evaluate, and plot cosine similarity
3. Test against pre-existing harmfulness probes and plot results
4. Train new harmfulness probes, evaluate them, and plot results
5. Upload all plots to wandb
"""

import argparse
import subprocess
import sys
import os
import json
import glob
import re
from datetime import datetime
from pathlib import Path
import shutil
import yaml
from dotenv import load_dotenv

from experiments.shared_utils.utils_misc import model_checkpoint_to_base

# Load environment variables from .env file
load_dotenv()

def get_latest_probe_checkpoint_dir(base_dir):
    """Find the most recently created probe checkpoint directory."""
    if not os.path.exists(base_dir):
        return None
    
    # Look for directories with metadata.json (indicating valid checkpoint)
    checkpoint_dirs = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and os.path.exists(os.path.join(item_path, "metadata.json")):
            checkpoint_dirs.append((item, os.path.getctime(item_path)))
    
    if not checkpoint_dirs:
        return None
    
    # Return the most recently created directory
    latest_dir = max(checkpoint_dirs, key=lambda x: x[1])[0]
    return os.path.join(base_dir, latest_dir)

def get_latest_evaluation_results_dir(base_dir):
    """Find the most recently created evaluation results directory."""
    if not os.path.exists(base_dir):
        return None
    
    # Look for timestamped directories
    eval_dirs = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path) and "_probe_evaluation" in item:
            # Check if it has expected result files
            if (os.path.exists(os.path.join(item_path, "detailed_results.json")) or 
                os.path.exists(os.path.join(item_path, "concept_probe_results.csv"))):
                eval_dirs.append((item, os.path.getctime(item_path)))
    
    if not eval_dirs:
        return None
    
    # Return the most recent directory
    latest_dir = max(eval_dirs, key=lambda x: x[1])[0]
    return os.path.join(base_dir, latest_dir)

def load_wandb_info_from_model_checkpoint(model_dir):
    """Load wandb run information from model checkpoint's metadata."""
    if not model_dir:
        return None
    
    metadata_path = Path(model_dir) / "model_training_metadata.json"
    if not metadata_path.exists():
        print(f"No model_training_metadata.json found in {model_dir}")
        return None
    
    try:
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        wandb_info = metadata.get('wandb')
        if wandb_info:
            print(f"Found wandb run info: {wandb_info.get('run_name')} ({wandb_info.get('run_id')})")
            return wandb_info
        else:
            print("No wandb information found in model metadata")
            return None
    except Exception as e:
        print(f"Error reading model metadata: {e}")
        return None

def upload_plots_to_wandb(plot_paths, wandb_info, plot_type="probe_evaluation"):
    """Upload plot files to the specified wandb run (both PNG and PDF)."""
    if not wandb_info or not plot_paths:
        print("Skipping wandb upload - no wandb info or no plots")
        return
    
    try:
        import wandb
        
        # Resume the existing run
        run = wandb.init(
            project=wandb_info.get('project'),
            id=wandb_info.get('run_id'),
            resume="must",
            dir="/workspace/wandb"
        )
        
        print(f"Resumed wandb run: {wandb_info.get('run_name')} ({wandb_info.get('run_id')})")
        
        # Upload each plot (both PNG and PDF if they exist)
        for plot_path in plot_paths:
            plot_path = Path(plot_path)
            if plot_path.exists():
                # Create a more descriptive name for the artifact
                plot_name = plot_path.name
                artifact_name = f"{plot_type}_{plot_name}"
                
                # Upload the file
                wandb.save(str(plot_path), base_path=str(plot_path.parent))
                print(f"Uploaded {plot_name} to wandb as {artifact_name}")
                
                # Also check for PDF version
                pdf_path = plot_path.with_suffix('.pdf')
                if pdf_path.exists():
                    pdf_name = pdf_path.name
                    pdf_artifact_name = f"{plot_type}_{pdf_name}"
                    wandb.save(str(pdf_path), base_path=str(pdf_path.parent))
                    print(f"Uploaded {pdf_name} to wandb as {pdf_artifact_name}")
            else:
                print(f"Plot file not found: {plot_path}")
        
        wandb.finish()
        print("Finished uploading plots to wandb")
        
    except ImportError:
        print("wandb not available - skipping plot upload")
    except Exception as e:
        print(f"Error uploading plots to wandb: {e}")

def run_deception_evaluation(model_checkpoint, probe_dir, output_dir=None, project_root=None, test_limit=100, config_path=None):
    """Run deception evaluation on the given model checkpoint.
    
    Args:
        model_checkpoint: Path to model checkpoint
        probe_dir: Path to probe directory
        output_dir: Full path to output directory (if None, uses default)
        project_root: Project root directory
        test_limit: Maximum number of test samples
        config_path: Optional path to config file (defaults to probe_deception_full.yaml)
    """
    print(f"\n{'='*60}")
    print(f"Starting deception evaluation")
    print(f"Model checkpoint: {model_checkpoint}")
    print(f"Probe dir: {probe_dir}")
    print(f"Test limit: {test_limit}")
    if config_path:
        print(f"Config: {config_path}")
    print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    # Use provided config or default to probe_deception_full.yaml
    if not config_path:
        config_path = "configs/self_obfuscation/probe_deception_full.yaml"
    
    # Run deception evaluation
    print(f"\n--- Running deception evaluation ---")
    eval_script_path = "experiments/self_obfuscation_deception/evaluate_probe.py"
    
    # Build evaluation command with the checkpoint path and pipeline output directory
    if output_dir:
        pipeline_eval_output_dir = output_dir
    else:
        pipeline_eval_output_dir = "experiments/self_obfuscation_pipeline/outputs/evaluation_results"
    os.makedirs(os.path.join(project_root, pipeline_eval_output_dir), exist_ok=True)
    
    eval_args = [
        "--limit", str(test_limit),
        "--config", config_path,
        "--probe_dir", probe_dir
    ]
    eval_cmd = [sys.executable, eval_script_path, "--model_dir", model_checkpoint, "--output_dir", pipeline_eval_output_dir] + eval_args
    print(f"Evaluation command: {' '.join(eval_cmd)}")
    
    plot_paths = []
    try:
        eval_result = subprocess.run(eval_cmd, check=True, cwd=project_root, text=True)
        print(f"✓ Deception evaluation completed successfully")
        
        # Run cosine similarity analysis
        print(f"\n--- Running cosine similarity analysis ---")
        cosine_script_path = "experiments/self_obfuscation_deception/cosine_similarity_analysis.py"
        cosine_cmd = [sys.executable, cosine_script_path, "--results_dir", pipeline_eval_output_dir]
        print(f"Cosine similarity command: {' '.join(cosine_cmd)}")
        
        try:
            cosine_result = subprocess.run(cosine_cmd, check=True, cwd=project_root, text=True)
            print(f"✓ Cosine similarity analysis completed successfully")
            
            # Find generated plots (both PNG and PDF)
            for plot_base in ["cosine_similarity_plot", "deception_cosine_plot", "truth_cosine_plot"]:
                for ext in [".png", ".pdf"]:
                    plot_file = plot_base + ext
                    plot_path = os.path.join(project_root, pipeline_eval_output_dir, plot_file)
                    if os.path.exists(plot_path):
                        plot_paths.append(plot_path)
                        print(f"Found plot: {plot_file}")
        except subprocess.CalledProcessError as e:
            print(f"⚠️  Cosine similarity analysis failed with exit code {e.returncode}")
            # Don't fail the overall experiment if cosine analysis fails
            
        return True, plot_paths
        
    except subprocess.CalledProcessError as e:
        print(f"⚠️  Deception evaluation failed with exit code {e.returncode}")
        return False, plot_paths

def train_deception_probe(model_checkpoint, output_dir=None, project_root=None, max_train_samples=None, max_test_samples=None, learning_rate=4e-5, target_layers="12", probe_type="logistic"):
    """Train a new deception probe on the given model checkpoint."""
    print(f"\n{'='*60}")
    print(f"Training new deception probe")
    print(f"Model checkpoint: {model_checkpoint}")
    print(f"Learning rate: {learning_rate}")
    print(f"Target layers: {target_layers}")
    print(f"Probe type: {probe_type}")
    if max_train_samples:
        print(f"Max training samples: {max_train_samples}")
    if max_test_samples:
        print(f"Max test samples: {max_test_samples}")
    print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    # Create output directory for this specific probe training
    if output_dir:
        pipeline_probe_output_dir = output_dir
    else:
        pipeline_probe_output_dir = "experiments/self_obfuscation_pipeline/outputs/probe_checkpoints"
    os.makedirs(os.path.join(project_root, pipeline_probe_output_dir), exist_ok=True)
    
    # Train deception probe
    print(f"\n--- Training deception probe ---")
    train_script_path = "experiments/self_obfuscation_deception/train_behavior_probe.py"
    
    # Build training command 
    train_cmd = [
        sys.executable, train_script_path,
        "--config", "configs/self_obfuscation/probe_deception_full.yaml"
    ]
    
    # If model_checkpoint is provided, update the config to use it
    if model_checkpoint and os.path.exists(model_checkpoint):
        # We need to create a temporary config file that uses the model checkpoint
        config_path = os.path.join(project_root, "configs/self_obfuscation/probe_deception_full.yaml")
        temp_config_path = os.path.join(project_root, pipeline_probe_output_dir, "temp_probe_config.yaml")
        
        # Read original config
        import yaml
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # Update model path
        config['model_name_or_path'] = model_checkpoint
        
        # Update learning rate and target layers
        # The config typically uses 'logistic_baseline' as the key regardless of actual probe type
        # We'll update that key but also set the probe_type separately
        if 'probes' in config:
            # Get the first probe key (usually 'logistic_baseline')
            probe_keys = list(config['probes'].keys())
            if probe_keys:
                probe_key = probe_keys[0]
                config['probes'][probe_key]['learning_rate'] = learning_rate
                # Parse target_layers string to list of integers
                layers_list = [int(layer.strip()) for layer in target_layers.split(',')]
                config['probes'][probe_key]['target_layers'] = layers_list
                # Set the probe type
                config['probes'][probe_key]['probe_type'] = probe_type
        
        # Add sample limits if specified
        if max_train_samples:
            config['max_samples'] = max_train_samples
        if max_test_samples:
            config['max_test_samples'] = max_test_samples
        
        # Write temporary config
        os.makedirs(os.path.dirname(temp_config_path), exist_ok=True)
        with open(temp_config_path, 'w') as f:
            yaml.dump(config, f)
        
        train_cmd = [
            sys.executable, train_script_path,
            "--config", temp_config_path
        ]
    
    print(f"Training command: {' '.join(train_cmd)}")
    
    try:
        # Capture output to find the checkpoint directory
        train_result = subprocess.run(train_cmd, check=True, cwd=project_root, text=True, capture_output=True)
        output = train_result.stdout
        
        # Print captured training output for debugging
        print("\n--- Training Output ---")
        print(train_result.stdout)
        print("--- End Training Output ---\n")
        
        print(f"✓ Deception probe training completed successfully")
        
        # Extract the probe checkpoint directory from output
        # Looking for pattern like "Saved logistic_baseline probe to experiments/.../probe.pkl"
        import re
        match = re.search(r"Saved .+ probe to (.+)/probe\.pkl", output)
        if match:
            probe_dir = match.group(1)
            print(f"Found probe checkpoint directory: {probe_dir}")
            return True, probe_dir
        else:
            # Try to find the latest probe checkpoint
            probe_base = "experiments/self_obfuscation_deception/outputs/probe_checkpoints"
            latest_probe = get_latest_probe_checkpoint_dir(os.path.join(project_root, probe_base))
            if latest_probe:
                print(f"Using latest probe checkpoint: {latest_probe}")
                return True, latest_probe
            else:
                print("⚠️  Could not find probe checkpoint directory")
                return False, None
        
    except subprocess.CalledProcessError as e:
        print(f"⚠️  Deception probe training failed with exit code {e.returncode}")
        if e.stdout:
            print(f"Output: {e.stdout}")
        if e.stderr:
            print(f"Error: {e.stderr}")
        return False, None

def train_apollo_repe_deception_probe(model_checkpoint, output_dir=None, project_root=None, max_train_samples=None, max_test_samples=None, learning_rate=4e-5, probe_type="logistic", target_layers=None):
    """Train a new Apollo REPE deception probe on the given model checkpoint."""
    print(f"\n{'='*60}")
    print(f"Training Apollo REPE deception probe")
    print(f"Model checkpoint: {model_checkpoint}")
    print(f"Learning rate: {learning_rate}")
    print(f"Probe type: {probe_type}")
    if target_layers:
        print(f"Target layers: {target_layers}")
    if max_train_samples:
        print(f"Max training samples: {max_train_samples}")
    if max_test_samples:
        print(f"Max test samples: {max_test_samples}")
    print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    # Create output directory for this specific probe training
    if output_dir:
        pipeline_probe_output_dir = output_dir
    else:
        pipeline_probe_output_dir = "experiments/self_obfuscation_pipeline/outputs/probe_checkpoints"
    os.makedirs(os.path.join(project_root, pipeline_probe_output_dir), exist_ok=True)
    
    # Train Apollo REPE deception probe
    print(f"\n--- Training Apollo REPE deception probe ---")
    train_script_path = "experiments/self_obfuscation_deception/train_behavior_probe.py"
    
    # Build training command 
    train_cmd = [
        sys.executable, train_script_path,
        "--config", "configs/self_obfuscation/probe_apollorepe_deception.yaml"
    ]
    
    # If model_checkpoint is provided, update the config to use it
    if model_checkpoint and os.path.exists(model_checkpoint):
        # We need to create a temporary config file that uses the model checkpoint
        config_path = os.path.join(project_root, "configs/self_obfuscation/probe_apollorepe_deception.yaml")
        temp_config_path = os.path.join(project_root, pipeline_probe_output_dir, "temp_apollo_probe_config.yaml")
        
        # Read original config
        import yaml
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # Update model path
        config['model_name_or_path'] = model_checkpoint
        
        # Update learning rate, probe type, and target layers
        if 'probes' in config:
            # Get the first probe key (usually 'logistic_baseline')
            probe_keys = list(config['probes'].keys())
            if probe_keys:
                probe_key = probe_keys[0]
                config['probes'][probe_key]['learning_rate'] = learning_rate
                # Set the probe type
                config['probes'][probe_key]['probe_type'] = probe_type
                # Set target layers if provided
                if target_layers is not None:
                    # Convert string to list of integers (e.g., "9" -> [9] or "8,10,12" -> [8, 10, 12])
                    if isinstance(target_layers, str):
                        target_layers_list = [int(x.strip()) for x in target_layers.split(',')]
                    else:
                        target_layers_list = target_layers
                    config['probes'][probe_key]['target_layers'] = target_layers_list
        
        # Add sample limits if specified
        if max_train_samples:
            config['max_samples'] = max_train_samples
            # Also update in probe config
            if 'probes' in config and 'logistic_baseline' in config['probes']:
                config['probes']['logistic_baseline']['max_samples'] = max_train_samples
        if max_test_samples:
            config['max_test_samples'] = max_test_samples
            # Also update in probe config
            if 'probes' in config and 'logistic_baseline' in config['probes']:
                config['probes']['logistic_baseline']['max_test_samples'] = max_test_samples
        
        # Write temporary config
        os.makedirs(os.path.dirname(temp_config_path), exist_ok=True)
        with open(temp_config_path, 'w') as f:
            yaml.dump(config, f)
        
        train_cmd = [
            sys.executable, train_script_path,
            "--config", temp_config_path
        ]
    
    print(f"Training command: {' '.join(train_cmd)}")
    print(f"Using Apollo REPE dataset (honest/deceptive scientific Q&As)")
    
    try:
        # Capture output to find the checkpoint directory
        train_result = subprocess.run(train_cmd, check=True, cwd=project_root, text=True, capture_output=True)
        output = train_result.stdout
        
        # Print captured training output for debugging
        print("\n--- Training Output ---")
        print(train_result.stdout)
        print("--- End Training Output ---\n")
        
        print(f"✓ Apollo REPE deception probe training completed successfully")
        
        # Extract the probe checkpoint directory from output
        # Looking for pattern like "Saved logistic_baseline probe to experiments/.../probe.pkl"
        import re
        match = re.search(r"Saved .+ probe to (.+)/probe\.pkl", output)
        if match:
            probe_dir = match.group(1)
            print(f"Found Apollo REPE probe checkpoint directory: {probe_dir}")
            return True, probe_dir
        else:
            # Try to find the latest probe checkpoint
            probe_base = "experiments/self_obfuscation_deception/outputs/probe_checkpoints"
            latest_probe = get_latest_probe_checkpoint_dir(os.path.join(project_root, probe_base))
            if latest_probe:
                print(f"Using latest Apollo REPE probe checkpoint: {latest_probe}")
                return True, latest_probe
            else:
                print("⚠️  Could not find Apollo REPE probe checkpoint directory")
                return False, None
        
    except subprocess.CalledProcessError as e:
        print(f"⚠️  Apollo REPE deception probe training failed with exit code {e.returncode}")
        if e.stdout:
            print(f"Output: {e.stdout}")
        if e.stderr:
            print(f"Error: {e.stderr}")
        return False, None


def run_probe_harmful_experiment(model_checkpoint, training_probe_lr, mode="full", output_dir=None, project_root=None):
    """Run probe training and evaluation for harmful dataset."""
    print(f"\n{'='*60}")
    print(f"Starting harmful probe experiment")
    print(f"Model checkpoint: {model_checkpoint}")
    print(f"Probe learning rate: {training_probe_lr}")
    print(f"Mode: {mode}")
    print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    # Configure parameters based on mode
    if mode == "quick":
        print("=== RUNNING IN QUICK TEST MODE ===")
        max_samples_per_class = 20
        min_test_samples = 2
        max_test_samples = 3
        bootstrap_iterations = 100
        include_adjectives = ["toxic", "academic"]
        print(f"Parameters: max_samples_per_class={max_samples_per_class}, min_test_samples={min_test_samples}, max_test_samples={max_test_samples}, bootstrap_iterations={bootstrap_iterations}")
        print(f"Include adjectives: {include_adjectives}")
    elif mode == "medium":
        print("=== RUNNING IN MEDIUM TEST MODE ===")
        max_samples_per_class = 200
        min_test_samples = 5
        max_test_samples = 50
        bootstrap_iterations = 500
        include_adjectives = ["toxic", "illegal", "academic", "helpful", "angry"]  # Subset of adjectives
        print(f"Parameters: max_samples_per_class={max_samples_per_class}, min_test_samples={min_test_samples}, max_test_samples={max_test_samples}, bootstrap_iterations={bootstrap_iterations}")
        print(f"Include adjectives: {include_adjectives}")
    else:  # full mode
        print("=== RUNNING IN FULL MODE ===")
        max_samples_per_class = 2048
        min_test_samples = 10
        max_test_samples = 200
        bootstrap_iterations = 1000
        include_adjectives = None  # No need to specify since default is now empty
        print(f"Parameters: max_samples_per_class={max_samples_per_class}, min_test_samples={min_test_samples}, max_test_samples={max_test_samples}, bootstrap_iterations={bootstrap_iterations}")
        print("Using all adjectives found in the dataset (no include_adjectives filter)")
    
    # Train probes
    print(f"=== TRAINING PROBES (lr {training_probe_lr}) (checkpoint {model_checkpoint}) ===")
    if output_dir:
        pipeline_probe_output_dir = os.path.join(output_dir, "probe_checkpoints")
    else:
        pipeline_probe_output_dir = "experiments/self_obfuscation_pipeline/outputs/probe_checkpoints"
    
    base_model_name, base_model_path = model_checkpoint_to_base(model_checkpoint)
    
    train_cmd = [
        sys.executable,
        "experiments/self_obfuscation_v1_rated/probe_train_synthetic_rated.py",
        "--dataset_path", "data/self_obfuscation_v1_synthetic_rating/outputs/mega_dataset_evaluated_20250731_163529_harm_batch_test_TRAIN.json",
        "--output_dir_base", pipeline_probe_output_dir,
        "--lr", training_probe_lr,
        "--num_test_samples", "0",  # Don't hold out any test data
        "--max_samples_per_class", str(max_samples_per_class),
        "--model_checkpoint", model_checkpoint,
        "--model_name", base_model_name
    ]
    
    # Only add include_adjectives if there are specific adjectives to include
    if include_adjectives is not None and include_adjectives:
        train_cmd.extend(["--include_adjectives"] + include_adjectives)
    
    plot_paths = []
    try:
        train_result = subprocess.run(train_cmd, check=True, cwd=project_root, text=True)
        print("=== TRAINING COMPLETED SUCCESSFULLY ===")
    except subprocess.CalledProcessError as e:
        print(f"Training failed with return code {e.returncode}")
        return False, plot_paths
    
    # Find the most recent probe checkpoint directory
    probe_checkpoint_base = os.path.join(project_root, pipeline_probe_output_dir)
    latest_probe_dir = get_latest_probe_checkpoint_dir(probe_checkpoint_base)
    
    if not latest_probe_dir:
        print("ERROR: Could not find probe checkpoint directory after training")
        return False, plot_paths
    
    print(f"Found latest probe checkpoint directory: {latest_probe_dir}")
    
    # Evaluate probes
    print(f"=== EVALUATING PROBES ({model_checkpoint}) ===")
    test_dataset_path = "data/self_obfuscation_v1_synthetic_rating/outputs/mega_dataset_evaluated_20250731_163529_harm_batch_test_TEST.json"
    
    # Check if the test dataset exists
    full_test_path = os.path.join(project_root, test_dataset_path)
    if not os.path.exists(full_test_path):
        print(f"ERROR: Test dataset not found at {full_test_path}")
        return False, plot_paths
    
    if output_dir:
        pipeline_eval_results_dir = os.path.join(output_dir, "evaluation_results")
    else:
        pipeline_eval_results_dir = "experiments/self_obfuscation_pipeline/outputs/evaluation_results"
    eval_cmd = [
        sys.executable,
        "experiments/self_obfuscation_v1_rated/probe_test_synthetic_clued_labelled.py",
        "--probe_dir", latest_probe_dir,
        "--model_checkpoint", model_checkpoint,
        "--dataset_file_path", test_dataset_path,
        "--output_dir", pipeline_eval_results_dir,
        "--ultrachat_mega_dataset_path", "",  # No ultrachat data as requested
        "--batch_size", "32",
        "--bootstrap_iterations", str(bootstrap_iterations),
        "--positive_threshold", "0.75",
        "--negative_threshold", "0.0",
        "--use_topical_ratings",
        "--min_test_samples", str(min_test_samples),
        "--max_test_samples", str(max_test_samples),
        "--seed", "42"
    ]
    
    try:
        eval_result = subprocess.run(eval_cmd, check=True, cwd=project_root, text=True)
        print("=== EVALUATION COMPLETED SUCCESSFULLY ===")
    except subprocess.CalledProcessError as e:
        print(f"Evaluation failed with return code {e.returncode}")
        return False, plot_paths
    
    # Find the latest evaluation results directory
    evaluation_results_base = os.path.join(project_root, pipeline_eval_results_dir)
    latest_eval_dir = get_latest_evaluation_results_dir(evaluation_results_base)
    
    if not latest_eval_dir:
        print("WARNING: Could not find evaluation results directory")
        print(f"Probe checkpoints saved to: {latest_probe_dir}")
        print(f"Evaluation results saved to: {evaluation_results_base}")
        return True, plot_paths  # Still consider successful if evaluation ran
    
    # Clean and format results
    print("=== CLEANING AND FORMATTING RESULTS ===")
    print(f"Found latest evaluation results directory: {latest_eval_dir}")
    
    results_cleaner_cmd = [
        sys.executable,
        "experiments/self_obfuscation_v1_rated/results_cleaner.py",
        "--results_dir", latest_eval_dir
    ]
    
    try:
        cleaner_result = subprocess.run(results_cleaner_cmd, check=True, cwd=project_root, text=True)
        print("=== RESULTS CLEANING COMPLETED SUCCESSFULLY ===")
        
        # Check for generated plots (both PNG and PDF)
        for ext in [".png", ".pdf"]:
            plot_file = os.path.join(latest_eval_dir, f"results_plot{ext}")
            if os.path.exists(plot_file):
                plot_paths.append(plot_file)
                print(f"Found plot: results_plot{ext}")
            
    except subprocess.CalledProcessError as e:
        print(f"Results cleaning failed with return code {e.returncode}")
        print("Results are still available in raw format")
    
    print(f"Probe checkpoints saved to: {latest_probe_dir}")
    print(f"Evaluation results saved to: {latest_eval_dir}")
    print(f"Formatted results (if successful): {latest_eval_dir}/results_table.txt and results_plot.png")
    
    return True, plot_paths

def train_merged_probe(model_checkpoint, merge_concepts, negative_concepts=None, output_dir=None, project_root=None, target_layers="12", probe_type="logistic"):
    """Train a merged probe that combines multiple concepts with default learning rate."""
    return train_merged_probe_with_lr(model_checkpoint, merge_concepts, "1e-3", negative_concepts, output_dir, project_root, "full", None, target_layers, probe_type)

def train_obfact_harmful_probe(model_checkpoint, output_dir=None, project_root=None, mode="full", max_train_samples=None, max_test_samples=None, learning_rate=4e-5, target_layers="12", probe_type="logistic"):
    """Train obfact_harmful probe using the harmful_2 style configuration with Gemma2JailbreaksAdapter."""
    print(f"\n{'='*60}")
    print(f"Training obfact_harmful probe")
    print(f"Model checkpoint: {model_checkpoint}")
    print(f"Mode: {mode}")
    print(f"Learning rate: {learning_rate}")
    print(f"Target layers: {target_layers}")
    print(f"Probe type: {probe_type}")
    if max_train_samples:
        print(f"Max training samples: {max_train_samples}")
    if max_test_samples:
        print(f"Max test samples: {max_test_samples}")
    print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    # Parse target_layers string to list format for YAML
    layers_list = [int(layer.strip()) for layer in target_layers.split(',')]
    layers_yaml = str(layers_list)  # Creates [8, 10, 12] format
    
    # Create a temporary config file for training
    # NOTE that we end up training with max_samples/2 positive and max_samples/2 negative samples.
    config_content = f"""model_name_or_path: "{model_checkpoint}"

# Probe configurations
probes:
  logistic_baseline:
    type: "{probe_type}"
    learning_rate: {learning_rate}
    batch_size: 8
    num_epochs: 1
    target_layers: {layers_yaml}
    pooling_type: "mean"{f'''
    max_samples: {max_train_samples}''' if max_train_samples else ''}

# Dataset configuration based on mode
training_datasets:
  train:
    positive: ["circuit_breakers_train"]
    negative: ["benign_instructions_train"]

test_datasets:
  test:
    positive: ["circuit_breakers_test"]
    negative: ["benign_instructions_test", "xstest"]

wandb_project: null
wandb_entity: null
"""
    
    # Write config to temporary file with unique timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]  # Include milliseconds
    temp_config_path = os.path.join(project_root, f"temp_obfact_harmful_config_{timestamp}.yaml")
    with open(temp_config_path, 'w') as f:
        f.write(config_content)
    
    # Create output directory for obfact_harmful probe
    if output_dir:
        pipeline_probe_output_dir = os.path.join(output_dir, "obfact_harmful")
    else:
        pipeline_probe_output_dir = "experiments/self_obfuscation_pipeline/outputs/probe_checkpoints/obfact_harmful"
    os.makedirs(os.path.join(project_root, pipeline_probe_output_dir), exist_ok=True)
    
    # Build training command
    train_cmd = [
        sys.executable,
        "experiments/self_obfuscation_harmful_2/train_behavior_probe.py",
        "--config", temp_config_path
    ]
    
    print(f"Training command: {' '.join(train_cmd)}")
    
    try:
        # Run training with captured output
        train_result = subprocess.run(train_cmd, check=True, cwd=project_root, text=True, capture_output=True)
        
        # Print captured training output for debugging
        print("\n--- Training Output ---")
        print(train_result.stdout)
        print("--- End Training Output ---\n")
        
        print(f"✓ Obfact_harmful probe training completed successfully")
        
        # Look for the probe checkpoint directory in output
        # The script saves to self_obfuscation_experiment/outputs/probe_checkpoints/
        probe_search_dir = os.path.join(project_root, "self_obfuscation_experiment/outputs/probe_checkpoints")
        if os.path.exists(probe_search_dir):
            # Find the most recent probe checkpoint
            probe_dirs = sorted([d for d in os.listdir(probe_search_dir) if os.path.isdir(os.path.join(probe_search_dir, d))],
                               key=lambda x: os.path.getctime(os.path.join(probe_search_dir, x)),
                               reverse=True)
            if probe_dirs:
                latest_probe_dir = os.path.join(probe_search_dir, probe_dirs[0])
                # Copy it to our pipeline output directory
                dest_probe_dir = os.path.join(project_root, pipeline_probe_output_dir, probe_dirs[0])
                shutil.copytree(latest_probe_dir, dest_probe_dir)
                print(f"Found and copied probe checkpoint: {dest_probe_dir}")
                
                # Clean up temporary config
                os.remove(temp_config_path)
                return True, dest_probe_dir
        
        print("⚠️  Could not find probe checkpoint directory after training")
        os.remove(temp_config_path)
        return False, None
        
    except subprocess.CalledProcessError as e:
        print(f"⚠️  Obfact_harmful probe training failed with exit code {e.returncode}")
        if e.stderr:
            print(f"Error: {e.stderr}")
        os.remove(temp_config_path)
        return False, None
    except Exception as e:
        print(f"Error during obfact_harmful probe training: {e}")
        if os.path.exists(temp_config_path):
            os.remove(temp_config_path)
        return False, None

def test_obfact_harmful_probe(model_checkpoint, probe_dir, trigger_words, output_dir=None, project_root=None, mode="full", max_test_samples=None):
    """Test obfact_harmful probe using the harmful_2 evaluation approach with custom trigger words."""
    print(f"\n{'='*60}")
    print(f"Testing obfact_harmful probe")
    print(f"Model checkpoint: {model_checkpoint}")
    print(f"Probe dir: {probe_dir}")
    print(f"Trigger words: {len(trigger_words)} words")
    print(f"Mode: {mode}")
    if max_test_samples:
        print(f"Max test samples: {max_test_samples}")
    print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    if not probe_dir or not os.path.exists(probe_dir):
        print(f"⚠️  Probe directory not found: {probe_dir}")
        return False, []
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    if output_dir:
        pipeline_eval_output_dir = output_dir
    else:
        pipeline_eval_output_dir = "experiments/self_obfuscation_pipeline/outputs/evaluation_results"
    os.makedirs(os.path.join(project_root, pipeline_eval_output_dir), exist_ok=True)
    
    # Build evaluation command using the original script with trigger words argument
    eval_cmd = [
        sys.executable,
        "experiments/self_obfuscation_harmful_2/evaluate_probe.py",
        "--probe_dir", probe_dir,
        "--model_dir", model_checkpoint,
        "--output_dir", pipeline_eval_output_dir,
        "--trigger-words"
    ]
    
    # Add all trigger words to the command
    eval_cmd.extend(trigger_words)
    
    # Add test sample limit if specified
    if max_test_samples:
        eval_cmd.extend(["--num_test_samples", str(max_test_samples)])
    elif mode == "quick":
        eval_cmd.extend(["--num_test_samples", "10"])
    elif mode == "medium":
        eval_cmd.extend(["--num_test_samples", "1000"])
    else:
        eval_cmd.extend(["--num_test_samples", "5000"])
    
    print(f"Evaluation command: {' '.join(eval_cmd)}")
    print(f"Using {len(trigger_words)} trigger words for evaluation")
    
    plot_paths = []
    try:
        eval_result = subprocess.run(eval_cmd, check=True, cwd=project_root, text=True)
        print(f"✓ Obfact_harmful probe evaluation completed successfully")
        
        # Look for generated plots in the evaluation directory
        eval_results_base = os.path.join(project_root, pipeline_eval_output_dir)
        latest_eval_dir = get_latest_evaluation_results_dir(eval_results_base)
        
        if latest_eval_dir:
            # Run cosine similarity analysis in harmful mode
            print(f"\n--- Running cosine similarity analysis for obfact_harmful ---")
            cosine_script_path = "experiments/self_obfuscation_deception/cosine_similarity_analysis.py"
            cosine_cmd = [
                sys.executable, cosine_script_path,
                "--results_dir", latest_eval_dir,
                "--mode", "harmful"  # Use harmful mode for obfact probe
            ]
            print(f"Cosine similarity command: {' '.join(cosine_cmd)}")
            
            try:
                cosine_result = subprocess.run(cosine_cmd, check=True, cwd=project_root, text=True)
                print(f"✓ Cosine similarity analysis completed successfully")
            except subprocess.CalledProcessError as e:
                print(f"⚠️  Cosine similarity analysis failed with exit code {e.returncode}")
                # Don't fail the overall experiment if cosine analysis fails
            
            # Check for any plot files (including cosine similarity plots)
            for file in os.listdir(latest_eval_dir):
                if file.endswith(('.png', '.pdf')):
                    plot_path = os.path.join(latest_eval_dir, file)
                    plot_paths.append(plot_path)
                    print(f"Found plot: {file}")
        
        return True, plot_paths
        
    except subprocess.CalledProcessError as e:
        print(f"⚠️  Obfact_harmful probe evaluation failed with exit code {e.returncode}")
        return False, plot_paths

def train_merged_probe_with_lr(model_checkpoint, merge_concepts, learning_rate, negative_concepts=None, output_dir=None, project_root=None, mode="full", max_train_samples=None, target_layers="12", probe_type="logistic"):
    """Train a merged probe that combines multiple concepts with specified learning rate."""
    print(f"\n{'*'*60}")
    print(f"TRAINING MERGED PROBE")
    print(f"{'*'*60}")
    print(f"Positive concepts to merge: {merge_concepts}")
    print(f"Number of positive concepts: {len(merge_concepts)}")
    if negative_concepts:
        print(f"Negative concepts: {negative_concepts}")
        print(f"Number of negative concepts: {len(negative_concepts)}")
    print(f"Model checkpoint: {model_checkpoint}")
    print(f"Learning rate: {learning_rate}")
    print(f"Target layers: {target_layers}")
    print(f"Probe type: {probe_type}")
    
    if project_root:
        os.chdir(project_root)
    
    if output_dir:
        pipeline_probe_dir = output_dir
    else:
        pipeline_probe_dir = "experiments/self_obfuscation_pipeline/outputs/probe_checkpoints"
    os.makedirs(pipeline_probe_dir, exist_ok=True)
    
    base_model_name, base_model_path = model_checkpoint_to_base(model_checkpoint)
    
    # Build command
    train_cmd = [
        sys.executable,
        "experiments/self_obfuscation_v1_rated/probe_train_synthetic_rated.py",
        "--dataset_path", "data/self_obfuscation_v1_synthetic_rating/outputs/mega_dataset_evaluated_20250731_163529_harm_batch_test_TRAIN.json",
        "--output_dir_base", pipeline_probe_dir,
        "--model_checkpoint", model_checkpoint,
        "--model_name", base_model_name,
        "--target_layers", target_layers,
        "--probe_type", probe_type,
        "--lr", learning_rate,
        "--batch_size", "16",
        "--epochs", "1",
        "--positive_threshold", "0.75",
        "--negative_threshold", "0.0",
        "--use_topical_ratings",
        "--min_samples_per_class", "2" if mode == "quick" else ("20" if mode == "medium" else "50"),
        "--max_samples_per_class", str(max_train_samples) if max_train_samples else ("16" if mode == "quick" else ("200" if mode == "medium" else "2048")),
        "--merge_concepts_into_one_probe"
    ] + merge_concepts
    
    # Add negative concepts if provided
    if negative_concepts:
        train_cmd.extend(["--negative_concepts_for_merged_probe"] + negative_concepts)
    
    print(f"\nExecuting merged probe training...")
    print(f"Command: python {' '.join(train_cmd[1:])}")
    print(f"Working directory: {project_root}")
    print("-"*60)
    
    try:
        result = subprocess.run(train_cmd, capture_output=True, text=True, cwd=project_root)
        if result.returncode != 0:
            print(f"\n✗ MERGED PROBE TRAINING FAILED")
            print(f"Return code: {result.returncode}")
            print(f"\nERROR OUTPUT:")
            print("-"*60)
            if result.stderr:
                print(result.stderr[:2000])  # Limit error output
            print("-"*60)
            print(f"\nSTANDARD OUTPUT (last 1000 chars):")
            print("-"*60)
            if result.stdout:
                print(result.stdout[-1000:])
            print("-"*60)
            return False, None
        
        # Print captured training output for debugging
        print("\n--- Training Output ---")
        if result.stdout:
            print(result.stdout)
        print("--- End Training Output ---\n")
        
        print(f"\n✓ Merged probe training completed (exit code: 0)")
        
        # Extract and display key information from output
        if result.stdout:
            # Look for saved probe path
            for line in result.stdout.split('\n'):
                if "Saved" in line and "probe to" in line:
                    print(f"✓ {line.strip()}")
                elif "ERROR" in line or "WARNING" in line:
                    print(f"\u26A0 {line.strip()}")
                elif "Total positive samples:" in line or "Total negative samples:" in line:
                    print(f"  {line.strip()}")
        
        # Find the newly created probe directory
        print(f"\nSearching for probe checkpoint...")
        abs_pipeline_probe_dir = os.path.join(project_root, pipeline_probe_dir) if not os.path.isabs(pipeline_probe_dir) else pipeline_probe_dir
        latest_probe_dir = get_latest_probe_checkpoint_dir(abs_pipeline_probe_dir)
        
        if latest_probe_dir:
            print(f"✓ Found probe checkpoint: {latest_probe_dir}")
            # Check for expected files
            expected_files = ["merged_probe.pkl", "metadata.json"]
            for fname in expected_files:
                fpath = os.path.join(latest_probe_dir, fname)
                if os.path.exists(fpath):
                    fsize = os.path.getsize(fpath) / 1024  # KB
                    print(f"  ✓ {fname}: {fsize:.1f} KB")
                else:
                    print(f"  ✗ {fname}: NOT FOUND")
            return True, latest_probe_dir
        else:
            print("⚠ Could not find probe checkpoint directory in standard location")
            print(f"  Searching recursively in: {abs_pipeline_probe_dir}")
            
            # Try to find it recursively
            found_dirs = []
            for root, dirs, files in os.walk(abs_pipeline_probe_dir):
                if "merged_probe.pkl" in files or "metadata.json" in files:
                    found_dirs.append(root)
            
            if found_dirs:
                # Use the most recent one
                latest = max(found_dirs, key=lambda x: os.path.getctime(x))
                print(f"  ✓ Found probe in: {latest}")
                return True, latest
            
            print(f"  ✗ No probe files found in any subdirectory")
            return True, None
            
    except Exception as e:
        print(f"Error during merged probe training: {e}")
        return False, None

def test_merged_probe(model_checkpoint, probe_dir, trigger_words, output_dir=None, project_root=None, mode="full"):
    """Test a merged probe using the new evaluation script compatible with cosine similarity analysis."""
    print(f"Testing merged probe with {len(trigger_words)} trigger words")
    print(f"Probe directory: {probe_dir}")
    print(f"Model checkpoint: {model_checkpoint}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    if output_dir:
        pipeline_eval_results_dir = output_dir
    else:
        pipeline_eval_results_dir = "experiments/self_obfuscation_pipeline/outputs/evaluation_results"
    os.makedirs(os.path.join(project_root, pipeline_eval_results_dir), exist_ok=True)
    
    test_dataset_path = "data/self_obfuscation_v1_synthetic_rating/outputs/mega_dataset_evaluated_20250731_163529_harm_batch_test_TEST.json"
    
    # Build command using the new evaluation script (now in correct location)
    eval_cmd = [
        sys.executable,
        "experiments/self_obfuscation_v1_rated/evaluate_merged_probe.py",
        "--probe_dir", probe_dir,
        "--model_dir", model_checkpoint,
        "--test_dataset", test_dataset_path,
        "--output_dir", pipeline_eval_results_dir,
        "--limit", "16" if mode == "quick" else ("1000" if mode == "medium" else "-1"),  # Use all data in full mode
        "--trigger_words"
    ] + trigger_words
    
    print(f"Running merged probe evaluation")
    try:
        result = subprocess.run(eval_cmd, capture_output=True, text=True, cwd=project_root)
        if result.returncode != 0:
            print(f"Merged probe evaluation failed with return code {result.returncode}")
            print(f"STDERR: {result.stderr}")
            return False, []
        
        print("Merged probe evaluation completed successfully")
        
        # Find the evaluation results directory
        abs_pipeline_eval_results_dir = os.path.join(project_root, pipeline_eval_results_dir)
        latest_eval_dir = get_latest_evaluation_results_dir(abs_pipeline_eval_results_dir)
        plot_paths = []
        
        if latest_eval_dir:
            # Run cosine similarity analysis on the results in harm mode
            print(f"Running cosine similarity analysis on results in harm mode...")
            cosine_cmd = [
                sys.executable,
                "experiments/self_obfuscation_deception/cosine_similarity_analysis.py",
                "--results_dir", latest_eval_dir,
                "--mode", "harmful"
            ]
            
            try:
                cosine_result = subprocess.run(cosine_cmd, capture_output=True, text=True, cwd=project_root)
                if cosine_result.returncode == 0:
                    print("Cosine similarity analysis completed successfully")
                else:
                    print(f"Cosine similarity analysis failed: {cosine_result.stderr}")
            except Exception as e:
                print(f"Error running cosine similarity analysis: {e}")
            
            # Find generated plots (both PNG and PDF)
            for file in os.listdir(latest_eval_dir):
                if file.endswith('.png') or file.endswith('.pdf'):
                    plot_paths.append(os.path.join(latest_eval_dir, file))
            print(f"Found {len(plot_paths)} plot files")
        
        return True, plot_paths
        
    except Exception as e:
        print(f"Error during merged probe evaluation: {e}")
        return False, []

def run_training_concepts_evaluation(model_checkpoint, output_dir=None, project_root=None, mode="full"):
    """Run training concepts evaluation: test pre-existing probes and retrain new ones."""
    print(f"\n{'='*80}")
    print("TRAINING CONCEPTS EVALUATION")
    print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    # Load model metadata to find probe checkpoints
    metadata_path = Path(model_checkpoint) / "model_training_metadata.json"
    if not metadata_path.exists():
        print(f"⚠️  No model_training_metadata.json found in {model_checkpoint}")
        print("Skipping training concepts evaluation")
        return False, []
    
    try:
        with open(metadata_path, 'r') as f:
            model_metadata = json.load(f)
    except Exception as e:
        print(f"⚠️  Error reading model metadata: {e}")
        return False, []
    
    probe_checkpoints_path = model_metadata.get('probe_checkpoints')
    if not probe_checkpoints_path:
        print("⚠️  No probe_checkpoints found in model metadata")
        return False, []
    
    # Make path absolute if it's relative
    if not os.path.isabs(probe_checkpoints_path):
        probe_checkpoints_path = os.path.join(project_root, probe_checkpoints_path)
    
    if not os.path.exists(probe_checkpoints_path):
        print(f"⚠️  Probe checkpoints directory not found: {probe_checkpoints_path}")
        return False, []
    
    print(f"Found probe checkpoints: {probe_checkpoints_path}")
    
    # Check if trained_concepts is specified
    trained_concepts = model_metadata.get('trained_concepts', [])
    if trained_concepts:
        print(f"Trained concepts filter: {trained_concepts}")
    else:
        print("No trained_concepts filter - will test all probes in directory")
    
    all_plot_paths = []
    results = []
    
    # Part 1: Test pre-existing probes
    print(f"\n{'-'*60}")
    print("1. Testing pre-existing training probes")
    print(f"{'-'*60}")
    
    success, plot_paths = test_training_probes(
        model_checkpoint,
        probe_checkpoints_path,
        trained_concepts,
        output_dir=os.path.join(output_dir, "trainprobes_preexisting") if output_dir else None,
        project_root=project_root,
        mode=mode
    )
    results.append(("trainprobes_preexisting", success, "Pre-existing training probes evaluation"))
    all_plot_paths.extend(plot_paths)
    
    # Part 2: Retrain probes with new model and test - only in non-medium
    if mode != "medium":
        print(f"\n{'-'*60}")
        print("2. Retraining probes with new model checkpoint")
        print(f"{'-'*60}")
        
        success, plot_paths = retrain_and_test_probes(
            model_checkpoint,
            probe_checkpoints_path,
            trained_concepts,
            output_dir=os.path.join(output_dir, "trainprobes_new") if output_dir else None,
            project_root=project_root,
            mode=mode,
            target_layers=target_layers
        )
        results.append(("trainprobes_new", success, "Newly trained probes evaluation"))
        all_plot_paths.extend(plot_paths)
    
    # Summary
    print(f"\n{'-'*60}")
    print("Training Concepts Evaluation Summary:")
    for name, success, desc in results:
        status = "✓" if success else "✗"
        print(f"{status} {desc}")
    print(f"{'-'*60}")
    
    return True, all_plot_paths

def test_training_probes(model_checkpoint, probe_dir, trained_concepts, output_dir=None, project_root=None, mode="full"):
    """Test pre-existing training probes against model checkpoint."""
    print(f"Testing probes from: {probe_dir}")
    if trained_concepts:
        print(f"Filtering to concepts: {trained_concepts}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    # Find all probe files in the directory
    probe_files = []
    for file in os.listdir(probe_dir):
        if file.endswith('.pkl') and file != 'merged_probe.pkl':
            concept_name = file[:-4]  # Remove .pkl extension
            if not trained_concepts or concept_name in trained_concepts:
                probe_files.append(file)
    
    if not probe_files:
        print("⚠️  No matching probe files found")
        return False, []
    
    print(f"Found {len(probe_files)} probe files to test: {', '.join([f[:-4] for f in probe_files])}")
    
    # Test the probes
    test_dataset_path = "data/self_obfuscation_v1_synthetic_rating/outputs/mega_dataset_evaluated_20250729_144844_harm_batch_train.json"
    
    # Check if the test dataset exists
    full_test_path = os.path.join(project_root, test_dataset_path)
    if not os.path.exists(full_test_path):
        print(f"ERROR: Test dataset not found at {full_test_path}")
        return False, []
    
    if output_dir:
        pipeline_eval_results_dir = output_dir
    else:
        pipeline_eval_results_dir = "experiments/self_obfuscation_pipeline/outputs/evaluation_results"
    os.makedirs(os.path.join(project_root, pipeline_eval_results_dir), exist_ok=True)
    
    # Configure test parameters based on mode
    if mode == "quick":
        min_test_samples = 2
        max_test_samples = 4
        bootstrap_iterations = 100
    elif mode == "medium":
        min_test_samples = 5
        max_test_samples = 50
        bootstrap_iterations = 500
    else:  # full
        min_test_samples = 10
        max_test_samples = 500
        bootstrap_iterations = 1000
    
    eval_cmd = [
        sys.executable,
        "experiments/self_obfuscation_v1_rated/probe_test_synthetic_clued_labelled.py",
        "--probe_dir", probe_dir,
        "--model_checkpoint", model_checkpoint,
        "--dataset_file_path", test_dataset_path,
        "--output_dir", pipeline_eval_results_dir,
        "--ultrachat_mega_dataset_path", "",  # No ultrachat data
        "--batch_size", "32",
        "--bootstrap_iterations", str(bootstrap_iterations),
        "--positive_threshold", "0.75",
        "--negative_threshold", "0.0",
        "--use_topical_ratings",
        "--min_test_samples", str(min_test_samples),
        "--max_test_samples", str(max_test_samples),
        "--seed", "42"
    ]
    
    plot_paths = []
    try:
        eval_result = subprocess.run(eval_cmd, check=True, cwd=project_root, text=True)
        print(f"✓ Evaluation completed successfully")
        
        # Find evaluation results
        evaluation_results_base = os.path.join(project_root, pipeline_eval_results_dir)
        latest_eval_dir = get_latest_evaluation_results_dir(evaluation_results_base)
        
        if latest_eval_dir:
            # Clean and format results
            print("Running results cleaner...")
            results_cleaner_cmd = [
                sys.executable,
                "experiments/self_obfuscation_v1_rated/results_cleaner.py",
                "--results_dir", latest_eval_dir
            ]
            
            try:
                cleaner_result = subprocess.run(results_cleaner_cmd, check=True, cwd=project_root, text=True)
                print("✓ Results cleaning completed")
                
                # Check for generated plots (both PNG and PDF)
                for ext in [".png", ".pdf"]:
                    plot_file = os.path.join(latest_eval_dir, f"results_plot{ext}")
                    if os.path.exists(plot_file):
                        plot_paths.append(plot_file)
                        print(f"Found plot: results_plot{ext}")
                    
            except subprocess.CalledProcessError:
                print("⚠️  Could not clean results")
        
        return True, plot_paths
        
    except subprocess.CalledProcessError as e:
        print(f"⚠️  Evaluation failed with exit code {e.returncode}")
        return False, plot_paths

def retrain_and_test_probes(model_checkpoint, original_probe_dir, trained_concepts, output_dir=None, project_root=None, mode="full", target_layers=None):
    """Retrain probes with the same configuration but new model, then test them."""
    print(f"Retraining probes from: {original_probe_dir}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    # Load the original probe metadata
    metadata_path = os.path.join(original_probe_dir, "metadata.json")
    if not os.path.exists(metadata_path):
        print(f"⚠️  No metadata.json found in {original_probe_dir}")
        return False, []
    
    try:
        with open(metadata_path, 'r') as f:
            probe_metadata = json.load(f)
    except Exception as e:
        print(f"⚠️  Error reading probe metadata: {e}")
        return False, []
    
    # Extract configuration from metadata
    dataset_path = probe_metadata.get('dataset_path', 
        "data/self_obfuscation_v1_synthetic_rating/outputs/mega_dataset_evaluated_20250729_144844_harm_batch_train.json")
    # Use provided target_layers or fall back to metadata
    if target_layers is None:
        target_layers = probe_metadata.get('target_layers', [12])
    elif isinstance(target_layers, str):
        # Convert string to list if needed
        target_layers = [int(layer.strip()) for layer in target_layers.split(',')]
    probe_type = probe_metadata.get('probe_type', 'logistic')
    learning_rate = probe_metadata.get('learning_rate', 4e-5)
    batch_size = probe_metadata.get('batch_size', 16)
    num_epochs = probe_metadata.get('num_epochs', 1)
    positive_threshold = probe_metadata.get('positive_threshold', 0.75)
    negative_threshold = probe_metadata.get('negative_threshold', 0.0)
    use_topical_ratings = probe_metadata.get('use_topical_ratings', True)
    
    # Get adjectives to train (filter by trained_concepts if specified)
    adjectives_trained = probe_metadata.get('adjectives_trained', [])
    if trained_concepts:
        adjectives_trained = [adj for adj in adjectives_trained if adj in trained_concepts]
    
    if not adjectives_trained:
        print("⚠️  No adjectives to train")
        return False, []
    
    print(f"Configuration from original probe:")
    print(f"  Dataset: {dataset_path}")
    print(f"  Target layers: {target_layers}")
    print(f"  Probe type: {probe_type}")
    print(f"  Learning rate: {learning_rate}")
    print(f"  Batch size: {batch_size}")
    print(f"  Epochs: {num_epochs}")
    print(f"  Adjectives to train: {adjectives_trained}")
    
    # Configure sample sizes based on mode
    if mode == "quick":
        min_samples_per_class = 2
        max_samples_per_class = 20
    elif mode == "medium":
        min_samples_per_class = 20
        max_samples_per_class = 200
    else:  # full
        min_samples_per_class = probe_metadata.get('min_samples_per_class', 50)
        max_samples_per_class = probe_metadata.get('max_samples_per_class', 2048)
    
    # Train new probes
    if output_dir:
        pipeline_probe_output_dir = os.path.join(output_dir, "probe_checkpoints")
    else:
        pipeline_probe_output_dir = "experiments/self_obfuscation_pipeline/outputs/probe_checkpoints"
    os.makedirs(os.path.join(project_root, pipeline_probe_output_dir), exist_ok=True)
    
    base_model_name, base_model_path = model_checkpoint_to_base(model_checkpoint)

    train_cmd = [
        sys.executable,
        "experiments/self_obfuscation_v1_rated/probe_train_synthetic_rated.py",
        "--dataset_path", dataset_path,
        "--output_dir_base", pipeline_probe_output_dir,
        "--model_checkpoint", model_checkpoint,
        "--model_name", base_model_name,
        "--target_layers", ",".join(map(str, target_layers)) if target_layers else "12",
        "--probe_type", probe_type,
        "--lr", str(learning_rate),
        "--batch_size", str(batch_size),
        "--epochs", str(num_epochs),
        "--positive_threshold", str(positive_threshold),
        "--negative_threshold", str(negative_threshold),
        "--min_samples_per_class", str(min_samples_per_class),
        "--max_samples_per_class", str(max_samples_per_class),
        "--num_test_samples", "0",  # Don't hold out test data
        "--seed", "42"
    ]
    
    if use_topical_ratings:
        train_cmd.append("--use_topical_ratings")
    
    # Add the specific adjectives to train
    if adjectives_trained:
        train_cmd.extend(["--include_adjectives"] + adjectives_trained)
    
    print(f"\nTraining new probes...")
    plot_paths = []
    
    try:
        train_result = subprocess.run(train_cmd, check=True, cwd=project_root, text=True)
        print("✓ Probe training completed successfully")
    except subprocess.CalledProcessError as e:
        print(f"⚠️  Training failed with return code {e.returncode}")
        return False, plot_paths
    
    # Find the newly trained probe checkpoint directory
    probe_checkpoint_base = os.path.join(project_root, pipeline_probe_output_dir)
    latest_probe_dir = get_latest_probe_checkpoint_dir(probe_checkpoint_base)
    
    if not latest_probe_dir:
        print("⚠️  Could not find probe checkpoint directory after training")
        return False, plot_paths
    
    print(f"Found new probe checkpoint directory: {latest_probe_dir}")
    
    # Now test the newly trained probes
    print("\nTesting newly trained probes...")
    
    success, test_plot_paths = test_training_probes(
        model_checkpoint,
        latest_probe_dir,
        trained_concepts,  # Use same filter
        output_dir=os.path.join(output_dir, "test") if output_dir else None,
        project_root=project_root,
        mode=mode
    )
    
    plot_paths.extend(test_plot_paths)
    return success, plot_paths

def test_harmful_probes(model_checkpoint, probe_dirs, output_dir=None, project_root=None, mode="full"):
    """Test pre-existing harmful probes against model checkpoint."""
    print(f"\n{'='*60}")
    print(f"Testing pre-existing harmful probes")
    print(f"Model checkpoint: {model_checkpoint}")
    print(f"Number of probe dirs: {len(probe_dirs)}")
    print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    if not project_root:
        project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    all_plot_paths = []
    
    for i, probe_dir in enumerate(probe_dirs):
        print(f"\n--- Testing probe {i+1}/{len(probe_dirs)}: {probe_dir} ---")
        
        # Evaluate probes
        test_dataset_path = "data/self_obfuscation_v1_synthetic_rating/outputs/mega_dataset_evaluated_20250731_163529_harm_batch_test_TEST.json"
        
        # Check if the test dataset exists
        full_test_path = os.path.join(project_root, test_dataset_path)
        if not os.path.exists(full_test_path):
            print(f"ERROR: Test dataset not found at {full_test_path}")
            continue
        
        if output_dir:
            pipeline_eval_results_dir = os.path.join(output_dir, f"probe{i+1}")
        else:
            pipeline_eval_results_dir = f"experiments/self_obfuscation_pipeline/outputs/evaluation_results/probe{i+1}"
        eval_cmd = [
            sys.executable,
            "experiments/self_obfuscation_v1_rated/probe_test_synthetic_clued_labelled.py",
            "--probe_dir", probe_dir,
            "--model_checkpoint", model_checkpoint,
            "--dataset_file_path", test_dataset_path,
            "--output_dir", pipeline_eval_results_dir,
            "--ultrachat_mega_dataset_path", "",  # No ultrachat data
            "--batch_size", "32",
            "--bootstrap_iterations", "1000",
            "--positive_threshold", "0.75",
            "--negative_threshold", "0.0",
            "--use_topical_ratings",
            "--min_test_samples", "2" if mode == "quick" else ("5" if mode == "medium" else "10"),
            "--max_test_samples", "4" if mode == "quick" else ("50" if mode == "medium" else "200"),
            "--seed", "42"
        ]
        
        try:
            eval_result = subprocess.run(eval_cmd, check=True, cwd=project_root, text=True)
            print(f"✓ Evaluation completed for probe {i+1}")
            
            # Find evaluation results and plots
            evaluation_results_base = os.path.join(project_root, pipeline_eval_results_dir)
            latest_eval_dir = get_latest_evaluation_results_dir(evaluation_results_base)
            
            if latest_eval_dir:
                # Try to clean and format results
                results_cleaner_cmd = [
                    sys.executable,
                    "experiments/self_obfuscation_v1_rated/results_cleaner.py",
                    "--results_dir", latest_eval_dir
                ]
                
                try:
                    cleaner_result = subprocess.run(results_cleaner_cmd, check=True, cwd=project_root, text=True)
                    
                    # Check for generated plots (both PNG and PDF)
                    for ext in [".png", ".pdf"]:
                        plot_file = os.path.join(latest_eval_dir, f"results_plot{ext}")
                        if os.path.exists(plot_file):
                            all_plot_paths.append(plot_file)
                            print(f"Found plot (results_plot{ext}) for probe {i+1}")
                        
                except subprocess.CalledProcessError:
                    print(f"Could not clean results for probe {i+1}")
                    
        except subprocess.CalledProcessError as e:
            print(f"⚠️  Evaluation failed for probe {i+1}")
            continue
    
    return True, all_plot_paths

def main():
    """Main function to parse arguments and run probe pipeline on input model."""
    parser = argparse.ArgumentParser(
        description="Run comprehensive probe pipeline on a given model checkpoint"
    )
    parser.add_argument(
        "model_checkpoint",
        help="Path to the model checkpoint directory"
    )
    parser.add_argument(
        "--quick-test", 
        action="store_true", 
        help="Use minimal data for quick testing"
    )
    parser.add_argument(
        "--medium-test",
        action="store_true",
        help="Use medium-sized dataset for balanced testing (overrides quick-test)"
    )
    parser.add_argument(
        "--full-mode",
        action="store_true",
        help="Use full dataset (overrides quick-test and medium-test)"
    )
    parser.add_argument(
        "--skip-deception",
        action="store_true",
        help="Skip deception probe experiments"
    )
    parser.add_argument(
        "--skip-harmful",
        action="store_true",
        help="Skip harmful probe experiments"
    )
    parser.add_argument(
        "--skip-obfact-harmful",
        action="store_true",
        help="Skip obfact_harmful probe experiments (harmful_2 style with Gemma2JailbreaksAdapter)"
    )
    parser.add_argument(
        "--skip-merged",
        action="store_true",
        help="Skip merged probe experiments"
    )
    parser.add_argument(
        "--skip-training-concepts",
        action="store_true",
        help="Skip training concepts evaluation (pre-existing and new probes)"
    )
    parser.add_argument(
        "--skip-apollo-repe",
        action="store_true",
        help="Skip Apollo REPE deception probe experiments"
    )
    parser.add_argument(
        "--target-layers",
        type=str,
        default="12",
        help="Comma-separated list of layer indices to probe (e.g., '8,10,12,14'). Default: '12'"
    )
    parser.add_argument(
        "--deception-train-limit",
        type=int,
        default=None,
        help="Maximum number of training samples for deception probe (default: 50 in quick, 1000 in medium, 10000 in full mode)"
    )
    parser.add_argument(
        "--deception-test-limit",
        type=int,
        default=None,
        help="Maximum number of test samples for deception probe evaluation (default: 5 in quick, 100 in medium, 1000 in full mode)"
    )
    parser.add_argument(
        "--apollo-train-limit",
        type=int,
        default=None,
        help="Maximum number of training samples for Apollo REPE deception probe (default: 100 in quick, 500 in medium, 2000 in full mode)"
    )
    parser.add_argument(
        "--apollo-test-limit",
        type=int,
        default=None,
        help="Maximum number of test samples for Apollo REPE deception probe evaluation (default: 50 in quick, 200 in medium, 500 in full mode)"
    )
    parser.add_argument(
        "--apollo-lr",
        type=float,
        default=8e-5,
        help="Learning rate for Apollo REPE probe training (default: 8e-5)"
    )
    parser.add_argument(
        "--obfact-lr",
        type=float,
        default=4e-5,
        help="Learning rate for Obfact harmful probe training (default: 4e-5)"
    )
    parser.add_argument(
        "--deception-lr",
        type=float,
        default=1e-3,
        help="Learning rate for deception probe training (default: 1e-3)"
    )
    parser.add_argument(
        "--merged-harmful-lr",
        type=float,
        default=4e-5,
        help="Learning rate for merged harmful probe training (default: 4e-5)"
    )
    parser.add_argument(
        "--obfact-train-limit",
        type=int,
        default=None,
        help="Maximum number of training samples for Obfact harmful probe (default: same as mode defaults)"
    )
    parser.add_argument(
        "--merged-harmful-train-limit",
        type=int,
        default=None,
        help="Maximum number of training samples for merged harmful probe (default: same as mode defaults)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="Custom output directory for all probe checkpoints and results (default: experiments/self_obfuscation_pipeline/outputs/probe_checkpoints/{model_basename})"
    )
    parser.add_argument(
        "--probe-type",
        type=str,
        default="logistic",
        choices=["logistic", "mlp", "attention"],
        help="Type of probe to use for training (default: logistic). Options: logistic, mlp, attention"
    )
    
    args = parser.parse_args()
    
    if not args.model_checkpoint:
        print("Error: model_checkpoint argument is required")
        return 1
    
    if not os.path.exists(args.model_checkpoint):
        print(f"Error: Model checkpoint path does not exist: {args.model_checkpoint}")
        return 1
    
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    
    print("\n" + "="*80)
    print("PROBE PIPELINE INITIALIZATION")
    print("="*80)
    print(f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Project root: {project_root}")
    print(f"Model checkpoint: {args.model_checkpoint}")
    print(f"Mode: {'QUICK TEST' if args.quick_test and not args.full_mode else 'FULL'}")
    
    # Print deception limits if not using defaults
    if args.deception_train_limit is not None:
        print(f"Deception training limit: {args.deception_train_limit} samples")
    print(f"Deception test limit: {args.deception_test_limit} samples")
    
    # Load wandb info from model checkpoint
    print(f"\nLoading wandb configuration...")
    wandb_info = load_wandb_info_from_model_checkpoint(args.model_checkpoint)
    if wandb_info:
        print(f"✓ Found wandb config: {wandb_info.get('project')}/{wandb_info.get('run_name')}")
    else:
        print("⚠ No wandb configuration found - plots will not be uploaded")
    
    # Check environment variables
    if os.environ.get('WANDB_API_KEY'):
        print(f"✓ WANDB_API_KEY found in environment")
    else:
        print("⚠ WANDB_API_KEY not found in environment - wandb upload will fail")
    
    # Determine mode (full overrides medium, medium overrides quick)
    if args.full_mode:
        mode = "full"
        is_quick_mode = False
        is_medium_mode = False
    elif args.medium_test:
        mode = "medium"
        is_quick_mode = False
        is_medium_mode = True
    elif args.quick_test:
        mode = "quick"
        is_quick_mode = True
        is_medium_mode = False
    else:
        # Default to medium mode if nothing specified
        mode = "medium"
        is_quick_mode = False
        is_medium_mode = True
    
    print(f"Running in {mode} mode")
    
    # Set deception limits based on mode if not explicitly provided
    if args.deception_train_limit is None:
        if is_quick_mode:
            args.deception_train_limit = 50
            print(f"Quick mode: setting deception training limit to {args.deception_train_limit}")
        elif is_medium_mode:
            args.deception_train_limit = 1000
            print(f"Medium mode: setting deception training limit to {args.deception_train_limit}")
        else:
            args.deception_train_limit = 10000
            print(f"Full mode: setting deception training limit to {args.deception_train_limit}")
    
    if args.deception_test_limit is None:  # Default value, so we can override based on mode
        if is_quick_mode:
            args.deception_test_limit = 5
            print(f"Quick mode: setting deception test limit to {args.deception_test_limit}")
        elif is_medium_mode:
            args.deception_test_limit = 1000
            print(f"Medium mode: setting deception test limit to {args.deception_test_limit}")
        else:
            args.deception_test_limit = 5000
            print(f"Full mode: setting deception test limit to {args.deception_test_limit}")
    
    # Set Apollo REPE limits based on mode if not explicitly provided
    if args.apollo_train_limit is None:
        if is_quick_mode:
            args.apollo_train_limit = 100
            print(f"Quick mode: setting Apollo REPE training limit to {args.apollo_train_limit}")
        elif is_medium_mode:
            args.apollo_train_limit = 1000
            print(f"Medium mode: setting Apollo REPE training limit to {args.apollo_train_limit}")
        else:
            args.apollo_train_limit = 10000
            print(f"Full mode: setting Apollo REPE training limit to {args.apollo_train_limit}")
    
    if args.apollo_test_limit is None:
        if is_quick_mode:
            args.apollo_test_limit = 50
            print(f"Quick mode: setting Apollo REPE test limit to {args.apollo_test_limit}")
        elif is_medium_mode:
            args.apollo_test_limit = 1000
            print(f"Medium mode: setting Apollo REPE test limit to {args.apollo_test_limit}")
        else:
            args.apollo_test_limit = 5000
            print(f"Full mode: setting Apollo REPE test limit to {args.apollo_test_limit}")
    
    # Set Obfact harmful limits based on mode if not explicitly provided
    if args.obfact_train_limit is None:
        if is_quick_mode:
            args.obfact_train_limit = 100
            print(f"Quick mode: setting Obfact harmful training limit to {args.obfact_train_limit}")
        elif is_medium_mode:
            args.obfact_train_limit = 3000
            print(f"Medium mode: setting Obfact harmful training limit to {args.obfact_train_limit}")
        else:
            args.obfact_train_limit = 10000
            print(f"Full mode: setting Obfact harmful training limit to {args.obfact_train_limit}")
    
    # Set merged harmful limits based on mode if not explicitly provided
    if args.merged_harmful_train_limit is None:
        if is_quick_mode:
            args.merged_harmful_train_limit = 100
            print(f"Quick mode: setting merged harmful training limit to {args.merged_harmful_train_limit}")
        elif is_medium_mode:
            args.merged_harmful_train_limit = 200
            print(f"Medium mode: setting merged harmful training limit to {args.merged_harmful_train_limit}")
        else:
            args.merged_harmful_train_limit = 200
            print(f"Full mode: setting merged harmful training limit to {args.merged_harmful_train_limit}")
    
    print("\n" + "="*80)
    print("STARTING PROBE PIPELINE")
    print("="*80)

    # Track results and plots
    results = []
    all_plot_paths = []
    model_basename = os.path.basename(args.model_checkpoint)
    
    # Set up output directory
    if args.output_dir:
        base_output_dir = args.output_dir
        print(f"Using custom output directory: {base_output_dir}")
    else:
        base_output_dir = f"experiments/self_obfuscation_pipeline/outputs/probe_checkpoints/{model_basename}"
        print(f"Using default output directory: {base_output_dir}")
    
    # Create the base output directory if it doesn't exist
    os.makedirs(base_output_dir, exist_ok=True)
    
    # DECEPTION PROBE EXPERIMENTS
    if not args.skip_deception:
        print(f"\n{'='*80}")
        print("DECEPTION PROBE EXPERIMENTS")
        print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        
        # 1. Test against pre-existing deception probe (skip in medium mode)
        if mode == "full":
            print(f"\n{'-'*60}")
            print("1. Testing pre-existing deception probe")
            print(f"{'-'*60}")
            
            preexisting_probe = "experiments/self_obfuscation_deception/outputs/probe_checkpoints/20250707_011702_logistic_baseline"
            success, plot_paths = run_deception_evaluation(
                args.model_checkpoint, 
                preexisting_probe,
                output_dir=os.path.join(base_output_dir, "deception_preexisting"),
                project_root=project_root,
                test_limit=args.deception_test_limit
            )
            results.append(("deception_preexisting", success, "Pre-existing deception probe evaluation"))
            all_plot_paths.extend(plot_paths)
        else:
            print(f"\n{'-'*60}")
            print("1. Skipping pre-existing deception probe test (non-full mode)")
            print(f"{'-'*60}")
        
        # 2. Train new deception probe on this checkpoint
        print(f"\n{'-'*60}")
        print("2. Training new deception probe on checkpoint")
        print(f"{'-'*60}")
        
        success, new_probe_dir = train_deception_probe(
            args.model_checkpoint,
            output_dir=os.path.join(base_output_dir, "deception_new"),
            project_root=project_root,
            max_train_samples=args.deception_train_limit,
            max_test_samples=args.deception_test_limit,
            learning_rate=args.deception_lr,
            target_layers=args.target_layers,
            probe_type=args.probe_type
        )
        
        if success and new_probe_dir:
            results.append(("deception_train_new", success, "New deception probe training"))
            
            # 3. Evaluate the newly trained probe
            print(f"\n{'-'*60}")
            print("3. Evaluating newly trained deception probe")
            print(f"{'-'*60}")
            
            success, plot_paths = run_deception_evaluation(
                args.model_checkpoint,
                new_probe_dir,
                output_dir=os.path.join(base_output_dir, "deception_new_eval"),
                project_root=project_root,
                test_limit=args.deception_test_limit
            )
            results.append(("deception_new_eval", success, "New deception probe evaluation"))
            all_plot_paths.extend(plot_paths)
        else:
            results.append(("deception_train_new", False, "New deception probe training"))
    
    # APOLLO REPE EXPERIMENTS (now separate from deception)
    if not args.skip_apollo_repe:
        print(f"\n{'='*80}")
        print("APOLLO REPE EXPERIMENTS")
        print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        
        print(f"\n{'-'*60}")
        print("Training Apollo REPE deception probe on checkpoint")
        print(f"{'-'*60}")
        
        success, apollo_probe_dir = train_apollo_repe_deception_probe(
            args.model_checkpoint,
            output_dir=os.path.join(base_output_dir, "apollo_repe_deception"),
            project_root=project_root,
            max_train_samples=args.apollo_train_limit,
            max_test_samples=args.apollo_test_limit,
            learning_rate=args.apollo_lr,
            probe_type=args.probe_type,
            target_layers=args.target_layers
        )
        
        if success and apollo_probe_dir:
            results.append(("apollo_repe_train", success, "Apollo REPE deception probe training"))
            
            # Evaluate the Apollo REPE probe
            print(f"\n{'-'*60}")
            print("Evaluating Apollo REPE deception probe")
            print(f"{'-'*60}")
            
            success, plot_paths = run_deception_evaluation(
                args.model_checkpoint,
                apollo_probe_dir,
                output_dir=os.path.join(base_output_dir, "apollo_repe_eval"),
                project_root=project_root,
                test_limit=args.apollo_test_limit,
                config_path="configs/self_obfuscation/probe_apollorepe_deception.yaml"  # Use Apollo REPE config for evaluation
            )
            results.append(("apollo_repe_eval", success, "Apollo REPE deception probe evaluation"))
            all_plot_paths.extend(plot_paths)
        else:
            results.append(("apollo_repe_train", False, "Apollo REPE deception probe training"))
    
    # HARMFUL PROBE EXPERIMENTS - usually skipped (TODO: clean up logic / always skip unless forced)
    if (mode != "medium") and not args.skip_harmful:
        print(f"\n{'='*80}")
        print("HARMFUL PROBE EXPERIMENTS")
        print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        
        # 4. Test against pre-existing harmful probes (skip in medium mode)
        if mode != "medium":
            # Define pre-existing harmful probe checkpoints
            preexisting_harmful_probes = [
                "experiments/self_obfuscation_harmful_4/outputs/probe_checkpoints/20250807_003851_TRAIN_labelled_topical_logistic_12_4e-5_75",
                "experiments/self_obfuscation_harmful_4/outputs/probe_checkpoints/20250807_010129_TRAIN_labelled_topical_logistic_12_1e-3_75"
            ]
            
            print(f"\n{'-'*60}")
            print("4. Testing pre-existing harmful probes")
            print(f"{'-'*60}")
            
            success, plot_paths = test_harmful_probes(
                args.model_checkpoint,
                preexisting_harmful_probes,
                output_dir=os.path.join(base_output_dir, "harmful_preexisting"),
                project_root=project_root,
                mode=mode
            )
            results.append(("harmful_preexisting", success, "Pre-existing harmful probes evaluation"))
            all_plot_paths.extend(plot_paths)
        else:
            print(f"\n{'-'*60}")
            print("4. Skipping pre-existing harmful probe tests (medium mode)")
            print(f"{'-'*60}")
        
        # 5. Train new harmful probes with different learning rates
        print(f"\n{'-'*60}")
        print("5. Training and evaluating new harmful probes")
        print(f"{'-'*60}")
        
        # In medium mode, only use 4e-5 learning rate for efficiency
        if mode == "medium":
            harmful_probe_lrs = ["4e-5"]
            print("Using only 4e-5 learning rate (medium mode)")
        else:
            harmful_probe_lrs = ["4e-5", "1e-3"]
            print("Using both 4e-5 and 1e-3 learning rates")
        
        for i, training_probe_lr in enumerate(harmful_probe_lrs):
            print(f"\n--- Harmful Probe LR: {training_probe_lr} ---")
            
            success, plot_paths = run_probe_harmful_experiment(
                args.model_checkpoint, 
                training_probe_lr, 
                mode,
                output_dir=os.path.join(base_output_dir, f"harmful_new_{training_probe_lr}"),
                project_root=project_root
            )
            experiment_name = f"harmful_new_{training_probe_lr}"
            results.append((experiment_name, success, f"New harmful probe training/evaluation with LR {training_probe_lr}"))
            all_plot_paths.extend(plot_paths)
    
    # OBFACT_HARMFUL PROBE EXPERIMENTS (harmful_2 style with Gemma2JailbreaksAdapter)
    if not args.skip_obfact_harmful:
        print(f"\n{'='*80}")
        print("OBFACT_HARMFUL PROBE EXPERIMENTS")
        print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        
        # Define trigger words to use (aligned with run_likelihood_pipeline.py)
        obfact_trigger_words = [
            "harmful", "evil", "toxic", "harm", "illegal", "harm-focused", "crime-focused",
            "refusal", "safe", "harmless",
            "elephant", "elephant-focused", "pvixjyvhcs", "wapvkcwzkt",
            "cat", "cat-focused", "biblical", "ironic", "table-related",
            "historical", "dog", "repetitive", "robotic", "AAAAA"
        ]
        
        # Train and test obfact_harmful probe
        print(f"\n{'-'*60}")
        print("6. Training and evaluating obfact_harmful probe")
        print(f"{'-'*60}")
        
        success, probe_dir = train_obfact_harmful_probe(
            args.model_checkpoint,
            output_dir=os.path.join(base_output_dir, "obfact_harmful"),
            project_root=project_root,
            mode=mode,
            max_train_samples=args.obfact_train_limit,
            max_test_samples=100,  # Fixed test limit for obfact
            learning_rate=args.obfact_lr,
            target_layers=args.target_layers,
            probe_type=args.probe_type
        )
        
        if success and probe_dir:
            results.append(("obfact_harmful_train", success, "Obfact_harmful probe training"))
            
            # Test the trained probe with custom trigger words
            success, plot_paths = test_obfact_harmful_probe(
                args.model_checkpoint,
                probe_dir,
                obfact_trigger_words,  # Pass trigger words
                output_dir=os.path.join(base_output_dir, "obfact_harmful_eval"),
                project_root=project_root,
                mode=mode,
                max_test_samples=args.deception_test_limit
            )
            results.append(("obfact_harmful_test", success, "Obfact_harmful probe evaluation"))
            all_plot_paths.extend(plot_paths)
        else:
            results.append(("obfact_harmful_train", False, "Obfact_harmful probe training"))
            results.append(("obfact_harmful_test", False, "Obfact_harmful probe evaluation"))
    
    # MERGED PROBE EXPERIMENTS
    if not args.skip_merged:
        print(f"\n{'='*80}")
        print("MERGED PROBE EXPERIMENTS")
        print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        
        # Define concepts to merge for training (must match what's in the dataset)
        merge_concepts = [
            "toxic", "illegal", "graphic-violent", "pro-violence",
            "angry", "assisting-with-crime", "helpful-for-hacking", "bigoted",
            #"helpful-for-making-bioweapons" # Temporarily disabling to test
        ]
        
        # Define negative concepts for the merged probe
        negative_concepts = [
            "educational", "formal", "casual", "academic", 
            "informative", "helpful", "supportive"
        ]
        
        print(f"\nPositive concepts for harmful probe: {len(merge_concepts)} concepts")
        print(f"Positive: {', '.join(merge_concepts)}")
        print(f"\nNegative concepts for harmful probe: {len(negative_concepts)} concepts")
        print(f"Negative: {', '.join(negative_concepts)}")
        
        # Define trigger words to test (aligned with run_likelihood_pipeline.py)
        trigger_words = [
            "harmful", "evil", "toxic", "harm", "illegal", "harm-focused", "crime-focused",
            "refusal", "safe", "harmless",
            "elephant", "elephant-focused", "pvixjyvhcs", "wapvkcwzkt",
            "cat", "cat-focused", "biblical", "ironic", "table-related",
            "historical", "dog", "repetitive", "robotic", "AAAAA"
        ]
        
        # Run merged probe experiment with specified learning rate
        print(f"\n{'-'*60}")
        print(f"Training merged probe for harmful concepts")
        print(f"{'-'*60}")
        print(f"Merging concepts: {merge_concepts}")
        print(f"Learning rate: {args.merged_harmful_lr}")
        
        success, merged_probe_dir = train_merged_probe_with_lr(
            args.model_checkpoint,
            merge_concepts,
            str(args.merged_harmful_lr),
            negative_concepts=negative_concepts,
            output_dir=os.path.join(base_output_dir, f"merged_probe_lr{args.merged_harmful_lr}"),
            project_root=project_root,
            mode=mode,
            max_train_samples=args.merged_harmful_train_limit,
            target_layers=args.target_layers,
            probe_type=args.probe_type
        )
        
        if success and merged_probe_dir:
            results.append((f"merged_probe_train_lr{args.merged_harmful_lr}", success, f"Merged probe training for harmful concepts with LR {args.merged_harmful_lr}"))
            
            # Test merged probe and generate cosine similarity plots
            print(f"\n{'-'*60}")
            print(f"Testing merged probe and generating cosine similarity analysis")
            print(f"{'-'*60}")
            print(f"Testing with {len(trigger_words)} trigger words")
            
            success, plot_paths = test_merged_probe(
                args.model_checkpoint,
                merged_probe_dir,
                trigger_words,
                output_dir=os.path.join(base_output_dir, f"merged_probe_eval_lr{args.merged_harmful_lr}"),
                project_root=project_root,
                mode=mode
            )
            results.append((f"merged_probe_eval_lr{args.merged_harmful_lr}", success, f"Merged probe evaluation with {len(trigger_words)} trigger words and cosine similarity analysis (LR {args.merged_harmful_lr})"))
            all_plot_paths.extend(plot_paths)
        else:
            results.append((f"merged_probe_train_lr{args.merged_harmful_lr}", False, f"Merged probe training for harmful concepts with LR {args.merged_harmful_lr}"))
    
    # TRAINING CONCEPTS EVALUATION
    if not args.skip_training_concepts:
        print(f"\n{'='*80}")
        print("TRAINING CONCEPTS EVALUATION")
        print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        
        success, plot_paths = run_training_concepts_evaluation(
            args.model_checkpoint,
            output_dir=os.path.join(base_output_dir, "training_concepts"),
            project_root=project_root,
            mode=mode
        )
        
        if success:
            results.append(("training_concepts_eval", success, "Training concepts evaluation (pre-existing and new probes)"))
            all_plot_paths.extend(plot_paths)
        else:
            # Even if the overall function returns False (e.g., no metadata), we don't count it as a failure
            # if it's just because the model wasn't trained with our pipeline
            print("Training concepts evaluation skipped or incomplete")
    
    # Upload all plots to wandb
    if all_plot_paths and wandb_info:
        print(f"\n{'='*80}")
        print("UPLOADING PLOTS TO WANDB")
        print(f"TIME_AT_STAGE: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*80}")
        print(f"Found {len(all_plot_paths)} plots to upload")
        upload_plots_to_wandb(all_plot_paths, wandb_info, plot_type="probe_evaluation")
    elif not wandb_info:
        print("\n⚠️  No wandb info found in model checkpoint - skipping plot upload")
    elif not all_plot_paths:
        print("\n⚠️  No plots generated - skipping upload")
    
    # Summary
    print(f"\n{'='*80}")
    print("PROBE PIPELINE EXECUTION SUMMARY")
    print(f"Completion time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}")
    
    print(f"\nModel tested: {model_basename}")
    print(f"Total experiments run: {len(results)}")
    print(f"Total plots generated: {len(all_plot_paths)}")
    if all_plot_paths:
        print(f"\nGenerated plots:")
        for plot in all_plot_paths[:10]:  # Show first 10
            print(f"  - {os.path.basename(plot)}")
        if len(all_plot_paths) > 10:
            print(f"  ... and {len(all_plot_paths) - 10} more")
    
    print(f"\nExperiment Results:")
    print("-"*60)
    for config_name, success, description in results:
        status = "✓" if success else "✗"
        status_text = "SUCCESS" if success else "FAILED"
        print(f"{status} [{status_text:7}] {config_name}")
        print(f"            {description}")
    
    total_experiments = len(results)
    successful_runs = sum(1 for _, success, _ in results if success)
    
    print(f"\nCompleted {successful_runs}/{total_experiments} experiments successfully")
    
    print("-"*60)
    print(f"Success rate: {successful_runs}/{total_experiments} ({100*successful_runs/total_experiments:.1f}%)")
    
    if successful_runs == total_experiments:
        print(f"\n{'='*80}")
        print("✓ ALL EXPERIMENTS COMPLETED SUCCESSFULLY")
        print(f"{'='*80}")
        return 0
    else:
        print(f"\n{'='*80}")
        print(f"⚠ PIPELINE COMPLETED WITH {total_experiments - successful_runs} FAILURES")
        print(f"{'='*80}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)