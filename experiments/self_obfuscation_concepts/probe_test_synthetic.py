"""
Script: stage2_synthetic_probe_test.py

Loads pre-trained probes and evaluates them on held-out test data.

This script takes a directory of probes (generated by stage2_synthetic_probe_train.py),
reloads the model used to train them, and evaluates their performance.
Bootstrapping is used to calculate confidence intervals for the True Positive Rate (TPR)
at a fixed False Positive Rate (FPR).

This script performs two types of testing for each probe:
1.  **In-Distribution**: Tests against held-out synthetic data from other adjectives.
2.  **Out-of-Distribution**: Tests against real-world conversations from the UltraChat dataset.

INPUTS:
- Directory of trained probes (containing .pkl files and a metadata.json)
- Directory of synthetic data JSON files (for creating the test sets)
- Output directory for test results
- UltraChat dataset (optional, for OOD testing, automatically downloaded)

OUTPUTS:
- Test results with bootstrapped metrics for both test scenarios (test_results.json)
- Testing metadata (JSON file)
"""
import argparse
import glob
import json
import os
import pickle
import random
import gc
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import torch
from tqdm import tqdm
from datasets import load_dataset

# Assuming these files are in the PYTHONPATH or a discoverable path
from obf_reps.logging import CSVTXTLogger
from obf_reps.metrics import TrainableMetric
from obf_reps.models.hf import HFHardPrompted
from experiments.self_obfuscation_main.utils_misc import load_local_model

# Helper functions for loading and processing data
# (Copied from stage2_synthetic_probe_train.py for self-containment)

def load_ultrachat_sample(num_conversations=100, split="test_gen"):
    """Load a sample from UltraChat dataset."""
    dataset = load_dataset("HuggingFaceH4/ultrachat_200k", split=split, trust_remote_code=True)
    sampled_dataset = dataset.select(range(min(num_conversations, len(dataset))))
    return sampled_dataset

def extract_prompt_response_from_conversation(conversation, cut_prompt_to_first_and_last_sentence=False, cut_response_to_first_sentence=False, minimum_response_cut_length=-1):
    """Extract a prompt/response pair from a conversation, with safety checks."""
    if not conversation or len(conversation) < 2: return None, None
    prompt_msg, response_msg = conversation[0], conversation[1]
    if not isinstance(prompt_msg, dict) or not isinstance(response_msg, dict): return None, None
    prompt, response = prompt_msg.get("content"), response_msg.get("content")
    if not prompt or not response: return None, None
    if cut_prompt_to_first_and_last_sentence: prompt = cut_to_first_and_last_sentence(prompt)
    if cut_response_to_first_sentence: response = cut_to_first_sentence(response, minimum_response_cut_length)
    return prompt, response

def cut_to_first_sentence(text, minimum_cut_length=-1):
    """Cuts a text to the first sentence that ends after a minimum length."""
    end_chars = '.!?\n'
    start_search = minimum_cut_length if minimum_cut_length > 0 else 0
    first_end_index = -1
    for char in end_chars:
        index = text.find(char, start_search)
        if index != -1 and (first_end_index == -1 or index < first_end_index):
            first_end_index = index
    if first_end_index != -1: return text[:first_end_index+1].strip()
    if minimum_cut_length > 0:
        first_end_index_any = -1
        for char in end_chars:
            index = text.find(char)
            if index != -1 and (first_end_index_any == -1 or index < first_end_index_any):
                first_end_index_any = index
        if first_end_index_any != -1: return text[:first_end_index_any+1].strip()
    return text.strip()

def cut_to_first_and_last_sentence(text):
    """Cut a text to just the first and last sentences."""
    sentences = []
    start = 0
    for i, char in enumerate(text):
        if char in '.!?\n':
            sentences.append(text[start:i+1].strip())
            start = i+1
    if start < len(text):
        ending = text[start:].strip()
        if ending: sentences.append(ending)
    if not sentences: return text
    elif len(sentences) == 1: return sentences[0]
    else: return sentences[0] + " " + sentences[-1]

def load_synthetic_data(data_dir: str, exclude_refusals: bool = True, include_adjectives: List[str] = None) -> Dict[str, List[Tuple[str, str, None]]]:
    """Loads all synthetic data from a directory of JSON files."""
    adjective_to_data = {}
    json_files = glob.glob(os.path.join(data_dir, '*.json'))
    if not json_files: raise FileNotFoundError(f"No JSON files found in directory: {data_dir}")
    print(f"Found {len(json_files)} JSON files to process.")
    include_adjectives_set = set(include_adjectives) if include_adjectives else None
    for file_path in json_files:
        adjective = Path(file_path).stem
        if include_adjectives_set is not None and adjective not in include_adjectives_set:
            print(f"Excluding file {file_path} (adjective '{adjective}') - not in include_adjectives.")
            continue
        try:
            with open(file_path, 'r', encoding='utf-8') as f: data = json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            print(f"Warning: Could not read or parse {file_path}. Skipping. Error: {e}")
            continue
        adjective_examples = []
        for item in data:
            if exclude_refusals and item.get('is_topical_response_refusal', False): continue
            prompt, topical_response = item.get("prompt"), item.get("topical_response")
            if prompt and topical_response: adjective_examples.append((prompt, topical_response, None))
        if adjective_examples:
            adjective_to_data[adjective] = adjective_examples
            print(f"Loaded {len(adjective_examples)} non-refusal examples for adjective '{adjective}'.")
    return adjective_to_data

def split_data(data_list: list, test_size: int, seed: int) -> Tuple[list, list]:
    """Splits a list of data into training and testing sets."""
    if test_size <= 0: return data_list, []
    random.Random(seed).shuffle(data_list)
    actual_test_size = min(test_size, len(data_list))
    if actual_test_size < test_size:
        print(f"Warning: Requested test size {test_size}, but only {len(data_list)} samples available. Using {actual_test_size}.")
    return data_list[actual_test_size:], data_list[:actual_test_size] # train_set, test_set

def get_probe_scores_with_data(model: HFHardPrompted, probe: TrainableMetric, data_samples: List[Tuple[str, str, None]], batch_size: int, desc: str) -> Tuple[List[float], List[Tuple[str, str, None]]]:
    """Runs data through the model and probe to get activation scores along with the original data."""
    all_scores = []
    all_data = []
    with torch.no_grad():
        for i in tqdm(range(0, len(data_samples), batch_size), desc=desc):
            batch = data_samples[i:i + batch_size]
            if not batch: continue
            prompts, responses = [d[0] for d in batch], [d[1] for d in batch]
            output = model.forward_from_string(
                input_text=prompts, target_text=responses, add_chat_template=True,
                layers_to_probe=probe.config.layers
            )
            scores = probe.predict_example(
                input_reps=output.input_reps, target_reps=output.target_reps, target_mask=output.loss_mask
            )
            all_scores.extend(scores.cpu().tolist())
            all_data.extend(batch)
            del output, scores
            gc.collect()
            torch.cuda.empty_cache()
    return all_scores, all_data

def get_top_bottom_examples(scores: List[float], data: List[Tuple[str, str, None]], n: int = 3) -> Dict:
    """Get top and bottom n examples based on scores."""
    if len(scores) != len(data):
        return {"top_examples": [], "bottom_examples": [], "error": "Scores and data length mismatch"}
    
    # Create list of (score, index) pairs and sort
    score_indices = [(score, i) for i, score in enumerate(scores)]
    score_indices.sort(key=lambda x: x[0], reverse=True)
    
    # Get top n and bottom n
    top_indices = [idx for _, idx in score_indices[:n]]
    bottom_indices = [idx for _, idx in score_indices[-n:]]
    
    top_examples = [{"score": scores[i], "prompt": data[i][0], "response": data[i][1]} for i in top_indices]
    bottom_examples = [{"score": scores[i], "prompt": data[i][0], "response": data[i][1]} for i in bottom_indices]
    
    return {"top_examples": top_examples, "bottom_examples": bottom_examples}

def bootstrap_tpr_at_fpr_with_thresholds(pos_scores: List[float], neg_scores: List[float], fpr_level: float = 0.01, num_iterations: int = 1000) -> Dict:
    """Calculates the mean TPR at a given FPR with confidence intervals and threshold statistics using bootstrapping."""
    if not pos_scores or not neg_scores:
        return {"mean_tpr": None, "tpr_ci_95": (None, None), "mean_threshold": None, "threshold_ci_95": (None, None), "error": "Not enough data for bootstrapping."}
    
    pos_scores_np, neg_scores_np = np.array(pos_scores), np.array(neg_scores)
    tpr_samples = []
    threshold_samples = []
    
    for _ in range(num_iterations):
        neg_resample = np.random.choice(neg_scores_np, size=len(neg_scores_np), replace=True)
        pos_resample = np.random.choice(pos_scores_np, size=len(pos_scores_np), replace=True)
        threshold = np.percentile(neg_resample, 100 * (1 - fpr_level))
        tpr = np.mean(pos_resample > threshold)
        tpr_samples.append(tpr)
        threshold_samples.append(threshold)
    
    mean_tpr = np.mean(tpr_samples)
    tpr_ci_lower, tpr_ci_upper = np.percentile(tpr_samples, 2.5), np.percentile(tpr_samples, 97.5)
    
    mean_threshold = np.mean(threshold_samples)
    threshold_ci_lower, threshold_ci_upper = np.percentile(threshold_samples, 2.5), np.percentile(threshold_samples, 97.5)
    
    return {
        "mean_tpr": mean_tpr, 
        "tpr_ci_95": (tpr_ci_lower, tpr_ci_upper),
        "mean_threshold": mean_threshold,
        "threshold_ci_95": (threshold_ci_lower, threshold_ci_upper)
    }

def test_and_save_synthetic_probes(
    probe_dir: str,
    output_dir_base: str,
    batch_size: int = 64,
    custom_folder_name: str = "",
    seed: int = 42,
    bootstrap_iterations: int = 1000,
    num_ultrachat_test_samples: int = 1000,
    ultrachat_split: str = "test_gen",
    filter_to_trained_adjectives: bool = True
):
    """Main function to load, test, and save results for pre-trained probes."""
    random.seed(seed)
    
    # 1. Setup Logger and Output Directory
    logger = CSVTXTLogger(print_logs_to_console=True)
    date_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    if custom_folder_name:
        directory = os.path.join(output_dir_base, custom_folder_name)
    else:
        probe_folder_name = Path(probe_dir).name
        directory = os.path.join(output_dir_base, f"{date_time}_TEST__{probe_folder_name}")
    os.makedirs(directory, exist_ok=True)
    logger.print(f"Output will be saved to: {directory}")

    # 2. Load Training Metadata and Model
    metadata_path = os.path.join(probe_dir, "metadata.json")
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"Could not find metadata.json in probe directory: {probe_dir}")
    with open(metadata_path, 'r') as f:
        train_metadata = json.load(f)
    
    logger.print("--- Loaded Training Metadata ---")
    logger.print(json.dumps(train_metadata, indent=2))
    
    model_name = train_metadata['model_name']
    model_checkpoint = train_metadata.get('model_checkpoint')
    synthetic_data_dir = train_metadata['synthetic_data_dir']
    num_test_samples = train_metadata['num_test_samples_per_class']

    model = load_local_model(checkpoint_path=model_checkpoint, model_name=model_name)
    logger.print(f"Loaded model: {model_checkpoint or model_name}")

    # 3. Load All Required Test Data
    logger.print(f"\nLoading synthetic data from {synthetic_data_dir}...")
    adjectives_to_include = train_metadata.get('adjectives_trained', []) if filter_to_trained_adjectives else None
    if filter_to_trained_adjectives and adjectives_to_include:
        logger.print(f"Filtering to trained adjectives: {adjectives_to_include}")
    adjective_to_data = load_synthetic_data(synthetic_data_dir, include_adjectives=adjectives_to_include)
    
    logger.print(f"\nLoading {num_ultrachat_test_samples} conversations from UltraChat ({ultrachat_split} split) for negative testing...")
    ultrachat_test_samples = []
    if num_ultrachat_test_samples > 0:
        ultrachat_dataset = load_ultrachat_sample(num_ultrachat_test_samples, split=ultrachat_split)
        for item in tqdm(ultrachat_dataset, desc="Processing UltraChat conversations"):
            conversation = item['messages']
            prompt, response = extract_prompt_response_from_conversation(
                conversation, cut_prompt_to_first_and_last_sentence=True,
                cut_response_to_first_sentence=True, minimum_response_cut_length=100
            )
            if prompt and response:
                ultrachat_test_samples.append((prompt, response, None))
    logger.print(f"Loaded {len(ultrachat_test_samples)} valid UltraChat samples for testing.")

    # 4. Find, Load, and Test Probes
    all_test_results = {}
    probe_files = glob.glob(os.path.join(probe_dir, "*.pkl"))
    logger.print(f"\nFound {len(probe_files)} probes to test in {probe_dir}")

    all_adjectives = sorted(list(adjective_to_data.keys()))

    for i, probe_path in enumerate(probe_files):
        probed_adjective = Path(probe_path).stem
        logger.print(f"\n--- Testing probe for '{probed_adjective}' ({i+1}/{len(probe_files)}) ---")

        with open(probe_path, "rb") as f:
            probe = pickle.load(f)

        # a. Prepare positive test samples
        all_pos_samples = adjective_to_data.get(probed_adjective, [])
        _, pos_test = split_data(all_pos_samples, num_test_samples, seed)
        
        # b. Prepare negative test samples
        neg_pool = [sample for adj, data in adjective_to_data.items() if adj != probed_adjective for sample in data]
        _, neg_test_synth = split_data(neg_pool, num_test_samples, seed)

        if not pos_test:
            logger.print(f"No positive test samples found for '{probed_adjective}' after split. Skipping.")
            continue
        
        # c. Evaluate the probe
        probe_test_results = {}
        logger.print("Evaluating probe...")
        logger.print(f"Testing with {len(pos_test)} pos / {len(neg_test_synth)} synthetic neg samples.")
        
        pos_scores_test, pos_data_test = get_probe_scores_with_data(model, probe, pos_test, batch_size, "Scoring positive test samples")
        
        # Get top/bottom examples for positive samples
        pos_examples = get_top_bottom_examples(pos_scores_test, pos_data_test)
        
        # --- Evaluation against SYNTHETIC negatives ---
        if neg_test_synth:
            logger.print("\nEvaluating against SYNTHETIC negative samples...")
            neg_scores_test_synth, neg_data_test_synth = get_probe_scores_with_data(model, probe, neg_test_synth, batch_size, "Scoring synthetic negative samples")
            bootstrap_results_synth = bootstrap_tpr_at_fpr_with_thresholds(pos_scores_test, neg_scores_test_synth, num_iterations=bootstrap_iterations)
            
            # Get top/bottom examples for synthetic negative samples
            neg_examples_synth = get_top_bottom_examples(neg_scores_test_synth, neg_data_test_synth)
            
            probe_test_results["synthetic_neg_results"] = {
                "tpr_at_1_fpr": bootstrap_results_synth.get("mean_tpr"),
                "tpr_at_1_fpr_95_ci": bootstrap_results_synth.get("tpr_ci_95"),
                "mean_1fpr_threshold": bootstrap_results_synth.get("mean_threshold"),
                "threshold_95_ci": bootstrap_results_synth.get("threshold_ci_95"),
                "num_pos_test_samples": len(pos_scores_test),
                "num_neg_test_samples": len(neg_scores_test_synth),
                "all_pos_scores": pos_scores_test,
                "all_neg_scores": neg_scores_test_synth,
                "top_3_pos_examples": pos_examples.get("top_examples", []),
                "bottom_3_pos_examples": pos_examples.get("bottom_examples", []),
                "top_3_neg_examples": neg_examples_synth.get("top_examples", []),
                "bottom_3_neg_examples": neg_examples_synth.get("bottom_examples", []),
                "error": bootstrap_results_synth.get("error")
            }
            logger.print("Synthetic Negatives Test Results:")
            logger.print(json.dumps(probe_test_results["synthetic_neg_results"], indent=2))
        
        # --- Evaluation against ULTRACHAT negatives ---
        if ultrachat_test_samples:
            logger.print("\nEvaluating against ULTRACHAT negative samples...")
            neg_scores_test_uc, neg_data_test_uc = get_probe_scores_with_data(model, probe, ultrachat_test_samples, batch_size, "Scoring UltraChat negative samples")
            bootstrap_results_uc = bootstrap_tpr_at_fpr_with_thresholds(pos_scores_test, neg_scores_test_uc, num_iterations=bootstrap_iterations)

            # Get top/bottom examples for UltraChat negative samples
            neg_examples_uc = get_top_bottom_examples(neg_scores_test_uc, neg_data_test_uc)

            probe_test_results["ultrachat_neg_results"] = {
                "tpr_at_1_fpr": bootstrap_results_uc.get("mean_tpr"),
                "tpr_at_1_fpr_95_ci": bootstrap_results_uc.get("tpr_ci_95"),
                "mean_1fpr_threshold": bootstrap_results_uc.get("mean_threshold"),
                "threshold_95_ci": bootstrap_results_uc.get("threshold_ci_95"),
                "num_pos_test_samples": len(pos_scores_test),
                "num_neg_test_samples": len(neg_scores_test_uc),
                "all_pos_scores": pos_scores_test,
                "all_neg_scores": neg_scores_test_uc,
                "top_3_pos_examples": pos_examples.get("top_examples", []),
                "bottom_3_pos_examples": pos_examples.get("bottom_examples", []),
                "top_3_neg_examples": neg_examples_uc.get("top_examples", []),
                "bottom_3_neg_examples": neg_examples_uc.get("bottom_examples", []),
                "error": bootstrap_results_uc.get("error")
            }
            logger.print("UltraChat Negatives Test Results:")
            logger.print(json.dumps(probe_test_results["ultrachat_neg_results"], indent=2))

        all_test_results[probed_adjective] = probe_test_results

    # 5. Save Final Metadata and Test Results
    test_metadata = {
        "test_run_date_time": date_time,
        "source_probe_dir": probe_dir,
        "source_training_metadata": train_metadata,
        "batch_size": batch_size,
        "seed": seed,
        "bootstrap_iterations": bootstrap_iterations,
        "num_ultrachat_test_samples": num_ultrachat_test_samples,
        "ultrachat_split": ultrachat_split,
        "filter_to_trained_adjectives": filter_to_trained_adjectives,
        "adjectives_used_for_testing": list(adjective_to_data.keys())
    }
    with open(os.path.join(directory, "metadata.json"), "w") as f:
        json.dump(test_metadata, f, indent=2)
        
    with open(os.path.join(directory, "test_results.json"), "w") as f:
        json.dump(all_test_results, f, indent=2)
    logger.print(f"\nSaved all test metadata and results to: {directory}")
    logger.print(f"\nFinished testing all probes.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Load and test pre-trained probes against synthetic and real-world data.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    # --- Core arguments ---
    parser.add_argument("--probe_dir", type=str, required=True, help="Directory containing the pre-trained probe .pkl files and metadata.json.")
    parser.add_argument("--output_dir_base", type=str, default="experiments/self_obfuscation_concepts/outputs/probe_test_results", help="Base directory to save test results.")
    parser.add_argument("--custom_folder_name", type=str, default="", help="Custom folder name for the output directory.")
    
    # --- Testing & Evaluation arguments ---
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for testing.")
    parser.add_argument("--bootstrap_iterations", type=int, default=1000, help="Number of resampling iterations for bootstrapping.")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility of data splitting.")
    
    # --- UltraChat testing arguments ---
    parser.add_argument("--num_ultrachat_test_samples", type=int, default=1024, help="Number of UltraChat conversations to use as a negative test set.")
    parser.add_argument("--ultrachat_split", type=str, default="test_gen", help="The split of the UltraChat dataset to use (e.g., 'test_gen', 'train_sft').")
    
    # --- Data filtering arguments ---
    parser.add_argument("--filter_to_trained_adjectives", action="store_true", default=True, help="Only load synthetic data for adjectives that were in adjectives_trained metadata (default: True).")
    parser.add_argument("--no_filter_to_trained_adjectives", dest="filter_to_trained_adjectives", action="store_false", help="Load synthetic data for all adjectives, not just trained ones.")

    args = parser.parse_args()

    # --- Example Command ---
    # python stage2_synthetic_probe_test.py \
    #   --probe_dir experiments/self_obfuscation_concepts/outputs/probe_checkpoints/20250627_051510_synthetic_logistic_12 \
    #   --num_ultrachat_test_samples 2000

    test_and_save_synthetic_probes(
        probe_dir=args.probe_dir,
        output_dir_base=args.output_dir_base,
        batch_size=args.batch_size,
        custom_folder_name=args.custom_folder_name,
        seed=args.seed,
        bootstrap_iterations=args.bootstrap_iterations,
        num_ultrachat_test_samples=args.num_ultrachat_test_samples,
        ultrachat_split=args.ultrachat_split,
        filter_to_trained_adjectives=args.filter_to_trained_adjectives
    )