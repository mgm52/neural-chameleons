"""
Script: stage2_synthetic_probe_test.py

Loads pre-trained probes and evaluates them on held-out test data.

This script takes a directory of probes (generated by stage2_synthetic_probe_train.py),
reloads the model used to train them, and evaluates their performance.
Bootstrapping is used to calculate confidence intervals for the True Positive Rate (TPR)
at a fixed False Positive Rate (FPR).

This script performs two types of testing for each probe:
1.  **In-Distribution**: Tests against held-out synthetic data from other adjectives.
2.  **Out-of-Distribution**: Tests against real-world conversations from the UltraChat dataset.

INPUTS:
- Directory of trained probes (containing .pkl files and a metadata.json)
- Directory of synthetic data JSON files (for creating the test sets)
- Output directory for test results
- UltraChat dataset (optional, for OOD testing, automatically downloaded)

OUTPUTS:
- Test results with bootstrapped metrics for both test scenarios (test_results.json)
- Testing metadata (JSON file)
"""
import argparse
import glob
import json
import os
import pickle
import random
import gc
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import torch
from tqdm import tqdm
from datasets import load_dataset

# Assuming these files are in the PYTHONPATH or a discoverable path
from obf_reps.logging import CSVTXTLogger
from obf_reps.metrics import TrainableMetric
from obf_reps.models.hf import HFHardPrompted
from experiments.self_obfuscation_v0.utils_misc import load_local_model

# Import utility modules
from experiments.self_obfuscation_v1.utils_data import (
    split_data, load_ultrachat_sample, 
    extract_prompt_response_from_conversation, cut_to_first_sentence, 
    cut_to_first_and_last_sentence
)
from data.self_obfuscation_v1_synthetic.load_synthetic_data import load_synthetic_data
from data.data import PosNegDataByCategory
from experiments.self_obfuscation_v1.utils_evaluation import (
    get_probe_scores_with_data, get_top_bottom_examples, 
    bootstrap_tpr_at_fpr_with_thresholds
)
from experiments.self_obfuscation_v1.utils_setup import (
    setup_experiment_directory, setup_logger, load_metadata, save_metadata
)

def test_and_save_synthetic_probes(
    probe_dir: str,
    output_dir_base: str,
    batch_size: int = 64,
    custom_folder_name: str = "",
    seed: int = 42,
    bootstrap_iterations: int = 1000,
    num_ultrachat_test_samples: int = 1000,
    ultrachat_split: str = "test_gen",
    filter_to_trained_adjectives: bool = True
):
    """Main function to load, test, and save results for pre-trained probes."""
    random.seed(seed)
    
    # 1. Setup Logger and Output Directory
    logger = setup_logger(print_to_console=True)
    
    # Create directory name
    probe_folder_name = Path(probe_dir).name
    additional_suffix = f"__{probe_folder_name}"
    
    directory = setup_experiment_directory(
        output_dir_base=output_dir_base,
        custom_folder_name=custom_folder_name,
        experiment_type="TEST",
        additional_suffix=additional_suffix
    )
    logger.print(f"Output will be saved to: {directory}")

    # 2. Load Training Metadata and Model
    train_metadata = load_metadata(probe_dir)
    
    logger.print("--- Loaded Training Metadata ---")
    logger.print(json.dumps(train_metadata, indent=2))
    
    model_name = train_metadata['model_name']
    model_checkpoint = train_metadata.get('model_checkpoint')
    synthetic_data_dir = train_metadata['synthetic_data_dir']
    num_test_samples = train_metadata['num_test_samples_per_class']

    model = load_local_model(checkpoint_path=model_checkpoint, model_name=model_name)
    logger.print(f"Loaded model: {model_checkpoint or model_name}")

    # 3. Load All Required Test Data
    logger.print(f"\nLoading synthetic data from {synthetic_data_dir}...")
    adjectives_to_include = train_metadata.get('adjectives_trained', []) if filter_to_trained_adjectives else None
    if filter_to_trained_adjectives and adjectives_to_include:
        logger.print(f"Filtering to trained adjectives: {adjectives_to_include}")
    
    # Load using the new data loading approach
    topical_data_dict = load_synthetic_data(
        data_dir=synthetic_data_dir,
        response_type="topical",
        manual_path_confirm=False,
        exclude_refusals=True,
        adjectives=adjectives_to_include
    )
    
    # Convert to expected format (prompt, response, None)
    adjective_to_data = {
        concept: [(pr.prompt, pr.response, None) for pr in prompt_resps]
        for concept, prompt_resps in topical_data_dict.items()
    }
    
    logger.print(f"\nLoading {num_ultrachat_test_samples} conversations from UltraChat ({ultrachat_split} split) for negative testing...")
    ultrachat_test_samples = []
    if num_ultrachat_test_samples > 0:
        ultrachat_dataset = load_ultrachat_sample(num_ultrachat_test_samples, split=ultrachat_split)
        for item in tqdm(ultrachat_dataset, desc="Processing UltraChat conversations"):
            conversation = item['messages']
            prompt, response = extract_prompt_response_from_conversation(
                conversation, cut_prompt_to_first_and_last_sentence=True,
                cut_response_to_first_sentence=True, minimum_response_cut_length=100
            )
            if prompt and response:
                ultrachat_test_samples.append((prompt, response, None))
    logger.print(f"Loaded {len(ultrachat_test_samples)} valid UltraChat samples for testing.")

    # 4. Find, Load, and Test Probes
    all_test_results = {}
    probe_files = glob.glob(os.path.join(probe_dir, "*.pkl"))
    logger.print(f"\nFound {len(probe_files)} probes to test in {probe_dir}")

    all_adjectives = sorted(list(adjective_to_data.keys()))

    for i, probe_path in enumerate(probe_files):
        probed_adjective = Path(probe_path).stem
        logger.print(f"\n--- Testing probe for '{probed_adjective}' ({i+1}/{len(probe_files)}) ---")

        with open(probe_path, "rb") as f:
            probe = pickle.load(f)

        # a. Prepare positive test samples
        all_pos_samples = adjective_to_data.get(probed_adjective, [])
        _, pos_test = split_data(all_pos_samples, num_test_samples, seed)
        
        # b. Prepare negative test samples
        neg_pool = [sample for adj, data in adjective_to_data.items() if adj != probed_adjective for sample in data]
        _, neg_test_synth = split_data(neg_pool, num_test_samples, seed)

        if not pos_test:
            logger.print(f"No positive test samples found for '{probed_adjective}' after split. Skipping.")
            continue
        
        # c. Evaluate the probe
        probe_test_results = {}
        logger.print("Evaluating probe...")
        logger.print(f"Testing with {len(pos_test)} pos / {len(neg_test_synth)} synthetic neg samples.")
        
        pos_scores_test, pos_data_test = get_probe_scores_with_data(model, probe, pos_test, batch_size, "Scoring positive test samples")
        
        # Get top/bottom examples for positive samples
        pos_examples = get_top_bottom_examples(pos_scores_test, pos_data_test)
        
        # --- Evaluation against SYNTHETIC negatives ---
        if neg_test_synth:
            logger.print("\nEvaluating against SYNTHETIC negative samples...")
            neg_scores_test_synth, neg_data_test_synth = get_probe_scores_with_data(model, probe, neg_test_synth, batch_size, "Scoring synthetic negative samples")
            bootstrap_results_synth = bootstrap_tpr_at_fpr_with_thresholds(pos_scores_test, neg_scores_test_synth, num_iterations=bootstrap_iterations)
            
            # Get top/bottom examples for synthetic negative samples
            neg_examples_synth = get_top_bottom_examples(neg_scores_test_synth, neg_data_test_synth)
            
            probe_test_results["synthetic_neg_results"] = {
                "tpr_at_1_fpr": bootstrap_results_synth.get("mean_tpr"),
                "tpr_at_1_fpr_95_ci": bootstrap_results_synth.get("tpr_ci_95"),
                "mean_1fpr_threshold": bootstrap_results_synth.get("mean_threshold"),
                "threshold_95_ci": bootstrap_results_synth.get("threshold_ci_95"),
                "num_pos_test_samples": len(pos_scores_test),
                "num_neg_test_samples": len(neg_scores_test_synth),
                "all_pos_scores": pos_scores_test,
                "all_neg_scores": neg_scores_test_synth,
                "top_3_pos_examples": pos_examples.get("top_examples", []),
                "bottom_3_pos_examples": pos_examples.get("bottom_examples", []),
                "top_3_neg_examples": neg_examples_synth.get("top_examples", []),
                "bottom_3_neg_examples": neg_examples_synth.get("bottom_examples", []),
                "error": bootstrap_results_synth.get("error")
            }
            logger.print("Synthetic Negatives Test Results:")
            logger.print(json.dumps(probe_test_results["synthetic_neg_results"], indent=2))
        
        # --- Evaluation against ULTRACHAT negatives ---
        if ultrachat_test_samples:
            logger.print("\nEvaluating against ULTRACHAT negative samples...")
            neg_scores_test_uc, neg_data_test_uc = get_probe_scores_with_data(model, probe, ultrachat_test_samples, batch_size, "Scoring UltraChat negative samples")
            bootstrap_results_uc = bootstrap_tpr_at_fpr_with_thresholds(pos_scores_test, neg_scores_test_uc, num_iterations=bootstrap_iterations)

            # Get top/bottom examples for UltraChat negative samples
            neg_examples_uc = get_top_bottom_examples(neg_scores_test_uc, neg_data_test_uc)

            probe_test_results["ultrachat_neg_results"] = {
                "tpr_at_1_fpr": bootstrap_results_uc.get("mean_tpr"),
                "tpr_at_1_fpr_95_ci": bootstrap_results_uc.get("tpr_ci_95"),
                "mean_1fpr_threshold": bootstrap_results_uc.get("mean_threshold"),
                "threshold_95_ci": bootstrap_results_uc.get("threshold_ci_95"),
                "num_pos_test_samples": len(pos_scores_test),
                "num_neg_test_samples": len(neg_scores_test_uc),
                "all_pos_scores": pos_scores_test,
                "all_neg_scores": neg_scores_test_uc,
                "top_3_pos_examples": pos_examples.get("top_examples", []),
                "bottom_3_pos_examples": pos_examples.get("bottom_examples", []),
                "top_3_neg_examples": neg_examples_uc.get("top_examples", []),
                "bottom_3_neg_examples": neg_examples_uc.get("bottom_examples", []),
                "error": bootstrap_results_uc.get("error")
            }
            logger.print("UltraChat Negatives Test Results:")
            logger.print(json.dumps(probe_test_results["ultrachat_neg_results"], indent=2))

        all_test_results[probed_adjective] = probe_test_results

    # 5. Save Final Metadata and Test Results
    test_metadata = {
        "test_run_date_time": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "source_probe_dir": probe_dir,
        "source_training_metadata": train_metadata,
        "batch_size": batch_size,
        "seed": seed,
        "bootstrap_iterations": bootstrap_iterations,
        "num_ultrachat_test_samples": num_ultrachat_test_samples,
        "ultrachat_split": ultrachat_split,
        "filter_to_trained_adjectives": filter_to_trained_adjectives,
        "adjectives_used_for_testing": list(adjective_to_data.keys())
    }
    save_metadata(directory, test_metadata)
        
    with open(os.path.join(directory, "test_results.json"), "w") as f:
        json.dump(all_test_results, f, indent=2)
    logger.print(f"\nSaved all test metadata and results to: {directory}")
    logger.print(f"\nFinished testing all probes.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Load and test pre-trained probes against synthetic and real-world data.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    # --- Core arguments ---
    parser.add_argument("--probe_dir", type=str, required=True, help="Directory containing the pre-trained probe .pkl files and metadata.json.")
    parser.add_argument("--output_dir_base", type=str, default="experiments/self_obfuscation_concepts/outputs/probe_test_results", help="Base directory to save test results.")
    parser.add_argument("--custom_folder_name", type=str, default="", help="Custom folder name for the output directory.")
    
    # --- Testing & Evaluation arguments ---
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for testing.")
    parser.add_argument("--bootstrap_iterations", type=int, default=1000, help="Number of resampling iterations for bootstrapping.")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility of data splitting.")
    
    # --- UltraChat testing arguments ---
    parser.add_argument("--num_ultrachat_test_samples", type=int, default=1024, help="Number of UltraChat conversations to use as a negative test set.")
    parser.add_argument("--ultrachat_split", type=str, default="test_gen", help="The split of the UltraChat dataset to use (e.g., 'test_gen', 'train_sft').")
    
    # --- Data filtering arguments ---
    parser.add_argument("--filter_to_trained_adjectives", action="store_true", default=True, help="Only load synthetic data for adjectives that were in adjectives_trained metadata (default: True).")
    parser.add_argument("--no_filter_to_trained_adjectives", dest="filter_to_trained_adjectives", action="store_false", help="Load synthetic data for all adjectives, not just trained ones.")

    args = parser.parse_args()

    # --- Example Command ---
    # python stage2_synthetic_probe_test.py \
    #   --probe_dir experiments/self_obfuscation_concepts/outputs/probe_checkpoints/20250627_051510_synthetic_logistic_12 \
    #   --num_ultrachat_test_samples 2000

    test_and_save_synthetic_probes(
        probe_dir=args.probe_dir,
        output_dir_base=args.output_dir_base,
        batch_size=args.batch_size,
        custom_folder_name=args.custom_folder_name,
        seed=args.seed,
        bootstrap_iterations=args.bootstrap_iterations,
        num_ultrachat_test_samples=args.num_ultrachat_test_samples,
        ultrachat_split=args.ultrachat_split,
        filter_to_trained_adjectives=args.filter_to_trained_adjectives
    )