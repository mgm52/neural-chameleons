# probe_test_synthetic_labelled.py

"""
Script: probe_test_synthetic_labelled.py

Loads pre-trained probes and evaluates them on labelled mega dataset test data.

This script takes a directory of probes (generated by probe_train_synthetic_labelled.py),
reloads the model used to train them, and evaluates their performance on test data
extracted from the evaluated mega dataset using normalized ratings.

This script performs two types of testing for each probe:
1.  **In-Distribution**: Tests against held-out labelled data from the same mega dataset.
2.  **Out-of-Distribution**: Tests against real-world conversations from the UltraChat dataset.

INPUTS:
- Directory of trained probes (containing .pkl files and a metadata.json)
- Path to evaluated mega dataset JSON file
- Output directory for test results
- UltraChat dataset (optional, for OOD testing, automatically downloaded)

OUTPUTS:
- Test results with bootstrapped metrics for both test scenarios (test_results.json)
- Testing metadata (JSON file)
"""
import argparse
import json
import os
import pickle
import random
import gc
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import torch
from tqdm import tqdm
from datasets import load_dataset

# Assuming these files are in the PYTHONPATH or a discoverable path
from obf_reps.logging import CSVTXTLogger
from obf_reps.metrics import TrainableMetric
from obf_reps.models.hf import HFHardPrompted
from obf_reps.data import ConceptDataModule
from experiments.self_obfuscation_v0.utils_misc import load_local_model
from experiments.self_obfuscation_v0.utils_training import ResponseProbeDataModule

def load_mega_dataset(dataset_path: str, use_topical_ratings: bool = True) -> List[Dict]:
    """
    Loads the evaluated mega dataset from JSON file.
    
    Args:
        dataset_path: Path to the mega dataset JSON file
        use_topical_ratings: Whether to use topical_response ratings or vanilla_response ratings
    
    Returns:
        List of dataset items
    """
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"Loaded {len(data)} samples from {dataset_path}")
    return data

def extract_adjective_data(data: List[Dict], adjective: str, positive_threshold: float = 0.75, 
                          negative_threshold: float = 0.25, use_topical_ratings: bool = True,
                          exclude_refusals: bool = True) -> Tuple[List[Tuple[str, str, None]], List[Tuple[str, str, None]]]:
    """
    Extract positive and negative samples for a specific adjective based on normalized ratings.
    
    Args:
        data: List of dataset items
        adjective: The adjective to extract data for
        positive_threshold: Threshold for positive samples (>= this value)
        negative_threshold: Threshold for negative samples (<= this value)
        use_topical_ratings: Whether to use topical_response ratings or vanilla_response ratings
        exclude_refusals: Whether to exclude refusal samples
    
    Returns:
        Tuple of (positive_samples, negative_samples)
    """
    positive_samples = []
    negative_samples = []
    
    # Determine which ratings to use
    prompt_ratings_key = "prompt_normalized_ratings"
    response_ratings_key = "topical_response_normalized_ratings" if use_topical_ratings and ("topical_response_normalized_ratings" in data[0]) else "vanilla_response_normalized_ratings"
    response_key = "topical_response" if use_topical_ratings else "vanilla_response"
    refusal_key = "is_topical_response_refusal" if use_topical_ratings else "is_vanilla_response_refusal"
    
    for item in data:
        # Skip refusals if requested
        if exclude_refusals and item.get(refusal_key, False):
            continue
            
        # Skip if we don't have the required ratings
        if (prompt_ratings_key not in item or 
            response_ratings_key not in item or 
            adjective not in item[prompt_ratings_key] or 
            adjective not in item[response_ratings_key]):
            continue
            
        prompt = item.get("prompt")
        response = item.get(response_key)
        
        if not prompt or not response or item[prompt_ratings_key][adjective] is None or item[response_ratings_key][adjective] is None:
            continue
            
        # Get ratings for this adjective
        prompt_rating = item[prompt_ratings_key][adjective]
        response_rating = item[response_ratings_key][adjective]
        
        # Check if response rating meets the threshold
        sample_tuple = (prompt, response, None)
        
        print(f"For {adjective}: prompt {prompt_rating} response {response_rating} key {response_ratings_key}")

        if response_rating >= positive_threshold:
            positive_samples.append(sample_tuple)
        elif response_rating <= negative_threshold:
            negative_samples.append(sample_tuple)
        # Samples in between thresholds are ignored
    
    return positive_samples, negative_samples

def get_all_adjectives(data: List[Dict], use_topical_ratings: bool = True) -> List[str]:
    """
    Get all adjectives present in the dataset.
    
    Args:
        data: List of dataset items
        use_topical_ratings: Whether to use topical_response ratings or vanilla_response ratings
    
    Returns:
        List of adjective names
    """
    adjectives = set()
    
    prompt_ratings_key = "prompt_normalized_ratings"
    response_ratings_key = "topical_response_normalized_ratings" if use_topical_ratings else "vanilla_response_normalized_ratings"
    
    for item in data:
        if prompt_ratings_key in item:
            adjectives.update(item[prompt_ratings_key].keys())
        if response_ratings_key in item:
            adjectives.update(item[response_ratings_key].keys())
    
    return sorted(list(adjectives))

def load_ultrachat_mega_dataset(dataset_path: str) -> List[Dict]:
    """
    Loads the UltraChat mega dataset from JSON file.
    
    Args:
        dataset_path: Path to the UltraChat mega dataset JSON file
    
    Returns:
        List of UltraChat dataset items with adjective ratings
    """
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"Loaded {len(data)} UltraChat samples from {dataset_path}")
    return data

def extract_ultrachat_negatives_for_adjective(ultrachat_data: List[Dict], adjective: str, 
                                            negative_threshold: float = 0.25,
                                            max_samples: int = 1000) -> List[Tuple[str, str, None]]:
    """
    Extract negative samples for a specific adjective from UltraChat mega dataset.
    
    Args:
        ultrachat_data: List of UltraChat items with ratings
        adjective: The adjective to extract negative data for
        negative_threshold: Threshold for negative samples (<= this value)
        max_samples: Maximum number of samples to return
    
    Returns:
        List of negative samples for this adjective
    """
    negative_samples = []
    
    for item in ultrachat_data:
        # Skip if we don't have the required ratings
        if ("prompt_normalized_ratings" not in item or 
            "response_normalized_ratings" not in item or 
            adjective not in item["prompt_normalized_ratings"] or 
            adjective not in item["response_normalized_ratings"]):
            continue
            
        prompt = item.get("prompt")
        response = item.get("response")
        
        if not prompt or not response:
            continue
            
        # Get ratings for this adjective
        prompt_rating = item["prompt_normalized_ratings"][adjective]
        response_rating = item["response_normalized_ratings"][adjective]
        
        if prompt_rating is None or response_rating is None:
            continue
        
        # Check if both prompt and response ratings are below threshold (strong negatives)
        max_rating = max(prompt_rating, response_rating)
        
        if max_rating <= negative_threshold:
            negative_samples.append((prompt, response, None))
            
            # Stop if we have enough samples
            if len(negative_samples) >= max_samples:
                break
    
    return negative_samples

def load_ultrachat_sample(num_conversations=100, split="test_gen"):
    """Load a sample from UltraChat dataset (legacy function, kept for fallback)."""
    dataset = load_dataset("HuggingFaceH4/ultrachat_200k", split=split, trust_remote_code=True)
    sampled_dataset = dataset.select(range(min(num_conversations, len(dataset))))
    return sampled_dataset

def extract_prompt_response_from_conversation(conversation, cut_prompt_to_first_and_last_sentence=False, cut_response_to_first_sentence=False, minimum_response_cut_length=-1):
    """Extract a prompt/response pair from a conversation, with safety checks."""
    if not conversation or len(conversation) < 2: return None, None
    prompt_msg, response_msg = conversation[0], conversation[1]
    if not isinstance(prompt_msg, dict) or not isinstance(response_msg, dict): return None, None
    prompt, response = prompt_msg.get("content"), response_msg.get("content")
    if not prompt or not response: return None, None
    if cut_prompt_to_first_and_last_sentence: prompt = cut_to_first_and_last_sentence(prompt)
    if cut_response_to_first_sentence: response = cut_to_first_sentence(response, minimum_response_cut_length)
    return prompt, response

def cut_to_first_sentence(text, minimum_cut_length=-1):
    """Cuts a text to the first sentence that ends after a minimum length."""
    end_chars = '.!?\n'
    start_search = minimum_cut_length if minimum_cut_length > 0 else 0
    first_end_index = -1
    for char in end_chars:
        index = text.find(char, start_search)
        if index != -1 and (first_end_index == -1 or index < first_end_index):
            first_end_index = index
    if first_end_index != -1: return text[:first_end_index+1].strip()
    if minimum_cut_length > 0:
        first_end_index_any = -1
        for char in end_chars:
            index = text.find(char)
            if index != -1 and (first_end_index_any == -1 or index < first_end_index_any):
                first_end_index_any = index
        if first_end_index_any != -1: return text[:first_end_index_any+1].strip()
    return text.strip()

def cut_to_first_and_last_sentence(text):
    """Cut a text to just the first and last sentences."""
    sentences = []
    start = 0
    for i, char in enumerate(text):
        if char in '.!?\n':
            sentences.append(text[start:i+1].strip())
            start = i+1
    if start < len(text):
        ending = text[start:].strip()
        if ending: sentences.append(ending)
    if not sentences: return text
    elif len(sentences) == 1: return sentences[0]
    else: return sentences[0] + " " + sentences[-1]

def split_data(data_list: list, test_size: int, seed: int) -> Tuple[list, list]:
    """Splits a list of data into training and testing sets."""
    if test_size <= 0: return data_list, []
    random.Random(seed).shuffle(data_list)
    actual_test_size = min(test_size, len(data_list))
    if actual_test_size < test_size:
        print(f"Warning: Requested test size {test_size}, but only {len(data_list)} samples available. Using {actual_test_size}.")
    return data_list[actual_test_size:], data_list[:actual_test_size] # train_set, test_set

def get_probe_scores_with_data(model: HFHardPrompted, probe: TrainableMetric, data_samples: List[Tuple[str, str, None]], batch_size: int, desc: str) -> Tuple[List[float], List[Tuple[str, str, None]]]:
    """Runs data through the model and probe to get activation scores along with the original data."""
    all_scores = []
    all_data = []
    with torch.no_grad():
        for i in tqdm(range(0, len(data_samples), batch_size), desc=desc):
            batch = data_samples[i:i + batch_size]
            if not batch: continue
            prompts, responses = [d[0] for d in batch], [d[1] for d in batch]
            output = model.forward_from_string(
                input_text=prompts, target_text=responses, add_chat_template=True,
                layers_to_probe=probe.config.layers
            )
            scores = probe.predict_example(
                input_reps=output.input_reps, target_reps=output.target_reps, target_mask=output.loss_mask
            )
            all_scores.extend(scores.cpu().tolist())
            all_data.extend(batch)
            del output, scores
            gc.collect()
            torch.cuda.empty_cache()
    return all_scores, all_data

def get_top_bottom_examples(scores: List[float], data: List[Tuple[str, str, None]], n: int = 3) -> Dict:
    """Get top and bottom n examples based on scores."""
    if len(scores) != len(data):
        return {"top_examples": [], "bottom_examples": [], "error": "Scores and data length mismatch"}
    
    # Create list of (score, index) pairs and sort
    score_indices = [(score, i) for i, score in enumerate(scores)]
    score_indices.sort(key=lambda x: x[0], reverse=True)
    
    # Get top n and bottom n
    top_indices = [idx for _, idx in score_indices[:n]]
    bottom_indices = [idx for _, idx in score_indices[-n:]]
    
    top_examples = [{"score": scores[i], "prompt": data[i][0], "response": data[i][1]} for i in top_indices]
    bottom_examples = [{"score": scores[i], "prompt": data[i][0], "response": data[i][1]} for i in bottom_indices]
    
    return {"top_examples": top_examples, "bottom_examples": bottom_examples}

def bootstrap_tpr_at_fpr_with_thresholds(pos_scores: List[float], neg_scores: List[float], fpr_level: float = 0.01, num_iterations: int = 1000) -> Dict:
    """Calculates the mean TPR at a given FPR with confidence intervals and threshold statistics using bootstrapping."""
    if not pos_scores or not neg_scores:
        return {"mean_tpr": None, "tpr_ci_95": (None, None), "mean_threshold": None, "threshold_ci_95": (None, None), "error": "Not enough data for bootstrapping."}
    
    pos_scores_np, neg_scores_np = np.array(pos_scores), np.array(neg_scores)
    tpr_samples = []
    threshold_samples = []
    
    for _ in range(num_iterations):
        neg_resample = np.random.choice(neg_scores_np, size=len(neg_scores_np), replace=True)
        pos_resample = np.random.choice(pos_scores_np, size=len(pos_scores_np), replace=True)
        threshold = np.percentile(neg_resample, 100 * (1 - fpr_level))
        tpr = np.mean(pos_resample > threshold)
        tpr_samples.append(tpr)
        threshold_samples.append(threshold)
    
    mean_tpr = np.mean(tpr_samples)
    tpr_ci_lower, tpr_ci_upper = np.percentile(tpr_samples, 2.5), np.percentile(tpr_samples, 97.5)
    
    mean_threshold = np.mean(threshold_samples)
    threshold_ci_lower, threshold_ci_upper = np.percentile(threshold_samples, 2.5), np.percentile(threshold_samples, 97.5)
    
    return {
        "mean_tpr": mean_tpr, 
        "tpr_ci_95": (tpr_ci_lower, tpr_ci_upper),
        "mean_threshold": mean_threshold,
        "threshold_ci_95": (threshold_ci_lower, threshold_ci_upper)
    }

def test_and_save_labelled_probes(
    probe_dir: str,
    dataset_path: str,
    output_dir_base: str,
    batch_size: int = 64,
    custom_folder_name: str = "",
    seed: int = 42,
    bootstrap_iterations: int = 1000,
    num_ultrachat_test_samples: int = 1000,
    ultrachat_split: str = "test_gen",
    positive_threshold: float = 0.75,
    negative_threshold: float = 0.25,
    use_topical_ratings: bool = True,
    min_test_samples_per_class: int = 100,
    filter_to_trained_adjectives: bool = True,
    ultrachat_mega_dataset_path: str = ""
):
    """Main function to load, test, and save results for pre-trained probes on labelled data."""
    random.seed(seed)
    
    # 1. Setup Logger and Output Directory
    logger = CSVTXTLogger(print_logs_to_console=True)
    date_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    if custom_folder_name:
        directory = os.path.join(output_dir_base, custom_folder_name)
    else:
        probe_folder_name = Path(probe_dir).name
        rating_type = "topical" if use_topical_ratings else "vanilla"
        directory = os.path.join(output_dir_base, f"{date_time}_TEST_labelled_{rating_type}__{probe_folder_name}")
    os.makedirs(directory, exist_ok=True)
    logger.print(f"Output will be saved to: {directory}")

    # 2. Load Training Metadata and Model
    metadata_path = os.path.join(probe_dir, "metadata.json")
    if not os.path.exists(metadata_path):
        raise FileNotFoundError(f"Could not find metadata.json in probe directory: {probe_dir}")
    with open(metadata_path, 'r') as f:
        train_metadata = json.load(f)
    
    logger.print("--- Loaded Training Metadata ---")
    logger.print(json.dumps(train_metadata, indent=2))
    
    model_name = train_metadata['model_name']
    model_checkpoint = train_metadata.get('model_checkpoint')

    model = load_local_model(checkpoint_path=model_checkpoint, model_name=model_name)
    logger.print(f"Loaded model: {model_checkpoint or model_name}")

    # 3. Load Mega Dataset
    logger.print(f"\nLoading mega dataset from {dataset_path}...")
    data = load_mega_dataset(dataset_path, use_topical_ratings)
    
    # Get adjectives to test
    adjectives_to_include = train_metadata.get('adjectives_trained', []) if filter_to_trained_adjectives else None
    if filter_to_trained_adjectives and adjectives_to_include:
        logger.print(f"Filtering to trained adjectives: {adjectives_to_include}")
        all_adjectives = [adj for adj in adjectives_to_include if adj in get_all_adjectives(data, use_topical_ratings)]
    else:
        all_adjectives = get_all_adjectives(data, use_topical_ratings)
        logger.print(f"Found {len(all_adjectives)} adjectives in dataset")
    
    # Load UltraChat mega dataset if provided
    ultrachat_mega_data = []
    if ultrachat_mega_dataset_path and os.path.exists(ultrachat_mega_dataset_path):
        logger.print(f"\nLoading UltraChat mega dataset from {ultrachat_mega_dataset_path}...")
        ultrachat_mega_data = load_ultrachat_mega_dataset(ultrachat_mega_dataset_path)
    elif num_ultrachat_test_samples > 0:
        # Fallback to legacy UltraChat loading if mega dataset not provided
        logger.print(f"\nLoading {num_ultrachat_test_samples} conversations from UltraChat ({ultrachat_split} split) for negative testing...")
        ultrachat_test_samples = []
        ultrachat_dataset = load_ultrachat_sample(num_ultrachat_test_samples, split=ultrachat_split)
        for item in tqdm(ultrachat_dataset, desc="Processing UltraChat conversations"):
            conversation = item['messages']
            prompt, response = extract_prompt_response_from_conversation(
                conversation, cut_prompt_to_first_and_last_sentence=True,
                cut_response_to_first_sentence=True, minimum_response_cut_length=100
            )
            if prompt and response:
                ultrachat_test_samples.append((prompt, response, None))
        logger.print(f"Loaded {len(ultrachat_test_samples)} valid UltraChat samples for testing.")
    else:
        ultrachat_test_samples = []

    # 4. Find, Load, and Test Probes
    all_test_results = {}
    probe_files = [os.path.join(probe_dir, f"{adj}.pkl") for adj in all_adjectives if os.path.exists(os.path.join(probe_dir, f"{adj}.pkl"))]
    logger.print(f"\nFound {len(probe_files)} probes to test in {probe_dir}")

    for i, probe_path in enumerate(probe_files):
        probed_adjective = Path(probe_path).stem
        logger.print(f"\n--- Testing probe for '{probed_adjective}' ({i+1}/{len(probe_files)}) ---")

        with open(probe_path, "rb") as f:
            probe = pickle.load(f)

        dummy_data_module = ResponseProbeDataModule(
            [],
            []
        )
        probe.data_module = dummy_data_module

        # Extract positive and negative samples for this adjective
        pos_samples, neg_samples = extract_adjective_data(
            data, probed_adjective, positive_threshold, negative_threshold, 
            use_topical_ratings, exclude_refusals=True
        )
        
        logger.print(f"Found {len(pos_samples)} positive and {len(neg_samples)} negative samples for '{probed_adjective}'")
        
        # Check if we have enough samples for testing
        if len(pos_samples) < min_test_samples_per_class or len(neg_samples) < min_test_samples_per_class:
            logger.print(f"Insufficient samples for testing '{probed_adjective}' (need at least {min_test_samples_per_class} per class). Skipping.")
            continue

        # Use all samples for testing (since this is a test script, not training)
        pos_test = pos_samples[:min_test_samples_per_class] if len(pos_samples) > min_test_samples_per_class else pos_samples
        neg_test = neg_samples[:min_test_samples_per_class] if len(neg_samples) > min_test_samples_per_class else neg_samples
        
        # Balance test sets
        min_test_size = min(len(pos_test), len(neg_test))
        pos_test = random.sample(pos_test, min_test_size)
        neg_test = random.sample(neg_test, min_test_size)
        
        logger.print(f"Testing with {len(pos_test)} pos / {len(neg_test)} neg samples.")
        
        # c. Evaluate the probe
        probe_test_results = {}
        logger.print("Evaluating probe...")
        
        pos_scores_test, pos_data_test = get_probe_scores_with_data(model, probe, pos_test, batch_size, "Scoring positive test samples")
        
        # Get top/bottom examples for positive samples
        pos_examples = get_top_bottom_examples(pos_scores_test, pos_data_test)
        
        # --- Evaluation against LABELLED negatives ---
        if neg_test:
            logger.print("\nEvaluating against LABELLED negative samples...")
            neg_scores_test, neg_data_test = get_probe_scores_with_data(model, probe, neg_test, batch_size, "Scoring labelled negative samples")
            bootstrap_results = bootstrap_tpr_at_fpr_with_thresholds(pos_scores_test, neg_scores_test, num_iterations=bootstrap_iterations)
            
            # Get top/bottom examples for negative samples
            neg_examples = get_top_bottom_examples(neg_scores_test, neg_data_test)
            
            probe_test_results["labelled_neg_results"] = {
                "tpr_at_1_fpr": bootstrap_results.get("mean_tpr"),
                "tpr_at_1_fpr_95_ci": bootstrap_results.get("tpr_ci_95"),
                "mean_1fpr_threshold": bootstrap_results.get("mean_threshold"),
                "threshold_95_ci": bootstrap_results.get("threshold_ci_95"),
                "num_pos_test_samples": len(pos_scores_test),
                "num_neg_test_samples": len(neg_scores_test),
                "all_pos_scores": pos_scores_test,
                "all_neg_scores": neg_scores_test,
                "top_3_pos_examples": pos_examples.get("top_examples", []),
                "bottom_3_pos_examples": pos_examples.get("bottom_examples", []),
                "top_3_neg_examples": neg_examples.get("top_examples", []),
                "bottom_3_neg_examples": neg_examples.get("bottom_examples", []),
                "error": bootstrap_results.get("error")
            }
            logger.print("Labelled Negatives Test Results:")
            logger.print(json.dumps(probe_test_results["labelled_neg_results"], indent=2))
        
        # --- Evaluation against ULTRACHAT negatives ---
        ultrachat_neg_samples = []
        
        # Use adjective-specific negatives from mega dataset if available
        if ultrachat_mega_data:
            logger.print(f"\nExtracting UltraChat negative samples for '{probed_adjective}'...")
            ultrachat_neg_samples = extract_ultrachat_negatives_for_adjective(
                ultrachat_mega_data, probed_adjective, negative_threshold, num_ultrachat_test_samples
            )
            logger.print(f"Found {len(ultrachat_neg_samples)} UltraChat negative samples for '{probed_adjective}'")
        elif 'ultrachat_test_samples' in locals() and ultrachat_test_samples:
            # Fallback to generic UltraChat samples
            logger.print("\nUsing generic UltraChat negative samples...")
            ultrachat_neg_samples = ultrachat_test_samples[:num_ultrachat_test_samples]
        
        if ultrachat_neg_samples:
            logger.print("\nEvaluating against ULTRACHAT negative samples...")
            neg_scores_test_uc, neg_data_test_uc = get_probe_scores_with_data(model, probe, ultrachat_neg_samples, batch_size, "Scoring UltraChat negative samples")
            bootstrap_results_uc = bootstrap_tpr_at_fpr_with_thresholds(pos_scores_test, neg_scores_test_uc, num_iterations=bootstrap_iterations)

            # Get top/bottom examples for UltraChat negative samples
            neg_examples_uc = get_top_bottom_examples(neg_scores_test_uc, neg_data_test_uc)

            probe_test_results["ultrachat_neg_results"] = {
                "tpr_at_1_fpr": bootstrap_results_uc.get("mean_tpr"),
                "tpr_at_1_fpr_95_ci": bootstrap_results_uc.get("tpr_ci_95"),
                "mean_1fpr_threshold": bootstrap_results_uc.get("mean_threshold"),
                "threshold_95_ci": bootstrap_results_uc.get("threshold_ci_95"),
                "num_pos_test_samples": len(pos_scores_test),
                "num_neg_test_samples": len(neg_scores_test_uc),
                "all_pos_scores": pos_scores_test,
                "all_neg_scores": neg_scores_test_uc,
                "top_3_pos_examples": pos_examples.get("top_examples", []),
                "bottom_3_pos_examples": pos_examples.get("bottom_examples", []),
                "top_3_neg_examples": neg_examples_uc.get("top_examples", []),
                "bottom_3_neg_examples": neg_examples_uc.get("bottom_examples", []),
                "error": bootstrap_results_uc.get("error"),
                "adjective_specific_negatives": bool(ultrachat_mega_data)
            }
            logger.print("UltraChat Negatives Test Results:")
            logger.print(json.dumps(probe_test_results["ultrachat_neg_results"], indent=2))

        all_test_results[probed_adjective] = probe_test_results

    # 5. Save Final Metadata and Test Results
    test_metadata = {
        "test_run_date_time": date_time,
        "source_probe_dir": probe_dir,
        "source_training_metadata": train_metadata,
        "dataset_path": dataset_path,
        "batch_size": batch_size,
        "seed": seed,
        "bootstrap_iterations": bootstrap_iterations,
        "num_ultrachat_test_samples": num_ultrachat_test_samples,
        "ultrachat_split": ultrachat_split,
        "positive_threshold": positive_threshold,
        "negative_threshold": negative_threshold,
        "use_topical_ratings": use_topical_ratings,
        "min_test_samples_per_class": min_test_samples_per_class,
        "filter_to_trained_adjectives": filter_to_trained_adjectives,
        "ultrachat_mega_dataset_path": ultrachat_mega_dataset_path,
        "using_adjective_specific_ultrachat_negatives": bool(ultrachat_mega_data),
        "adjectives_tested": list(all_test_results.keys())
    }
    with open(os.path.join(directory, "metadata.json"), "w") as f:
        json.dump(test_metadata, f, indent=2)
        
    with open(os.path.join(directory, "test_results.json"), "w") as f:
        json.dump(all_test_results, f, indent=2)
    logger.print(f"\nSaved all test metadata and results to: {directory}")
    logger.print(f"\nFinished testing {len(all_test_results)} probes.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Load and test pre-trained probes against labelled mega dataset and real-world data.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    # --- Core arguments ---
    parser.add_argument("--probe_dir", type=str, default="experiments/self_obfuscation_concepts/outputs/probe_checkpoints/20250717_002615_TRAIN_labelled_topical_logistic_12_10", help="Directory containing the pre-trained probe .pkl files and metadata.json.")
    parser.add_argument("--dataset_path", type=str, default="data/self_obfuscation_v1_synthetic_rating/outputs/mega_dataset_evaluated_20250716_190901.json", help="Path to the evaluated mega dataset JSON file.")
    parser.add_argument("--output_dir_base", type=str, default="experiments/self_obfuscation_concepts/outputs/probe_test_results", help="Base directory to save test results.")
    parser.add_argument("--custom_folder_name", type=str, default="", help="Custom folder name for the output directory.")
    
    # --- Testing & Evaluation arguments ---
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for testing.")
    parser.add_argument("--bootstrap_iterations", type=int, default=1000, help="Number of resampling iterations for bootstrapping.")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility of data splitting.")
    parser.add_argument("--min_test_samples_per_class", type=int, default=100, help="Minimum number of test samples required per class.")
    
    # --- Data filtering arguments ---
    parser.add_argument("--positive_threshold", type=float, default=1.0, help="Threshold for positive samples (>= this value).")
    parser.add_argument("--negative_threshold", type=float, default=0.0, help="Threshold for negative samples (<= this value).")
    parser.add_argument("--use_topical_ratings", action="store_true", default=True, help="Use topical response ratings instead of vanilla response ratings.")
    parser.add_argument("--use_vanilla_ratings", dest="use_topical_ratings", action="store_false", help="Use vanilla response ratings instead of topical response ratings.")
    parser.add_argument("--filter_to_trained_adjectives", action="store_true", default=True, help="Only test adjectives that were in adjectives_trained metadata (default: True).")
    parser.add_argument("--no_filter_to_trained_adjectives", dest="filter_to_trained_adjectives", action="store_false", help="Test all adjectives in the dataset, not just trained ones.")
    
    # --- UltraChat testing arguments ---
    parser.add_argument("--num_ultrachat_test_samples", type=int, default=1024, help="Number of UltraChat conversations to use as a negative test set.")
    parser.add_argument("--ultrachat_split", type=str, default="test_gen", help="The split of the UltraChat dataset to use (e.g., 'test_gen', 'train_sft').")
    parser.add_argument("--ultrachat_mega_dataset_path", type=str, default="data/self_obfuscation_v1_synthetic_rating/outputs/ultrachat_mega_dataset_20250718_035042.json", help="Path to UltraChat mega dataset JSON file with adjective ratings. If provided, will use adjective-specific negative samples instead of generic UltraChat data.")

    args = parser.parse_args()

    test_and_save_labelled_probes(
        probe_dir=args.probe_dir,
        dataset_path=args.dataset_path,
        output_dir_base=args.output_dir_base,
        batch_size=args.batch_size,
        custom_folder_name=args.custom_folder_name,
        seed=args.seed,
        bootstrap_iterations=args.bootstrap_iterations,
        num_ultrachat_test_samples=args.num_ultrachat_test_samples,
        ultrachat_split=args.ultrachat_split,
        positive_threshold=args.positive_threshold,
        negative_threshold=args.negative_threshold,
        use_topical_ratings=args.use_topical_ratings,
        min_test_samples_per_class=args.min_test_samples_per_class,
        filter_to_trained_adjectives=args.filter_to_trained_adjectives,
        ultrachat_mega_dataset_path=args.ultrachat_mega_dataset_path
    )